{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lib input sys.path\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import libs.linear_models as lm\n",
    "import libs.data_util as data\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    "\n",
    "* For $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2$, the parameters are $a_0, a_1, a_2$.\n",
    "* For $h(x) \\in \\mathcal{H}_{10}$, we have $h(x) = \\sum^{10}_{k=0} a_k x^k$.\n",
    "\n",
    "For any given $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2 = \\sum^{10}_{k=0} a_k x^k$ where $a_k = 0$ when $k\\gt 2$. So $h(x) \\in \\mathcal{H}_{10}$ as well. We thus conclude $\\mathcal{H}_2 \\subset \\mathcal{H}_{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2\n",
    "Reading exercise.\n",
    "\n",
    "#### Exercise 4.3\n",
    "\n",
    "* (a) Consider a given $\\mathcal{H}$\n",
    "  * If the best approximation from $\\mathcal{H}$ is less complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general should increase, since it'll be harder for functions in $\\mathcal{H}$ to fit the target function. There'll be a higher tendency to overfit. \n",
    "  * If the best approximation from $\\mathcal{H}$ is more complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general may decrease first, reducing the deterministic noise and there'll be a lower tendency to overfit. But once the complexity of $f$ exceeds the best function approximation from $\\mathcal{H}$, and if we continue increase the complexity of $f$, we will increase the deterministic noise and thus increase the tendency to overfit.\n",
    "  \n",
    "* (b) Given a fixed $f$ \n",
    "  * If the best approximation from $\\mathcal{H}$ is less complex than the target function, then when we decrease the complexity of $\\mathcal{H}$, we increase the deterministic noise thus increasing the tendency of overfit. \n",
    "  * If the best approximation from $\\mathcal{H}$ is more complex than the target function, then when we decrease the complexity of $\\mathcal{H}$, we will decrease the deterministic noise thus decreasing the tendency of overfit. Well, if we continue to decrease the complexity of $\\mathcal{H}$, passing the point where its complexity is equal to $f$, we start to increase the deministic noise again and thus increasing overfit.\n",
    "  \n",
    "#### Exercise 4.4\n",
    "\n",
    "Let's compute $E_{in}(w)$: \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) &= \\frac{1}{N}\\sum^N_{n=1}\\left(w^Tz_n - y_n\\right)^2\\\\\n",
    "&= \\frac{1}{N} \\|Zw - y\\|^2\\\\\n",
    "&= \\frac{1}{N} (Zw - y)^T(Zw - y)\\\\\n",
    "&= \\frac{1}{N} (w^TZ^T - y^T)(Zw - y)\\\\\n",
    "&= \\frac{1}{N} w^TZ^TZw - w^TZ^Ty - y^TZw + y^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "On the other hand, the equation (4.3) is\n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) &= \\frac{(w-w_{lin})^TZ^TZ(w-w_{lin})+y^T(I-H)y}{N}\\\\\n",
    "&= \\frac{(w^T-w^T_{lin})Z^TZ(w-w_{lin})+y^Ty-y^THy}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^TZw_{lin}-w^T_{lin}Z^TZw+w^T_{lin}Z^TZw_{lin}+y^Ty-y^THy}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^TZ(Z^TZ)^{-1}Z^Ty-\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZw}{N}\\\\\n",
    "&+\\frac{\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-T}Z^TZw}{N}\\\\\n",
    "&+\\frac{y^TZ(Z^TZ)^{-T}Z^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-1}Z^TZw}{N}\\\\\n",
    "&+\\frac{y^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZw+y^Ty}{N}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This agrees with the result derived above. So we proved equation (4.3).\n",
    "\n",
    "* (a) The value of $w$ that minimizes $E_{in}$ is $w_{lin}$, since it makes the first term $0$, which is a quadratic term on $w$ and is greater or equal to $0$ all the time. The second term doesn't depend on $w$. \n",
    "\n",
    "* (b) The minimum in-sample error when $w=w_{lin}$ is simply the second term, i.e. $\\min E_{in}(w) = \\frac{y^T(I-H)y}{N}$.\n",
    "\n",
    "#### Exercise 4.5 [Tikhonov regularizer]\n",
    "\n",
    "* (a) $\\sum^Q_{q=0}w^2_q = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix} = w^Tw$, so $\\Gamma = I$.\n",
    "\n",
    "* (b) $\\left(\\sum^Q_{q=0}w_q\\right)^2 = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}1\\\\1\\\\\\dots\\\\1\\end{bmatrix}[1,1,\\dots,1]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix}$, so $\\Gamma = [1,1,\\dots,1]$.\n",
    "\n",
    "#### Exercise 4.6\n",
    "\n",
    "If we use the hard-order constraint, with less parameters, the perceptron's VC dimension decreases, and it's less likely to classify the same amount of points with more parameters. If we use the soft-order constraint, it won't change the signs of $(w^Tx)$ even when $w$ is small. So we'll still be able to classify the points.\n",
    "\n",
    "#### Exercise 4.7\n",
    "* (a) Note that the expectation w.r.t. $\\mathcal{D}_{val}$ is equivalent to $x$ because the $y$ are assumed to be generated by a true $f(x)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2_{val} &= Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right]\\\\\n",
    "&= Var_{\\mathcal{D}_{val}}\\left[\\frac{1}{K}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[Var_{\\mathcal{D}_{val}}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{\\mathcal{D}_{val}} \\left[e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{x} \\left[e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}}\\sigma^2(g^{-})\\right]\\\\\n",
    "&= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* (b) In classification problem, $e\\left(g^{-}(x), y\\right) = 1(g^{-}(x) \\ne y)$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "E_x\\left[e\\left(g^{-}(x), y\\right)\\right] & = P(g^{-}(x) \\ne y)\\times 1 + P(g^{-}(x) = y)\\times 0\\\\\n",
    "& = P(g^{-}(x) \\ne y)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the variance is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2(g^{-}) &= Var_x\\left[e\\left(g^{-}(x), y\\right)\\right] \\\\\n",
    "&= E_x\\left[\\left(e - E_x[e]\\right)^2\\right]\\\\\n",
    "&= P(g^{-}(x) \\ne y)\\left[(1-E_x[e])^2\\right] + \\left(1-P(g^{-}(x) \\ne y)\\right)\\left[(0-E_x[e])^2\\right]\\\\\n",
    "&= P(1-P)^2 + (1-P)P^2\\\\\n",
    "&= P(1-P)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) In the end\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2_{val} &= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
    "&= \\frac{P(1-P)}{K}\\\\\n",
    "&= \\frac{-(P-0.5)^2+0.25}{K}\\\\\n",
    "&\\le \\frac{1}{4K}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (d) The squared error $e\\left(g^{-}(x),y\\right)$ is unbounded. The variance of it is also unbounded. So there's no uniform upper bound for $Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right] = \\frac{1}{K}\\sigma^2(g^{-})$. \n",
    "\n",
    "* (e) For regression with squared error, if we train using fewer points (smaller $N-K$) to get $g^{-}$, then the resulting $g^{-}$ will be worse, the expectation of the squared error $E\\left[e\\left(g^{-}(x),y\\right)\\right]$ becomes larger. For continuous, non-negative random variables, higher mean often implies higher variance, so $\\sigma^2(g^{-})$ will be higher.\n",
    "\n",
    "* (f) When we increasing the size of validation set $K$, the error between $E_{val}(g^{-})$ and $E_{out}(g^{-})$ is $\\frac{\\sigma(g^{-})}{\\sqrt{K}}$. It can drop in the case of classification. But for regression, it depends on which of $\\sigma(g^{-})$ or $K$ increases faster, so the  $E_{val}(g^{-})$ as an estimate of $E_{out}$ can become worse or better.\n",
    "\n",
    "\n",
    "* TODO\n",
    "\n",
    "Does it mean for classification the estimate will always become better when we increase the K? \n",
    "\n",
    "But note, the $E_{out}(g^{-})$ is only for the hypothesis $g^{-}$, which can be pretty bad when $K$ is large. So for classification problem, even the error between $E_{out}(g^{-})$ and $E_{val}(g^{-})$ goes to zero, but the  $E_{out}(g^{-})$ can be quite large.\n",
    "\n",
    "\n",
    "#### Exercise 4.8\n",
    "\n",
    "For each model $\\mathcal{H}_m$, $g^-_m$ is independently learned of from the validation set. When we take the expectation w.r.t. validation data set, the $g^-_m$ doesn't change, the expectation depends only on $x_n$, as in the derivation of equation (4.8). \n",
    "So I think $E_m$ is an unbiased estimate for the out-of-sample error $E_{out}(g^-_m)$.\n",
    "\n",
    "#### Exercise 4.9\n",
    "\n",
    "When $K$ increases, there are less points available to train the models, so the learned models become worse, and $E_{val}(g^-_m)$ becomes larger for each model $m$. $g^-_{m^*}$ is the model with the lowest validation error among all models, so  $E_{val}(g^-_{m^*})$ will also increases with $k$. The same logic applies to $E_{out}(g^-_{m^*})$ as well since $E_{val}(g^-_{m^*})$ is an estimate of $E_{out}(g^-_{m^*})$.\n",
    "\n",
    "When $K$ increases to certain large value, there are much less points $N-K$ available to train the models, complex models converge to simple models(TODO), so the optimistic validation error $E_{val}(g^-_{m^*})$  is closer to all validation errors $E_{val}(g^-_{m})$ for all models. Since each single validation error $E_{val}(g^-_{m})$  is an unbiased estimate of the $E_{out}(g^-_{m})$, the optimistic validation error $E_{val}(g^-_{m^*})$ converges to $E_{out}(g^-_{m^*})$ as well.\n",
    "\n",
    "#### Exercise 4.10 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.1\n",
    "\n",
    "As we can see from below graphs, when we increase the order, the graph of monomials $\\phi_i(x)$ don't seem to become more complex. This does not correspond to the intuitive notion of increasing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Problem 4.1 \n",
    "\n",
    "def monomials(x, i):\n",
    "    return np.exp(i*np.log(np.abs(x)))\n",
    "    #return np.power(x, i) #This causes overflow\n",
    "\n",
    "xs = np.arange(-1, 1, 0.02) #(1,50,1)\n",
    "figsize = plt.figaspect(1)\n",
    "f, axs = plt.subplots(4, 4, figsize=figsize)\n",
    "for i in np.arange(0,16):\n",
    "    ys = monomials(xs, i)\n",
    "    k = int(i / 4)\n",
    "    j = int(i - 4*k)\n",
    "    axs[k][j].plot(xs, ys)\n",
    "    #axs[k][j].set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.2\n",
    "\n",
    "$z = \\left[L_0(x), L_1(x), L_2(x) \\right]^T = \\left[1, x, \\frac{3}{2}x^2 - \\frac{1}{2}\\right]^T$. For the hypothesis with $w=[1,-1,1]^T$, we have\n",
    "\n",
    "$h(x) = w^Tz = [1, -1, 1]\\begin{bmatrix}1\\\\ x \\\\\\frac{3}{2}x^2 - \\frac{1}{2}\\end{bmatrix} = 1-x+\\frac{3}{2}x^2 - \\frac{1}{2} = \\frac{3}{2}x^2 - x + \\frac{1}{2}$. \n",
    "\n",
    "This is degree-2 polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.3\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "L_0(x) &= 1\\\\\n",
    "L_1(x) &= x\\\\\n",
    "L_2(x) &= \\frac{3}{2}x^2 - \\frac{1}{2}\\\\\n",
    "L_3(x) &= \\frac{5}{2}x^3 - \\frac{3}{2}x\\\\\n",
    "L_4(x) &= \\frac{35}{8}x^4 - \\frac{15}{4}x^2+\\frac{3}{8}\\\\\n",
    "L_5(x) &= \\frac{63}{8}x^5 - \\frac{35}{4}x^3+\\frac{75}{40}x\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Legendre(x, K):\n",
    "    \"\"\"Compute Legendre at x up to the Kth Legendre Polynomials\n",
    "    \"\"\"\n",
    "    \n",
    "    L0, L1 = 1, x\n",
    "    if K < 0:\n",
    "        raise ValueError(\"The order of Legendre polynomial can't be negative\")\n",
    "    \n",
    "    if K == 0:\n",
    "        return [L0]\n",
    "    elif K == 1:\n",
    "        return [L0, L1]\n",
    "\n",
    "    Ls = [L0, L1]\n",
    "    Lkm1, Lkm2 = L1, L0\n",
    "    for k in range(2, K+1):\n",
    "        Lk = (2*k-1)*x*Lkm1/k - (k-1)*Lkm2/k\n",
    "        Ls.append(Lk)\n",
    "        Lkm2 = Lkm1\n",
    "        Lkm1 = Lk\n",
    "    return Ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "xs = np.arange(-1,1,0.05) #Legendre polynomials are defined within [-1, 1]\n",
    "figsize = plt.figaspect(1)\n",
    "f, axs = plt.subplots(2, 3, figsize=figsize)\n",
    "np_arrays = []\n",
    "K=6\n",
    "\n",
    "for x in xs:\n",
    "    y = np.array(Legendre(x, K))\n",
    "    np_arrays.append(y)\n",
    "ys = np.vstack(np_arrays)\n",
    "\n",
    "for i in np.arange(0,K):\n",
    "    k = int(i / 3)\n",
    "    j = int(i - 3*k)\n",
    "    axs[k][j].plot(xs, ys[:,i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.3 (b) \n",
    "\n",
    "We use induction. It's easy to see that when $k=0$ and $k=1$, $L_k(x)$ is a linear combination of monomials $x^k, x^{k-2},\\dots$.\n",
    "\n",
    "Suppose this is also true for $L_m(x)$ where $m\\le k$, then when $m=k+1$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "L_{k+1}(x) &= \\frac{2k-1}{k}xL_{k-1}(x) - \\frac{k-1}{k}L_{k-2}(x)\\\\\n",
    "&= \\frac{2k-1}{k}x\\left(a_0x^k+a_1x^{k-2}+\\dots\\right) - \\frac{k-1}{k}\\left(b_0x^{k-1}+b_1x^{k-3}+\\dots\\right)\\\\\n",
    "&= \\frac{2k-1}{k}\\left(a_0x^{k+1}+a_1x^{k-1}+\\dots\\right) - \\frac{k-1}{k}\\left(b_0x^{k-1}+b_1x^{k-3}+\\dots\\right)\\\\\n",
    "&= c_0x^{k+1}+c_1x^{k-1}+\\dots\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So $L_{k+1}(x)$ is a linear combination of monomials $x^{k+1}, x^{k-1},\\dots$. Thus\n",
    "\n",
    "\\begin{align*}\n",
    "L_k(-x) = (-1)^kL_k(x)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Problem 4.3 (c)\n",
    "\n",
    "* $k=1$, $LHS=(x^2-1)\\frac{dL_1(x)}{dx} = (x^2-1)$, and $RHS=xL_1(x) - L_0(x) = x^2 -1$. So the equation is correct when $k=1$.\n",
    "\n",
    "* Suppose for all $m\\le k$, we have $\\frac{x^2-1}{m}\\frac{dL_m(x)}{dx}=xL_m(x)-L_{m-1}(x)$.\n",
    "* When $m=k+1$, we have (To make the display simple, we use $L_k$ to represent $L_k(x)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL_{k+1}(x)}{dx} &= \\frac{2(k+1)-1}{k+1}\\left(L_k + x\\frac{dL_k}{dx}\\right) - \\frac{k}{k+1}\\frac{dL_{k-1}}{dx}\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(L_k + x\\frac{k}{x^2-1}\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\frac{k-1}{x^2-1}\\left(xL_{k-1} - L_{k-2}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take $L_{k-2} = \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}$ into above equation we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL_{k+1}}{dx} &= \\frac{2k+1}{k+1}\\left(L_k + x\\frac{k}{x^2-1}\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\frac{k-1}{x^2-1}\\left(xL_{k-1} - \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Multiply both sides with $x^2-1$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "(x^2-1)\\frac{dL_{k+1}}{dx} &= \\frac{2k+1}{k+1}\\left((x^2-1)L_k + kx\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k(k-1)}{k+1}\\left(xL_{k-1} - \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}\\right)\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(\\left((k+1)x^2-1\\right)L_k - kxL_{k-1}\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\left(\\left(k-1\\right)xL_{k-1} - \\left(2k-1\\right)xL_{k-1} +kL_k\\right)\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(\\left((k+1)x^2-1\\right)L_k - kxL_{k-1}\\right)- \\frac{k^2}{k+1}\\left(L_k-xL_{k-1}\\right)\\\\\n",
    "&= \\left((2k+1)x^2-(k+1)\\right)L_k-kxL_{k-1}\\\\\n",
    "&= x\\left((2k+1)xL_k-kL_{k-1}\\right)-(k+1)L_k\\\\\n",
    "&= x(k+1)L_{k+1} - (k+1)L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have\n",
    "\\begin{align*}\n",
    "\\frac{x^2-1}{k+1}\\frac{dL_{k+1}}{dx} &=xL_{k+1} - L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Problem 4.3 (d) \n",
    "\n",
    "From problem 3.4(c), we have \n",
    "\\begin{align*}\n",
    "(x^2-1)\\frac{dL_{k}}{dx} &= k(xL_{k} - L_{k-1})\\\\\n",
    "(x^2-1)\\frac{dL_{k-1}}{dx} &= (k-1)(xL_{k-1} - L_{k-2})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.t $x$ on $\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}&= \\frac{d}{dx}\\left(kxL_k - kL_{k-1}\\right)  \\\\\n",
    "&= k\\left(L_k + x\\frac{dL_k}{dx}\\right) - k\\frac{dL_{k-1}}{dx}\\\\\n",
    "&= kL_k + k\\left(x\\frac{dL_k}{dx} - \\frac{dL_{k-1}}{dx}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Replace the above computed $\\frac{dL_{k}}{dx}$ and $\\frac{dL_{k-1}}{dx}$, we obtain\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}&= kL_k + k\\left(x\\frac{dL_k}{dx} - \\frac{dL_{k-1}}{dx}\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(xk(xL_{k} - L_{k-1}) - (k-1)(xL_{k-1} - L_{k-2})\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(kx^2L_{k} - \\left((2k-1)xL_{k-1} - (k-1)L_{k-2}\\right)\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(kx^2L_{k} - kL_k\\right)\\\\\n",
    "&= kL_k + k^2L_k\\\\\n",
    "&= k(k+1)L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (e) It doesn't hurt if we can prove that the equation is correct for $l\\le k$ for any given $k$. We use induction to prove this.\n",
    "  * When $k=0$, $l$ can only be $0$, so we have $\\int^1_{-1}dxL_0L_0 = \\int^1_{-1}dx = 2 = \\frac{2}{2k+1}$. The equation is thus correct when $k=0$.\n",
    "  * Assume that the equation is correct when $k\\le K$, let's look at the case when $k = K+1$.\n",
    "  \n",
    "\\begin{align*}\n",
    "L_{K+1}L_l &= \\frac{2K+1}{K+1}xL_{K}L_l - \\frac{K}{K+1}L_{K-1}L_l\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We need work out the integration of $\\int^1_{-1}xL_{K}L_ldx$. Let's do it.\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_ldx &= \\frac{1}{2}(x^2-1)L_KL_l|^1_{-1} - \\int^1_{-1}\\frac{1}{2}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\\\\\n",
    "&= - \\int^1_{-1}\\frac{1}{2}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\n",
    "\\end{align*}\n",
    "\n",
    "Plug the result of problem (c), i.e. $\\frac{x^2-1}{k}\\frac{dL_{k}}{dx} =xL_{k} - L_{k-1}$ for $k=K$ and $k=l$ into above equation, we have\n",
    "\n",
    "\\begin{align*}\n",
    "-2\\int^1_{-1}xL_{K}L_ldx &= \\int^1_{-1}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(L_K(x^2-1)dL_l + L_l(x^2-1)dL_K\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(L_K\\left(lxL_l-lL_{l-1}\\right) + L_l\\left(KxL_K-KL_{K-1}\\right)\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(\\left(l + K\\right)xL_KL_l -\\left(lL_KL_{l-1} + KL_lL_{K-1}\\right)\\right)dx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Move the term with $xL_KL_l$ to the same side, we have\n",
    "\\begin{align*}\n",
    "(2+K+l)\\int^1_{-1}xL_{K}L_ldx &= \\int^1_{-1}\\left(lL_KL_{l-1} + KL_lL_{K-1}\\right)dx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let's consider different cases of different $l$ values to compute the $\\int^1_{-1}xL_{K}L_ldx$.\n",
    "\n",
    "  * If $l \\lt K-1$, then clearly by our assumptions using induction, we have $\\int^1_{-1}xL_{K}L_ldx = 0$. Plug into the formula for $\\int^1_{-1}L_{K+1}L_ldx$ we have \n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_ldx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_l - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_l\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K-1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_{K-1}dx &= \\frac{1}{2K+1}\\int^1_{-1}\\left((K-1)L_KL_{K-2} + KL^2_{K-1}\\right)dx\\\\\n",
    "&= \\frac{1}{2K+1}\\int^1_{-1}KL^2_{K-1}dx\\\\\n",
    "&= \\frac{2K}{(2K-1)(2K+1)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_{K-1}dx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_{K-1} - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_{K-1}\\\\\n",
    "&= \\frac{2K+1}{K+1}\\frac{2K}{(2K-1)(2K+1)} - \\frac{K}{K+1}\\frac{2}{2K-1}\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K$\n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_Kdx &= \\frac{1}{2(K+1)} \\int^1_{-1}\\left(KL_KL_{K-1} + KL_KL_{K-1}\\right)dx\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_Kdx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_K - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_K\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K+1$\n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_{K+1}dx &= \\frac{1}{2K+3} \\int^1_{-1}\\left((K+1)L_KL_K + KL_{K+1}L_{K-1}\\right)dx\\\\\n",
    "&= \\frac{K+1}{2K+3}\\frac{2}{2K+1}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_{K+1}dx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_{K+1} - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_{K+1}\\\\\n",
    "&= \\frac{2K+1}{K+1}\\frac{K+1}{2K+3}\\frac{2}{2K+1}\\\\\n",
    "&= \\frac{2}{2K+3}\\\\\n",
    "&= \\frac{2}{2(K+1)+1}\\\\\n",
    "\\end{align*}  \n",
    "\n",
    "Where the seond term on the right hand side is $0$ because we just proved when $l =K-1$.\n",
    "\n",
    "We now conclude that the equation is correct for all $k$ and $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.4 TODO\n",
    "\n",
    "To scale the coefficients $a_q$, we need to compute the expectation $E_{a,x}[f^2]$ first. \n",
    "\n",
    "\\begin{align*}\n",
    "E_{a,x}[f^2(x)] &= E_{a,x}\\left(\\sum^{Q_f}_{q=0} a_qL_q(x)\\right)^2\\\\\n",
    "&= E_{a,x}\\left(\\sum^{Q_f}_{q=0} a^2_qL^2_q(x) + \\sum^{Q_f}_{p\\ne q} a_pa_qL_p(x)L_q(x)\\right)\\\\\n",
    "&= E_{a,x}\\left(\\sum^{Q_f}_{q=0} a^2_qL^2_q(x)\\right) + E_{a,x}\\left(\\sum^{Q_f}_{p\\ne q} a_pa_qL_p(x)L_q(x)\\right)\\\\\n",
    "&= \\sum^{Q_f}_{q=0} E_a a^2_qE_xL^2_q(x) + \\sum^{Q_f}_{p\\ne q} E_a[a_pa_q]E_x[L_p(x)L_q(x)]\\\\\n",
    "&= \\sum^{Q_f}_{q=0} E_xL^2_q(x)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the independence of random variables $a$ and $x$, and also $a$ is standard normal distributed, so $E_a[a_q] = 0$ and $E_a[a^2_q] = 1$.\n",
    "\n",
    "From problem 4.3(e) we have $\\int^1_{-1}L^2_k(x) = \\frac{2}{2k+1}$, also $x$ has a probability density $P(x) = \\frac{1}{2}$, so we have the expectation \n",
    "\n",
    "\\begin{align*}\n",
    "E_{a,x}[f^2(x)] &=\\sum^{Q_f}_{q=0} E_xL^2_q(x)\\\\\n",
    "&= \\sum^{Q_f}_{q=0}\\int^1_{-1}L^2_q(x)P(x)dx\\\\\n",
    "&= \\frac{1}{2}\\sum^{Q_f}_{q=0}\\int^1_{-1}L^2_q(x)dx\\\\\n",
    "&= \\frac{1}{2}\\sum^{Q_f}_{q=0}\\frac{2}{2q+1}\\\\\n",
    "&= \\sum^{Q_f}_{q=0}\\frac{1}{2q+1}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To scale $E_{a,x}[f^2(x)]$ to $1$, we need to re-scale each $a_q$ by $\\sqrt{\\frac{1}{\\sum^{Q_f}_{q=0}\\frac{1}{2q+1}}}$.\n",
    "\n",
    "* (a) Notice that $E_{a,x}[f] = 0$, so in fact we have $E_{a,x}[y_n] = 0$. Once we normalize $E_{a,x}[f^2(x)]=1$, we make the variance of $f$ to be 1, which can be compared with the variance on the noise term $\\epsilon$, which is $\\sigma^2$. This makes it easier to compare between different order of polynomials. \n",
    "\n",
    "* (b) Suppose we have data set $x_1,\\dots, x_N$ and $y_1,\\dots,y_N$, we apply polynomial $\\Phi_Q(x)$ transformation to each $x_n$ and obtain $z_n = \\Phi_Q(x_n)$, so we have an input matrix of $Z$ with dimension $N\\times (Q+1)$. The best fit hypotheses to the data is then from the least square algorithm: \n",
    "\n",
    "\\begin{align*}\n",
    "w &= (Z^TZ)^{-1}Z^Ty\\\\\n",
    "g_Q(x) &= \\Phi_Q(x)(Z^TZ)^{-1}Z^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) Suppose we obtained a best fit hypothesis $g_K(z) = z^Tw = \\Phi^T(x)w$ to the data from $\\mathcal{H}_K$, here $K=2$ or $K=10$. Let $M=\\begin{bmatrix}1\\\\x\\\\x^2\\\\ \\vdots \\\\ x^K\\end{bmatrix}_{(K+1)\\times 1}$, then we have $g_K(x) = M^Tw$. If we can express $g_K(x)$ in terms of Legendre polynomials, i.e. $g_K(x) = \\sum^K_{i=0}c_iL_i(x)$ then we can compute $E_{out}$ easily as:\n",
    "\n",
    "\\begin{align*}\n",
    "E_{out} &= E_{x}[(g_K(x) - f(x))^2]\\\\\n",
    "&= E_{x}\\left[\\left(\\sum^K_{i=0}c_iL_i(x) - \\sum^{Q_f}_{q=0}c_qL_q(x)\\right)^2\\right]\\\\\n",
    "&= E_{x}\\left[\\left(\\sum^{\\max{K,Q_f}}_{i=0}b_iL_i(x)\\right)^2\\right]\\\\\n",
    "&= E_{x}\\left(\\sum^{\\max{K,Q_f}}_{i=0}b^2_iL^2_i(x) + \\sum^{\\max{K,Q_f}}_{i\\ne j}b_ib_jL_i(x)L_j(x)\\right)\\\\\n",
    "&= \\sum^{\\max{K,Q_f}}_{i=0}b^2_iE_{x}L^2_i(x)\\\\\n",
    "&= \\sum^{\\max{K,Q_f}}_{i=0}b^2_i \\frac{2}{2i+1}\\\\\n",
    "&= \\sum^{\\max{K,Q_f}}_{i=0}\\frac{2b^2_i}{2i+1}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now let's figure out how to solve for $c_i$ that can make $g_K(x) = \\sum^K_{i=0}c_iL_i(x)$. \n",
    "First, let's define the matrix of Legendre coefficients of a polynomial of degree $K$, i.e. the coefficient vector for $L_0 = 1$ is $P_0 = \\begin{bmatrix}1\\\\0\\\\0\\\\ \\vdots \\\\0\\end{bmatrix}_{(K+1)\\times 1}$. Similarly for $L_1 = x$, we have $P_1 = \\begin{bmatrix}0\\\\1\\\\0\\\\ \\vdots \\\\0\\end{bmatrix}_{(K+1)\\times 1}$ and for $L_2 = \\frac{3}{2}x^2 - \\frac{1}{2}$, we have $P_2 = \\begin{bmatrix}- \\frac{1}{2}\\\\0\\\\\\frac{3}{2}\\\\ \\vdots \\\\0\\end{bmatrix}_{(K+1)\\times 1}$. Thus $L_K = M^TP_K$. we have\n",
    "\n",
    "\\begin{align*}\n",
    "g_K(x) &= \\sum^K_{i=0}c_iL_i(x)\\\\\n",
    "&= \\begin{bmatrix}1&x&x^2& \\dots & x^K\\end{bmatrix}\n",
    "\\begin{bmatrix}1&0&-\\frac{1}{2}&\\dots\\\\ 0&1&0&\\dots \\\\ 0 & 0 & \\frac{3}{2} \\dots \\\\ \\dots & \\dots & \\dots & \\dots \\\\\\end{bmatrix}\n",
    "\\begin{bmatrix}c_0\\\\c_1\\\\c_2\\\\ \\vdots \\\\c_K\\end{bmatrix}\\\\\n",
    "&= M^T\\begin{bmatrix}P_0 & P_1 & P_2 & \\dots & P_K\\end{bmatrix}C\\\\\n",
    "&= M^TPC\\\\\n",
    "\\end{align*}\n",
    "\n",
    "On the other hand, from least square solution, we have $g_K(x) = M^Tw$, compare the two equations we see that \n",
    "$PC = w$, i.e. $C = P^{-1}w$. This is the formula we are going to use to solve for coefficients $c_0, c_1, \\dots, c_K$. \n",
    "* (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = np.zeros((3,3))\n",
    "L[0,0] = L[1,1] =1\n",
    "L[2,2] = 1.5\n",
    "L[0,2] = -0.5\n",
    "w = np.array([0.5, 1, 1.5])\n",
    "np.matmul(np.linalg.inv(L),w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.5\n",
    "\n",
    "If $\\lambda \\lt 0$, since $w^Tw$ is positive, to minimize the $E_{aug}(w)$, the learning algorithm will try to increase $|w|$ as much as possible. Then this corresponds to a soft order constraint of $w^Tw =\\infty$.\n",
    "\n",
    "#### Problem 4.6\n",
    "\n",
    "For $\\Gamma = I$ and $\\lambda \\gt 0$. \n",
    "* (a) Assume we have $\\|w_{reg}\\| \\gt \\|w_{lin}\\|$, then $w^T_{reg}w_{reg}=\\|w_{reg}\\|^2 \\gt \\|w_{lin}\\|^2 =w^T_{lin}w_{lin} $\n",
    "\n",
    "\\begin{align*}\n",
    "E_{aug}(w_{reg}) &= E_{in}(w_{reg}) + \\lambda w^T_{reg}w_{reg}\\\\\n",
    "&\\gt E_{in}(w_{reg}) + \\lambda w^T_{lin}w_{lin}\\\\\n",
    "&\\ge E_{in}(w_{lin}) + \\lambda w^T_{lin}w_{lin}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The last step is valid because $w_{lin}$ minimizes $E_{in}(w)$, so we have $E_{in}(w_{lin}) \\le E_{in}(w_{reg})$.\n",
    "\n",
    "so we have $E_{aug}(w_{reg}) \\gt E_{aug}(w_{lin})$. This is contradictary though because $w_{reg}$ is assumed to minimize $E_{aug}(w)$.\n",
    "\n",
    "So we must have $\\|w_{reg}\\| \\le \\|w_{lin}\\|$\n",
    "\n",
    "* (b) For linear models, we have $w_{reg} = \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty$, then \n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}w_{reg} &= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-T} \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-1} \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-2} Z^Ty\\\\\n",
    "&= u^T\\left(Z^TZ+\\lambda I\\right)^{-2} u\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $u=Z^Ty$.\n",
    "\n",
    "Now we show that $Z^TZ+\\lambda I$ has the same eigenvectors with correspondingly larger eigenvalues as $Z^TZ$. \n",
    "\n",
    "Assume $v$ is an eigenvector of $Z^TZ$ and $\\gamma$ is the corresponding eigenvalue, then we have \n",
    "$Z^TZv=\\gamma v$.\n",
    "\n",
    "\\begin{align*}\n",
    "(Z^TZ+\\lambda I)v &= Z^TZv + lambda I v\\\\\n",
    "&= \\gamma v + \\lambda v\\\\\n",
    "&= (\\gamma + \\lambda)v\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So $v$ is also an eigenvector of $Z^Z+\\lambda I$ and the corresponding eigenvalue is $\\gamma + \\lambda \\gt \\gamma$ since $\\lambda \\gt 0$. \n",
    "\n",
    "Also note, for any matrix $A$, the eigenvectors of $A^{-1}$ are the same as the eigenvectors of $A$, as $A^{-1}v = \\lambda^{-1}v$. The eigenvalues of $A^{-1}$ are inverse of eigenvalues of $A$, i.e. $\\lambda^{-1}$. Similarly, the eigenvectors of $A^{-2}$ are the same as the eigenvectors of $A$, but the eigenvalues of $A^{-2}$ are to the $-2$ power of eigenvalues of $A$, i.e. $\\lambda^{-2}$.\n",
    "\n",
    "Suppose the $v_1,v_2,\\dots,v_{d+1}$ are the eigenvectors of $Z^TZ$ with corresponding eigenvalues of $\\lambda_1, \\lambda_2, \\dots, \\lambda_{d+1}$, then write $u$ in the eigenbasis of these vectors, we have $u = \\sum^{d+1}_{k=1}c_kv_k$.\n",
    "\n",
    "Let $A=Z^TZ+\\lambda I$, then we have\n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}w_{reg} &= u^TA^{-2}u \\\\\n",
    "&= u^TA^{-2}(\\sum^{d+1}_{k=1}c_kv_k)\\\\\n",
    "&= u^T(\\sum^{d+1}_{k=1}c_kA^{-2}v_k)\\\\\n",
    "&= u^T(\\sum^{d+1}_{k=1}c_k\\lambda^{-2}_kv_k)\\\\\n",
    "&= (\\sum^{d+1}_{k=1}c_kv_k)(\\sum^{d+1}_{k=1}c_k\\lambda^{-2}_kv_k)\\\\\n",
    "&=\\sum^{d+1}_{k=1}\\lambda^{-2}_kc^2_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The second to last equation is because the eigenvectors are orthogonal to each other. \n",
    "\n",
    "The $\\lambda_k$s in the equation are eigenvalues of matrix $Z^TZ+\\lambda I$, the corresponding matrix for $w_{lin}$ without weight decay is $Z^TZ$, and $w_{lin}^Tw_{lin} = u^T(Z^TZ)^{-2}u = \\sum^{d+1}_{k=1}(\\lambda^s_k)^{-2}c^2_k$. Assume the eigenvalues of matrix  $Z^TZ$ are $\\lambda^s_1,\\lambda^s_2,\\dots,\\lambda^s_{d+1}$, where $\\lambda^s_k \\le \\lambda_k$ as proved above. So \n",
    "we have $w^T_{reg}w_{reg} \\le w_{lin}^Tw_{lin}$ and achieve the equality when $\\lambda = 0$.\n",
    "\n",
    "#### Problem 4.7\n",
    "\n",
    "For a matrix $Z$ that is $n\\times d$, let the SVD of $Z=U\\Gamma V^T$, where $\\Gamma$ is $d\\times d$ square diagnoal matrix, and $r$ of its diagonal elements are positive with the rest zeroes. $U$ is $n \\times d$ and $V$ is $d \\times d$. Also we have $U^TU=I_{d\\times d}$ and $V^TV = VV^T = I_{d\\times d}$. Note that $UU^T$ is not identity matrix. Let $Z^TZ$ have eigenvalues $\\sigma^2_1, \\sigma^2_2, \\dots, \\sigma^2_d$, define $a=U^Ty$, so we have\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ &= V\\Gamma U^T U\\Gamma V^T\\\\\n",
    "&= V\\Gamma^2V^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the eigenvalues in $\\sigma^2_1, \\sigma^2_2, \\dots, \\sigma^2_d$ (there might be zeroes) are the elements on the diagonal of matrix $\\Gamma^2$. \n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T \\\\\n",
    "&= U\\Gamma V^T \\left(V\\Gamma^2V^T + \\lambda VV^T\\right)^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T \\left(V\\left(\\Gamma^2 + \\lambda I \\right)V^T\\right)^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T V^{-T} \\left(\\Gamma^2 + \\lambda I \\right)^{-1} V^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma^2 \\left(\\Gamma^2 + \\lambda I \\right)^{-1}U^T\\\\\n",
    "&= U\\Lambda U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where the diagonal matrix $\\Lambda$ has elements of $\\frac{\\sigma^2_i}{\\sigma^2_i + \\lambda}$ on its $i$th diagnoal. This is because the matrix $\\Gamma^2 + \\lambda I$ is always invertible when $\\lambda \\gt 0$.\n",
    "\n",
    "Let $\\lambda = 0$, we have $H= U\\Gamma V^T \\left(V\\Gamma^2V^T \\right)^{-1} V\\Gamma U^T$, since $\\Gamma$ may not have inverse, since some of its diagnoal entries might be zero. For its inverse, let the non-zero entries to be inverse, and let the zero entries to keep zeroes. We have $H= UU^T$.\n",
    "\n",
    "Now it's time to compute $E_{in}(w_{reg})$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w_{reg}) - E_{in}(w_{lin}) &= \\frac{1}{N}y^T(I-H(\\lambda))^2y - \\frac{1}{N}y^T(I-H)^2y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[(I-H(\\lambda))^2 - (I-H)^2\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[(I-H(\\lambda))^T(I-H(\\lambda)) - (I-H)\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[I-2H(\\lambda)+H(\\lambda)^2 - (I-H)\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[-2H(\\lambda)+H(\\lambda)^2 + H\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[-2U\\Lambda U^T+ U\\Lambda^2 U^T + UU^T\\right]y\\\\\n",
    "&= \\frac{1}{N}y^TU\\left[-2\\Lambda + \\Lambda^2  + I\\right]U^Ty\\\\\n",
    "&= \\frac{1}{N}a^T(I-\\Lambda)^2a\\\\\n",
    "&=\\frac{1}{N}\\sum^d_{i=1}a^2_i\\left(1-\\frac{\\sigma^2_i}{\\sigma^2_i + \\lambda}\\right)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We thus finished the proof.\n",
    "\n",
    "#### Problem 4.8\n",
    "\n",
    "In the augmented error minimization with $\\Gamma = I$ and $\\lambda \\gt 0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{aug}(w) &= E_{in}(w) + \\lambda w^T\\Gamma^T\\Gamma w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.t. $w$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{aug}(w) &= \\nabla E_{in}(w) + \\lambda \\left(2\\Gamma^T\\Gamma\\right)w\\\\\n",
    "&= \\nabla E_{in}(w) + 2\\lambda w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then the update rule is \n",
    "\n",
    "\\begin{align*}\n",
    "w(t+1) &= w(t) - \\eta \\nabla E_{aug}(w)\\\\\n",
    "&= w(t) - \\eta \\left(\\nabla E_{in}(w) + 2\\lambda w\\right)\\\\\n",
    "&= (1-2\\eta\\lambda)w(t) -  \\eta \\nabla E_{in}(w)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is the weight decay rule: $w(t)$ decays before being updated by the gradient of $E_{in}(w)$.\n",
    "\n",
    "#### Problem 4.9\n",
    "\n",
    "* (a) $\\sqrt{\\lambda}\\Gamma$ has a dimension of $k \\times (d+1)$, while $Z$ has a dimension of $n\\times (d+1)$. $Z_{aug} = \\begin{bmatrix}Z\\\\ \\sqrt{\\lambda}\\Gamma\\end{bmatrix}$ and $y=\\begin{bmatrix}y\\\\ 0\\end{bmatrix}$.\n",
    "\n",
    "We also have $Z^T_{aug} = \\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix}$\n",
    "\n",
    "* (b) The least square solution with augmented data is\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(Z^T_{aug}Z_{aug}\\right)^{-1}Z^T_{aug}y_{aug} &= \\left(\\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix} \\begin{bmatrix}Z\\\\ \\sqrt{\\lambda}\\Gamma\\end{bmatrix}\\right)^{-1} \\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix}\\begin{bmatrix}y\\\\ 0\\end{bmatrix}\\\\\n",
    "&= \\left(Z^TZ + \\lambda\\Gamma^T\\Gamma\\right)^{-1} Z^Ty\\\\\n",
    "&= w_{reg}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Problem 4.10\n",
    "\n",
    "* (a) If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, then $w_{reg} = w_{lin}$\n",
    "* (b) $E_{in}(w) = \\frac{1}{N}\\sum^N_{i=1}\\left(y_i - w^Tx_i\\right)^2$. $E_{in}(w)$ is thus a quadratic function of $w$, so the contours of constant $E_{in}$ are ellipsoids.  In this case, $w^T_{reg}\\Gamma^T\\Gamma w_{reg} = C$. First notice that the region $w^T\\Gamma^T\\Gamma w \\le C$ is convex. When $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, $w_{lin}$ is outside of this region, as we move away from $w_{lin}$, the $E_{in}(w)$ increases (because $w_{lin}$ minimizes $E_{in}(w)$) but $w^T\\Gamma^T\\Gamma w$ decreases. The $w$ that satisfies $w^T\\Gamma^T\\Gamma w =C$ has the minimum possible $E_{in}(w)$ under constraint. So this is the solution $w_{reg}$.\n",
    "\n",
    "* (c) From part (b), consider that case when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, we have $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$, let's replace the inequality constraint to equality constraint. \n",
    "\n",
    "Use Lagrange multiplier, we can instead minimize \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) + \\lambda \\left(w^T\\Gamma^T\\Gamma w - C\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.r. $w$, and let it equal to $0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{in}(w_{reg}) + 2\\lambda_C \\Gamma^T\\Gamma w_{reg} &= 0\\\\\n",
    "\\Gamma^T\\Gamma w_{reg} &= -\\frac{1}{2\\lambda_C}\\nabla E_{in}(w_{reg}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since we know that $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$, (or take derivative w.r.t. $\\lambda$), we have \n",
    "\n",
    "\\begin{align*}\n",
    "C &= -\\frac{1}{2\\lambda_C}w^T_{reg}\\nabla E_{in}(w_{reg})\\\\\n",
    "\\lambda_C &= -\\frac{1}{2C}w^T_{reg}\\nabla E_{in}(w_{reg})\n",
    "\\end{align*}\n",
    "\n",
    "Also note that when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, $w_{reg} = w_{lin}$, we have $\\nabla E_{in}(w_{reg})= 0$, thus $\\lambda_C = 0$.\n",
    "\n",
    "Combine above results, we see that $w_{reg}$ minimizes $E_{in}(w) + \\lambda_Cw^T\\Gamma^T\\Gamma w$. \n",
    "\n",
    "* (d) \n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, we have $w_{reg} = w_{lin}$, and $\\lambda_C = 0$.\n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, Consider the norm to the surface of $w^T\\Gamma^T\\Gamma w = C$, it's $2\\Gamma^T\\Gamma w$.\n",
    "\n",
    "We claim that the norm and the graident of $E_{in}$ have to be in the opposite directions at the point $w_{reg}$, which minimizes the augmented error. \n",
    "\n",
    "To prove this is correct, Let's look at the projection of $\\nabla E_{in}(w)$ onto the norm. If this projection is along the same direction as norm, then we can move along the opposite of this projection, which will decrease $E_{in}(w)$ (because we move along some component of $-\\nabla E_{in}(w))$ and decrease $w^T\\Gamma^T\\Gamma w$ (it's also the opposite of norm), which will move to region of $w^T\\Gamma^T\\Gamma w \\lt C$. This contradicts our conclusion in part(b) that  the optimal $w_{reg}$ satisfies $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$. So the projection of $\\nabla E_{in}(w)$ must be at the opposite of the norm. \n",
    "\n",
    "Now suppose that in addition to the projection component, there's a non-zero component of $\\nabla E_{in}(w)$ at $w_{reg}$ that is perpendicular to the norm. If we move along the opposite of this component, we stay on the surface, but we'll decrease the $E_{in}$, this contradicts with the assumption that $w_{reg}$ minimizes the agumented error.\n",
    "\n",
    "Now we proved that the norm at $w_{reg}$, i.e. $\\Gamma^T\\Gamma w_{reg}$ is in opposite direction of the gradient of $E_{in}$ at $w_{reg}$, i.e. $\\nabla E_{in}(w_{reg})$. From the derivation of part (c) we have following equation\n",
    "  \n",
    "\\begin{align*}\n",
    "\\Gamma^T\\Gamma w_{reg} &= -\\frac{1}{2\\lambda_C}\\nabla E_{in}(w_{reg}) \\\\\n",
    "\\end{align*} \n",
    "\n",
    "This shows that we must have $\\lambda_C \\gt 0$.\n",
    "\n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$. \n",
    "\n",
    "Note that $w_{reg}$ depends on $C$. We first work out $\\frac{d\\nabla E_{in}(w_{reg})}{dC}$. We know that $E_{in}(w_{reg}) = \\frac{1}{N}\\|Xw-Y\\|^2 = \\frac{1}{N}(Xw-Y)^T(Xw-Y) = \\frac{1}{N}(w^TX^TXw - w^TX^Ty - y^TX^Tw + y^Ty)$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{in}(w_{reg}) &=  \\frac{2}{N}\\left(X^TXw -X^Ty\\right)\\\\\n",
    "w^T_{reg}\\nabla E_{in}(w_{reg}) &=  \\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right)\\\\\n",
    "\\frac{d\\left(w^T_{reg}\\nabla E_{in}(w_{reg})\\right)}{dC} &= \\frac{2}{N}\\frac{dw}{dC}|_{w_{reg}}\\left(2X^TXw_{reg} -X^Ty\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also take derivative w.r.t. $C$ on surface $w^T\\Gamma^T\\Gamma w = C$, we have $2\\Gamma^T\\Gamma w \\frac{dw}{dC} = 1$, at $w = w_{reg}$, we have $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$ when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, that is $2w_{reg}^T\\Gamma^T\\Gamma w_{reg}\\frac{dw}{dC}|_{w_{reg}}=1$, we obtain\n",
    "$\\frac{dw}{dC}|_{w_{reg}} = \\frac{1}{2C}w^T_{reg}$.\n",
    "\n",
    "\n",
    "Let's now take derivative of $\\lambda_C$ w.r.t. $C$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d\\lambda_C}{dC} &= \\frac{1}{2C^2}w^T_{reg}\\nabla E_{in}(w_{reg}) -\\frac{1}{2C}\\frac{d\\left(w^T_{reg}\\nabla E_{in}(w_{reg})\\right)}{dC} \\\\\n",
    "&= \\frac{1}{2C^2}\\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{2C}\\frac{2}{N}\\frac{dw}{dC}|_{w_{reg}}\\left(2X^TXw_{reg} -X^Ty\\right) \\\\\n",
    "&= \\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{2C}\\frac{2}{N}\\frac{1}{2C}w^T_{reg}\\left(2X^TXw_{reg} -X^Ty\\right) \\\\\n",
    "&= \\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - \\frac{1}{2}w^T_{reg}X^Ty\\right) \\\\\n",
    "&= -\\frac{1}{2NC^2}w^T_{reg}X^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "From previous derivation, $\\lambda_C = -\\frac{1}{2C}w^T_{reg}\\nabla E_{in}(w_{reg})$, we know that $\\lambda_C \\gt 0$, so we have  $w^T_{reg}\\nabla E_{in}(w_{reg}) \\lt 0$ because $C \\gt 0$. \n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}\\nabla E_{in}(w_{reg}) &= \\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right)\\\\\n",
    "w^T_{reg}X^TXw_{reg} &\\lt w^T_{reg}X^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then\n",
    "\\begin{align*}\n",
    "\\frac{d\\lambda_C}{dC} &= -\\frac{1}{2NC^2}w^T_{reg}X^Ty\\\\\n",
    "&\\lt -\\frac{1}{2NC^2}w^T_{reg}X^TXw_{reg}\\\\\n",
    "&\\le 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where the last inequation comes from the fact that $X^TX$ is positive semi-definite matrix. \n",
    "\n",
    "  * Proof: For any non-zero vector $v$, $v^TX^TXv = (Xv)^T(Xv) \\ge 0$ since $Xv$ is a vector.\n",
    "  \n",
    "So we have proved that $\\frac{d\\lambda_C}{dC} \\lt 0$, $\\lambda_C$ is a strictly decreasing function of $C$ when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$.\n",
    "\n",
    "#### Problem 4.11\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "w_{lin} &= (Z^TZ)^{-1}Z^Ty\\\\\n",
    "&= (Z^TZ)^{-1}Z^T(Zw_f+\\epsilon)\\\\\n",
    "&= (Z^TZ)^{-1}Z^TZw_f+ (Z^TZ)^{-1}Z^T\\epsilon\\\\\n",
    "&= w_f+ (Z^TZ)^{-1}Z^T\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{lin}x]\\\\\n",
    "&= E_{\\mathcal{D}}[\\left(w_f+ (Z^TZ)^{-1}Z^T\\epsilon\\right)^Tx]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx + \\epsilon^T Z(Z^TZ)^{-T} x]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx] + E_{Z,\\epsilon}[\\epsilon^T Z(Z^TZ)^{-1} x]\\\\\n",
    "&= w^T_fx + E_{Z,\\epsilon}[\\epsilon^T]E_{Z,\\epsilon}[Z(Z^TZ)^{-1}]x\\\\\n",
    "&= w^T_fx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the average function $\\bar{g}(x)$ is exactly the target function $f(x)$, i.e. $\\bar{g}(x) = f(x)$. \n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}[\\left(w^T_fx - w^T_fx\\right)^2]\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) Now we can compute variance\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D},x}\\left[\\left(g^{\\mathcal{D}}(x)-\\bar{g}(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(w^T_{lin}\\Phi(x)-w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(((Z^TZ)^{-1}Z^T\\epsilon)^T\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)^T\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[trace\\left(\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[trace\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= E_{Z, x}trace\\left[E_{\\epsilon}\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= E_{Z, x}trace\\left[E_{\\epsilon}\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\right)E_{\\epsilon}\\left(\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left((Z^TZ)^{-1}Z^TZ(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}E_xtrace\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}traceE_x\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}trace\\left[E_x\\left[(Z^TZ)^{-1}\\right]E_x\\left[\\Phi(x)\\Phi^T(x)\\right]\\right]\\\\\n",
    "&= \\sigma^2E_{Z}trace\\left[(Z^TZ)^{-1}\\Sigma_{\\Phi}\\right]\\\\\n",
    "&= \\sigma^2trace\\left[\\Sigma_{\\Phi}E_{Z}(Z^TZ)^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}E_{Z}(\\frac{1}{N}Z^TZ)^{-1}\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}Z^TZ &= \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is the in-sample estimate of $\\Sigma_{\\Phi}$, by the law of large numbers, we have $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$. The $\\Sigma_{\\Phi}$ has a dimension of $(Q+1)\\times (Q+1)$ for a given $x$, so we have\n",
    "\n",
    "\\begin{align*}\n",
    "var &= \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}E_{Z}(\\frac{1}{N}Z^TZ)^{-1}\\right]\\\\\n",
    "&\\approx \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}(\\Sigma_{\\Phi}+o(1))^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{N}traceI_{(Q+1)\\times (Q+1)}\\\\\n",
    "&= \\frac{\\sigma^2 (Q+1)}{N}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So for the well specified linear model, the bias is zero and the variance is increasing as the model gets larger (i.e. $Q$ increases) but decreasing in $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.12\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\left(Zw_f+\\epsilon\\right)\\\\\n",
    "&= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^TZw_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= \\left(I - \\left(Z^TZ + \\lambda I\\right)^{-1}\\lambda \\right)w_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= w_f - \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{reg}x]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(w_f- \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\right)^Tx\\right]\\\\\n",
    "&= E_{Z,\\epsilon}\\left[w^T_fx - \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x + \\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x\\right]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx- \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x] + E_{Z,\\epsilon}[\\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x]\\\\\n",
    "&= w^T_fx - \\lambda E_Z\\left[ w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}\\right]x + E_{Z,\\epsilon}[\\epsilon^T]E_{Z,\\epsilon}[Z(Z^TZ+ \\lambda I)^{-1}]x\\\\\n",
    "&= w^T_fx- \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}x\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\frac{1}{N}Z^TZ = \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)$ is the in-sample estimate of $\\Sigma_{\\Phi} = E[\\Phi(x)\\Phi^T(x)] = I_{(d+1)\\times(d+1)}$, so by the law of large numbers, $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$, so $\\frac{1}{N}(Z^TZ + \\lambda I) = \\Sigma_{\\Phi} + \\frac{1}{N}\\lambda I + o(1) = (1+\\frac{\\lambda}{N})I + o(1)$.\n",
    "\n",
    "Then when $N$ is large\n",
    "\n",
    "\\begin{align*}\n",
    "E_Z\\left(Z^TZ + \\lambda I\\right)^{-1} &= \\frac{1}{N}E_Z\\left(\\frac{1}{N}(Z^TZ + \\lambda I)\\right)^{-1} \\\\\n",
    "&= \\frac{1}{N}(1+\\frac{\\lambda}{N})^{-1}I \\\\\n",
    "&= \\frac{1}{\\lambda + N}I \\\\\n",
    "\\end{align*}\n",
    "\n",
    " \n",
    "\n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)- w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(\\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)E_Z\\left(Z^TZ + \\lambda I\\right)^{-T}w_f w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)\\frac{1}{\\lambda + N}I w_f w^T_f\\frac{1}{\\lambda + N}I \\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} E_{\\mathcal{D},x}\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace E_{\\mathcal{D},x}\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace \\left[E_{\\mathcal{D},x}[\\Phi(x)\\Phi^T(x)]w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [Iw_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [w_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} \\|w_f\\|^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the assumption that the input probability satisfies $E_{x} \\left[\\Phi(x)\\Phi^T(x)\\right]= I $\n",
    "\n",
    "Also note, that $w_f w^T_f$ is a $(d+1)\\times (d+1)$ matrix, its trace is just $\\|w_f\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.12 (b)\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D}, x}\\left[(g^{\\mathcal{D}}(x) - \\bar{g}(x))^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left(w^T_{reg}\\Phi(x) - (w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left((w^T_f-w^T_f\\lambda(Z^TZ+\\lambda I)^{-1}+\\epsilon^TZ(Z^TZ+\\lambda I)^{-1})\\Phi(x) - (w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left((-w^T_f\\lambda(Z^TZ+\\lambda I)^{-1}+\\epsilon^TZ(Z^TZ+\\lambda I)^{-1})\\Phi(x) - ( - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To the first oder in $\\frac{1}{N}$, we know from 4.12 (a) that $\\frac{1}{N}(Z^TZ + \\lambda I) = (1+\\frac{\\lambda}{N})I + o(1)$ and $E_Z\\left(Z^TZ + \\lambda I\\right)^{-1} = \\frac{1}{\\lambda + N}I$\n",
    "\n",
    "So we have\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D}, x}\\left(\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)^2\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left(\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)\\\\\n",
    "&= E_{\\mathcal{D}, x}trace\\left(\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)\\\\\n",
    "&= E_{\\mathcal{D}, x}trace\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)E_{\\mathcal{D}, x}\\left(\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}, x}trace\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}, x}trace\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}}\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right) E_{ x}\\left(\\Phi(x)\\Phi^T(x)\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}}\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}}trace\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To first order in $\\frac{1}{N}$, we have $Z^TZ = NI$, take this into variance expression, we obtain\n",
    "\n",
    "\\begin{align*}\n",
    "var &= \\sigma^2  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\frac{\\sigma^2}{N}  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\frac{\\sigma^2}{N}  E_{\\mathcal{D}}[trace(H^2(\\lambda))]\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.13\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "H^2(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also from exercise 3.3 (d), when $\\lambda=0$, $H(\\lambda) = H$, and we have $trace(H) = \\tilde{d}+1$, and $H^2=H$, so $trace(H^2) = \\tilde{d}+1$.\n",
    "\n",
    "* (a) When $\\lambda = 0$, we have $trace(H(\\lambda)) = trace\\left(Z(Z^TZ)^{-1}Z^T\\right) = trace(H) = \\tilde{d}+1$\n",
    "  * (i) $d_{eff}(\\lambda) = 2trace(H(\\lambda)) - trace(H^2(\\lambda)) = 2trace(H) - trace(H^2) = trace(H) = \\tilde{d} + 1$.\n",
    "  * (ii) $d_{eff}(\\lambda)= trace(H(\\lambda)) = trace(H) = \\tilde{d} + 1$\n",
    "  * (iii) $d_{eff}(\\lambda)= trace(H^2(\\lambda)) = trace(H) = \\tilde{d} + 1$\n",
    "  \n",
    "* (b) When $\\lambda \\gt 0$. Let the SVD of $Z=U\\Gamma V^T$, where $U$ is $n\\times (\\tilde{d}+1)$ matrix, $U^TU=I_{(\\tilde{d} + 1) \\times (\\tilde{d}+1)}$ and $V$ is $(\\tilde{d} + 1) \\times (\\tilde{d}+1)$, $V^TV=VV^T=I_{(\\tilde{d} + 1) \\times (\\tilde{d}+1)}$, and $\\Gamma$ is $(\\tilde{d} + 1) \\times (\\tilde{d}+1)$ diagonal matrix. We have\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ &= V\\Gamma U^T U\\Gamma V^T\\\\\n",
    "&= V\\Gamma^2V^T\\\\\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "&= U\\Gamma V^T\\left(V\\Gamma^2V^T+\\lambda I\\right)^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T\\left(V(\\Gamma^2+\\lambda I)V^T\\right)^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^TV^{-T}(\\Gamma^2+\\lambda I)^{-1}V^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma^2(\\Gamma^2+\\lambda I)^{-1}U^T\\\\\n",
    "&= U\\Sigma U^T\\\\\n",
    "H^2(\\lambda) &= (U\\Sigma U^T)^TU\\Sigma U^T\\\\\n",
    "&= U\\Sigma^2U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\Sigma = \\Gamma^2(\\Gamma^2+\\lambda I)^{-1}$, let $\\sigma_1, \\sigma_2, \\dots, \\sigma_d$ be the eigenvalues of matrix $\\Gamma$.\n",
    "\n",
    "So the traces are\n",
    "\n",
    "\\begin{align*}\n",
    "trace(H(\\lambda)) &= trace(U\\Sigma U^T) \\\\\n",
    "&= trace(U^TU\\Sigma) \\\\\n",
    "&= trace(\\Sigma)\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^2_i}{\\sigma^2_i+\\lambda}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "trace(H^2(\\lambda)) &= trace(U\\Sigma^2 U^T) \\\\\n",
    "&= trace(U^TU\\Sigma^2) \\\\\n",
    "&= trace(\\Sigma^2)\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i}{(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (i) \n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= 2trace(H(\\lambda)) - trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{2\\sigma^2_i}{\\sigma^2_i+\\lambda} - \\frac{\\sigma^4_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i+2\\lambda\\sigma^2_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (ii)\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &=trace(H(\\lambda))\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^2_i}{\\sigma^2_i+\\lambda}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (iii)\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &=trace(H^2(\\lambda))\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also, it's clear that they are all larger or equal to zero. So we have finished the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.14 \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in} &= \\frac{1}{N}\\|y-Xw\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|y-H(\\lambda)y\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|\\left(I-H(\\lambda)\\right)y\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|\\left(I-H(\\lambda)\\right)(f+\\epsilon)\\|^2\\\\\n",
    "&= \\frac{1}{N}(f+\\epsilon)^T\\left(I-H(\\lambda)\\right)^T\\left(I-H(\\lambda)\\right)(f+\\epsilon)\\\\\n",
    "&= \\frac{1}{N}(f^T+\\epsilon^T)\\left(I-H(\\lambda)\\right)^2(f+\\epsilon)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}f^T\\left(I-H(\\lambda)\\right)^2\\epsilon +  \\frac{1}{N}\\\\epsilon^T\\left(I-H(\\lambda)\\right)^2f +  \\frac{1}{N}\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take expectation w.r.t. $\\epsilon$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}[E_{in}] &= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We need work out the expectation of the second term, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right] &= E_{\\epsilon}trace\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "&= E_{\\epsilon}trace\\left[\\epsilon\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\right]\\\\\n",
    "&= traceE_{\\epsilon}\\left[\\epsilon\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\right]\\\\\n",
    "&= traceE_{\\epsilon}[\\epsilon\\epsilon^T]E_{\\epsilon}\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= trace\\sigma^2\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= \\sigma^2trace\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the final expectation of \n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}[E_{in}] &= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2trace\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2trace\\left(I-2H(\\lambda)+H^2(\\lambda)\\right)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2\\left(N -\\left(2traceH(\\lambda)-trace(H^2(\\lambda))\\right)\\right)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\sigma^2(1 -\\frac{d_{eff}}{N})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (a) If the noise was not overfit, the term involving $\\sigma^2$ should equal to $\\sigma^2$ because \n",
    "$E_{\\epsilon}[E_{in}]$ is an estimate of the out-of-sample error, which should have constant term $\\sigma^2$. When the noise was overfit, $E_{\\epsilon}[E_{in}]$ underestimates the out-of-sample error. \n",
    "\n",
    "* (b) Hence, the degree to which the noise has been overfit is $\\frac{\\sigma^2d_{eff}}{N}$. It says that when $d_{eff}$ increases, the overfit to noise increases, when $N$ increases, the overift decreases. $d_{eff}$ acts as an effective number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.15 \n",
    "\n",
    "* (a) Let $\\tilde{Z} = Z\\Gamma^{-1}$, then $Z=\\tilde{Z}\\Gamma$, also assume $\\Gamma$ is square and invertible, we have\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda \\Gamma^T\\Gamma)^{-1}Z^T\\\\\n",
    "&= \\tilde{Z}\\Gamma((\\tilde{Z}\\Gamma)^T\\tilde{Z}\\Gamma+\\lambda \\Gamma^T\\Gamma)^{-1}(\\tilde{Z}\\Gamma)^T\\\\\n",
    "&= \\tilde{Z}\\Gamma(\\Gamma^T\\tilde{Z}^T\\tilde{Z}\\Gamma+\\Gamma^T\\lambda \\Gamma)^{-1}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\Gamma\\left(\\Gamma^T(\\tilde{Z}^T\\tilde{Z}+\\lambda I) \\Gamma\\right)^{-1}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\Gamma\\Gamma^{-1}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\Gamma^{-T}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\tilde{Z}^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now we can proceed similar with problem 4.13 (i), we let the SVD of $\\tilde{Z}=U\\Lambda V^T$, where $U$ is $n\\times (d+1)$ matrix, $U^TU=I_{(d + 1) \\times (d+1)}$ and $V$ is $(d + 1) \\times (d+1)$, $V^TV=VV^T=I_{(d + 1) \\times (d+1)}$, and $\\Lambda$ is $(d + 1) \\times (d+1)$ diagonal matrix with entries $s_0,s_1,\\dots,s_{d}$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{Z}^T\\tilde{Z} &= V\\Lambda U^T U\\Lambda V^T\\\\\n",
    "&= V\\Lambda^2V^T\\\\\n",
    "H(\\lambda) &= \\tilde{Z}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\tilde{Z}^T\\\\\n",
    "&= U\\Lambda V^T\\left(V\\Lambda^2V^T+\\lambda \\right)^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda V^T\\left(V(\\Lambda^2+\\lambda )V^T\\right)^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda V^TV^{-T}(\\Lambda^2+\\lambda I)^{-1}V^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda^2(\\Lambda^2+\\lambda I)^{-1}U^T\\\\\n",
    "&= U\\Sigma U^T\\\\\n",
    "H^2(\\lambda) &= (U\\Sigma U^T)^TU\\Sigma U^T\\\\\n",
    "&= U\\Sigma^2U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\Sigma = \\Lambda^2(\\Lambda^2+\\lambda I)^{-1}$ with entries on diagonal of $\\frac{s^2_i}{s^2_i+\\lambda}$, the effective degree of freedom is then\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= 2trace(H(\\lambda)) - trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^4_i+2\\lambda s^2_i} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{(s^2_i+\\lambda)^2-\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{d}_{i=0}1 - \\frac{\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= d+1 - \\sum^{d}_{i=0}\\frac{\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (b) Similarly, we have\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= trace(H(\\lambda)) \\\\\n",
    "&= trace(U\\Sigma U^T) \\\\\n",
    "&= trace(U^TU\\Sigma) \\\\\n",
    "&= trace(\\Sigma) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^2_i} {s^2_i+\\lambda}\\\\\n",
    "&= d+1- \\sum^{d}_{i=0}\\frac{\\lambda} {s^2_i+\\lambda}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) \n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^4_i} {(s^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In all cases, for $\\lambda \\ge 0$, we have $d_{eff}(\\lambda) \\ge 0$ because all the terms in the sum are non-negative. Also $d_{eff}(\\lambda) \\le d+1$ as shown in problem 4.13. \n",
    "\n",
    "When $\\lambda = 0$, we have $d_{eff}(0) = d+1$ as easily shown from above formulas. \n",
    "\n",
    "It's also easy to see that $d_{eff}$ is decreasing with $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.16\n",
    "\n",
    "For linear models and the general Tikhonov regularizer $\\Gamma$ with penalty term $\\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w$ in the augmented error, we have $E_{aug}(w) = E_{in}(w) + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w = \\frac{1}{N}\\|Zw-y\\|^2 + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w$. Take derivative w.r.t. $w$ and let it equal to $0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dE_{aug}}{dw} &= \\frac{1}{N}\\left(2Z^TZw - 2Z^Ty\\right) + \\frac{2\\lambda}{N}\\Gamma^T\\Gamma w\\\\\n",
    "&= \\frac{2}{N}\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)w - \\frac{2}{N}Z^Ty\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We then have\n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (a) For the in-sample predictions, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= Zw_{reg}\\\\\n",
    "&= Z\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "&= H(\\lambda)y\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (b) When $\\Gamma = Z$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &=\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^TZ + \\lambda Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left((I+\\lambda)Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= (Z^TZ)^{-1}(I+\\lambda)^{-1}Z^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}Z(Z^TZ)^{-1}Z^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}w_{lin}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is called uniform weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.17\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{E}_{in}(w) &= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left(w^T\\hat{x}_n - y_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left(w^T(x_n+\\epsilon_n) - y_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left((w^Tx_n-y_n)+w^T\\epsilon_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}(w^Tx_n-y_n)^2+2w^T\\epsilon_n(w^Tx_n-y_n) + (w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}(w^Tx_n-y_n)^2 + \\frac{1}{N}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\sum^N_{n=1}(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\sum^N_{n=1}(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}w^TE_{\\epsilon_n}[\\epsilon_n\\epsilon^T_n]w\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}w^T\\sigma^2_nIw\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}w^T\\left(\\sum^N_{n=1}\\sigma^2_nI\\right)w\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}w^T(\\sum^N_{n=1}\\sigma^2_n)w\\\\\n",
    "&= E_{in}(w) + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\lambda = 1$, and $\\Gamma = \\begin{bmatrix}\\sigma_1\\\\\\sigma_2\\\\\\dots\\\\\\sigma_N\\end{bmatrix}$\n",
    "\n",
    "So the weights $\\hat{w}_{lin}$ resulting from minimizing $\\hat{E}_{in}$ are equivalent to the weights that would have been obtained by minimizing $E_{in}$ for the observed data with Tikhonov regularization using $\\Gamma$ and $\\lambda$ as above. \n",
    "\n",
    "We can intrepret this result as: regularization enforces a robustness to potential measurement errors (noise) in the observed inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.18\n",
    "\n",
    "* (a) The regularized weights are given by\n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^T(I+\\lambda I)Z\\right)^{-1}Z^Ty\\\\\n",
    "&= Z^{-1}(I+\\lambda I)^{-1}Z^{-T}Z^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}Z^{-1}y\\\\\n",
    "&= \\frac{1}{1+\\lambda}Z^{-1}(Zw_f+\\epsilon)\\\\\n",
    "&= \\frac{1}{1+\\lambda}(w_f+Z^{-1}\\epsilon)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{reg}x]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\frac{1}{1+\\lambda}(w_f+Z^{-1}\\epsilon)^Tx\\right]\\\\\n",
    "&= \\frac{1}{1+\\lambda}E_{Z,\\epsilon}\\left[w^T_fx + \\epsilon^TZ^{-T}x\\right]\\\\\n",
    "&= \\frac{1}{1+\\lambda}\\left[E_{Z,\\epsilon}[w^T_fx] + E_{Z,\\epsilon}[\\epsilon^TZ^{-T}x]\\right]\\\\\n",
    "&= \\frac{1}{1+\\lambda}\\left[w^T_fx + E_{Z, \\epsilon}[\\epsilon^TZ^{-T}x]\\right]\\\\\n",
    "&= \\frac{1}{1+\\lambda}\\left[w^T_fx + E_{\\epsilon}[\\epsilon^T]E_{Z}[Z^{-T}x]\\right]\\\\\n",
    "&= \\frac{1}{1+\\lambda}w^T_fx\\\\\n",
    "&= \\frac{1}{1+\\lambda}f(x)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(\\frac{1}{1+\\lambda}w^T_f\\Phi(x)- w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}E_{\\mathcal{D},x}\\left[\\left(w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}E_{\\mathcal{D},x}trace\\left[\\left(w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}E_{\\mathcal{D},x}trace\\left[\\Phi^T(x)w_fw^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}E_{\\mathcal{D},x}trace\\left[\\Phi(x)\\Phi^T(x)w_fw^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}traceE_{\\mathcal{D},x}\\left[\\Phi(x)\\Phi^T(x)w_fw^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}trace\\left[E_{\\mathcal{D},x}[\\Phi(x)\\Phi^T(x)]w_fw^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}trace\\left[w_fw^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}\\|w_f\\|^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the assumption that the input probability satisfies $E_{x} \\left[\\Phi(x)\\Phi^T(x)\\right]= I $\n",
    "\n",
    "Also note, that $w_f w^T_f$ is a $(d+1)\\times (d+1)$ matrix, its trace is just $\\|w_f\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) \n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D}, x, \\epsilon}\\left[(g^{\\mathcal{D}}(x) - \\bar{g}(x))^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x, \\epsilon}\\left[\\left(w^T_{reg}\\Phi(x) - \\frac{1}{1+\\lambda}w^T_f\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x, \\epsilon}\\left[\\left(\\frac{1}{1+\\lambda}(w_f+Z^{-1}\\epsilon)^T\\Phi(x) - \\frac{1}{1+\\lambda}w^T_f\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^2} E_{\\mathcal{D}, x, \\epsilon}\\left[(\\epsilon^TZ^{-T}\\Phi(x))^2\\right]\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^2}E_{\\mathcal{D}, x, \\epsilon}\\left[\\Phi^T(x)Z^{-1}\\epsilon\\epsilon^TZ^{-T}\\Phi(x)\\right]\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^2}E_{\\mathcal{D}, x, \\epsilon}trace\\left[\\Phi^T(x)Z^{-1}\\epsilon\\epsilon^TZ^{-T}\\Phi(x)\\right]\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^2}E_{\\mathcal{D}, x, \\epsilon}trace\\left[Z^{-T}\\Phi(x)\\Phi^T(x)Z^{-1}\\epsilon\\epsilon^T\\right]\\\\\n",
    "&=\\frac{1}{(1+\\lambda)^2} trace E_{\\mathcal{D}, x, \\epsilon}\\left[Z^{-T}\\Phi(x)\\Phi^T(x)Z^{-1}\\epsilon\\epsilon^T\\right]\\\\\n",
    "&=\\frac{1}{(1+\\lambda)^2} trace E_{\\mathcal{D}, x}\\left[Z^{-T}\\Phi(x)\\Phi^T(x)Z^{-1}\\right]E_{\\epsilon}\\left[\\epsilon\\epsilon^T\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}, x}\\left[Z^{-T}\\Phi(x)\\Phi^T(x)Z^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2}  E_{\\mathcal{D}, x}trace\\left[Z^{-T}\\Phi(x)\\Phi^T(x)Z^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2}  E_{\\mathcal{D}, x}trace\\left[Z^{-1}Z^{-T}\\Phi(x)\\Phi^T(x)\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}, x}\\left[Z^{-1}Z^{-T}\\Phi(x)\\Phi^T(x)\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}}\\left[Z^{-1}Z^{-T}\\right]E_{x}\\left[\\Phi(x)\\Phi^T(x)\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}}\\left[(Z^TZ)^{-1}\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\frac{1}{N}Z^TZ = \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)$ is the in-sample estimate of $\\Sigma_{\\Phi} = E[\\Phi(x)\\Phi^T(x)] = I_{(d+1)\\times(d+1)}$, so by the law of large numbers, $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$, so $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1) = I + o(1)$.\n",
    "\n",
    "Then when $N$ is large\n",
    "\n",
    "\\begin{align*}\n",
    "var &= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}}\\left[(Z^TZ)^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace E_{\\mathcal{D}}\\left[(NI)^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} trace [\\frac{1}{N}I_{(d+1)\\times(d+1)}]\\\\\n",
    "&= \\frac{\\sigma^2}{(1+\\lambda)^2} \\frac{d+1}{N}\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (c) Combine part(a) and part(b),we have \n",
    "\n",
    "\\begin{align*}\n",
    "E[E_{out}] &= bias + var + \\sigma^2\\\\\n",
    "&= \\frac{\\lambda^2}{(1+\\lambda)^2}\\|w_f\\|^2 + \\frac{\\sigma^2}{(1+\\lambda)^2} \\frac{d+1}{N} + \\sigma^2\\\\\n",
    "&= \\frac{\\lambda^2\\|w_f\\|^2 +\\sigma^2\\frac{d+1}{N}}{(1+\\lambda)^2} + \\sigma^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.t. $\\lambda$, and set it equal to zero, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dE[E_{out}]}{d\\lambda} &= \\frac{1}{(1+\\lambda)^4} \\left(2\\|w_f\\|^2\\lambda(1+\\lambda)^2 - 2(1+\\lambda)(\\|w_f\\|^2\\lambda^2+\\sigma^2\\frac{d+1}{N})\\right)\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^4} \\left(\\|w_f\\|^2\\lambda(1+\\lambda) - (\\|w_f\\|^2\\lambda^2+\\sigma^2\\frac{d+1}{N})\\right)\\\\\n",
    "&= \\frac{1}{(1+\\lambda)^4} \\left(\\|w_f\\|^2\\lambda - \\sigma^2\\frac{d+1}{N}\\right)\\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "We find the optimal $\\lambda^* = \\frac{\\sigma^2(d+1)}{N\\|w_f\\|^2} = \\frac{\\frac{d+1}{N}}{\\frac{\\|w_f\\|^2}{\\sigma^2}}$\n",
    "\n",
    "* (d) From the optimal $\\lambda$, we see that\n",
    "  * If the model is complex, i.e. $d$ is large, we need more regularization, since it's easier to overfit, i.e. $\\lambda$ is large. \n",
    "  * More data, $N$ is large, requires less regularization\n",
    "  * With high variance of noise, it's easier to overfit, since it takes more efforts in weights to fit the noises, so need larger regularization.\n",
    "  * We can't control $w_f$, which is the true target function. But with larger $\\|w_f\\|$, it requires less regularization. Larger $\\|w_f\\|$ generally indicates more complex target function, we need reduce regularization to fit better the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.19 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.20\n",
    "\n",
    "* (a) For transformed problem, we have $Z = XA^T$\n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) &= \\frac{1}{N}\\|Zw - \\bar{y}\\|^2 \\\\\n",
    "&= \\frac{1}{N}(Zw - \\bar{y})^T(Zw - \\bar{y})\\\\\n",
    "&= \\frac{1}{N}(w^TZ^T - \\bar{y}^T)(Zw - \\bar{y})\\\\\n",
    "&= \\frac{1}{N}(w^TZ^TZw - w^TZ^T\\bar{y} - \\bar{y}^TZw + \\bar{y}^T\\bar{y})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The optimal weights are \n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{w} &= (Z^TZ)^{-1}Z^T\\bar{y}\\\\\n",
    "&= \\left((XA^T)^TXA^T\\right)^{-1}(XA^T)^T\\alpha y\\\\\n",
    "&= \\left(AX^TXA^T\\right)^{-1}AX^T\\alpha y\\\\\n",
    "&= (A^T)^{-1}(X^TX)^{-1}A^{-1}AX^T\\alpha y\\\\\n",
    "&= \\alpha(A^T)^{-1}(X^TX)^{-1}X^Ty\\\\\n",
    "&= \\alpha(A^T)^{-1}w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where we have used the fact that $A$ is invertible.\n",
    "\n",
    "* (b) The regularized solution for original data is:\n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg}(\\lambda) &= (X^TX+\\lambda X^TX)^{-1}X^Ty\\\\\n",
    "&= (X^T(1+\\lambda)X)^{-1}X^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}(X^TX)^{-1}X^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So for the transformed data, the regularized solution is: \n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{w}_{reg}(\\lambda) &= (Z^TZ+\\lambda Z^TZ)^{-1}Z^T\\bar{y}\\\\\n",
    "&=\\frac{1}{1+\\lambda}\\bar{w}\\\\\n",
    "&=\\frac{1}{1+\\lambda}\\alpha(A^T)^{-1}w\\\\\n",
    "&=\\alpha(A^T)^{-1}\\frac{1}{1+\\lambda}w\\\\\n",
    "&=\\alpha(A^T)^{-1}w_{reg}(\\lambda)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.21 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.22 \n",
    "\n",
    "For $N=100$ data points, we have $M=100$ hypothesis sets (models) $\\mathcal{H}_m$, where $m=1,2,\\dots, M$. All models have VC dimension $d = 10$. \n",
    "\n",
    "If we set aside $K=25$ points for validation and select the model $m^*$ (whose corresponding final hypothesis for this validation data is $g^-_{m^*}$), which produced minimum validation error of $E_{val}(g^-_{m^*}) = 0.25$. \n",
    "\n",
    "The out-of-sample error for this selected function $E_{out}(g^-_{m^*})$ is then bounded by equation (4.11):\n",
    "\n",
    "\\begin{align*}\n",
    "E_{out}(g^-_{m^*}) &\\le E_{val}(g^-_{m^*}) + O\\left(\\sqrt{\\frac{\\ln M}{K}}\\right)\\\\\n",
    "&= 0.25 + O\\left(\\sqrt{\\frac{\\ln 100}{25}}\\right)\\\\\n",
    "&= 0.25 + 0.4292\\\\\n",
    "&= 0.6792\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now instead we train each model on all the data and selected the function with minimum in-sample error. Our hypothesis set is actually the union of the 100 hypothesis sets. According to the bound in problem 2.14, the VC dimension of the union is bounded by:\n",
    "\n",
    "\\begin{align*}\n",
    "d_{VC}(\\mathcal{H}) &= O\\left(max(d_{VC}, K)\\log_2 max(d_{VC}, K)\\right)\\\\\n",
    "&= O\\left(max(10, 100)\\log_2 max(10, 100)\\right)\\\\\n",
    "&= O\\left(100\\log_2 100\\right)\\\\\n",
    "&\\approx 664\n",
    "\\end{align*}\n",
    "\n",
    "Then the growth function of the union is $m_{\\mathcal{H}}(N) \\le N^{d_{VC}} + 1$ according to formula (2.10).\n",
    "\n",
    "The resulting in-sample error $E_{in}(g_{m^*}) = 0.15$. The out-of-sample error in this case is then bounded by the VC generalization bound, i.e. formula (2.12) and formula (3.1):\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "E_{out}(g_{m^*}) &\\le E_{in}(g_{m^*}) + \\sqrt{\\frac{8}{N}\\ln\\frac{4m_{\\mathcal{H}}(2N)}{\\delta}}\\\\\n",
    "&\\le E_{in}(g_{m^*}) + O\\left(\\sqrt{\\frac{d_{VC}(\\mathcal{H})}{N}\\ln N}\\right)\\\\\n",
    "&= 0.15 + \\sqrt{\\frac{664}{100}\\ln 100}\\\\\n",
    "&\\approx 5.680\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is a much higher upper bound than the one obtained from validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.23\n",
    "\n",
    "* (a) $E_{cv} = \\frac{1}{N}\\sum^N_{n=1}e_n = \\frac{1}{N}\\sum^N_{n=1} e\\left(g^-_n(x_n), y_n\\right)$, then by the formula of variance-covariance: $Var(ax+by) = a^2Var(x) + b^2Var(y) + 2abCov(x,y)$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "Var_{\\mathcal{D}}[E_{cv}] &= Var_{\\mathcal{D}}\\left[\\frac{1}{N}\\sum^N_{n=1}e_n\\right]\\\\\n",
    "&= \\frac{1}{N^2}\\sum^N_{n=1}Var_{\\mathcal{D}}(e_n) + \\frac{1}{N^2}\\sum^N_{m\\ne n}Cov_{\\mathcal{D}}(e_m, e_n)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (b) \n",
    "\n",
    "For a given $n$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\mathcal{D}}[e_n] &= E_{\\mathcal{D}}[e\\left(g^-_n(x_n), y_n\\right)]\\\\\n",
    "&= E_{\\mathcal{D}}[e\\left(g^{N-2}(x_n)+\\delta_n, y_n\\right)]\\\\\n",
    "&= E_{\\mathcal{D}}[e\\left(g^{N-2}(x_n), y_n\\right) + k(\\delta_n)]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $k(\\delta_n)$ is a function in the power of $\\delta_n$. We have also used the assumption that for well behaved models, the learning process is 'stable', and so the change in the learned hypothesis should be small, $O(\\frac{1}{N})$. If a new data point $n$ is added to the data set of size $N$, so we have $g^-_n(x_n) = g^{N-2}(x_n) + \\delta_n$, we assume that any terms involving $\\delta_n$ are $O(\\frac{1}{N})$, we have $E[e\\left(g^{N-2}(x_n) + \\delta_n, y_n\\right)] = E[e(g^{N-2}(x_n), y_n)] + O(\\frac{1}{N})$\n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\mathcal{D}}[e_n] &= E_{\\mathcal{D}}[e\\left(g^{N-2}(x_n), y_n\\right)+ k(\\delta_n)]\\\\\n",
    "&= E_{\\mathcal{D}_{N-1}}E_{(x_n,y_n)}[e\\left(g^{N-2}(x_n), y_n\\right)+ k(\\delta_n)]\\\\\n",
    "&= E_{\\mathcal{D}_{N-1}}\\left[E_{out}(g^{N-2})+O(\\frac{1}{N})\\right]\\\\\n",
    "&= E_{\\mathcal{D}_{N-2}}E_{out}(g^{N-2})+O(\\frac{1}{N})\\\\\n",
    "&= \\bar{E}_{out}(g^{N-2})+O(\\frac{1}{N})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The last step is because $g^{N-2}$ only depends on $\\mathcal{D}_{N-2}$.\n",
    "\n",
    "To compute the covariance, we have\n",
    "\n",
    "\\begin{align*}\n",
    "Cov_{\\mathcal{D}}(e_m, e_n) &= E_{\\mathcal{D}}\\left[\\left(e_m-E[e_m]\\right)\\left(e_n - E[e_n]\\right)\\right]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(e_m-\\bar{E}_{out}(g^{N-2})-O(\\frac{1}{N})\\right)\\left(e_n - \\bar{E}_{out}(g^{N-2})-O(\\frac{1}{N})\\right)\\right]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)-O(\\frac{1}{N})\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)-O(\\frac{1}{N})\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)+O(\\frac{1}{N^2})\\right]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)\\right] \\\\\n",
    "&- O(\\frac{1}{N})E_{\\mathcal{D}}\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)-O(\\frac{1}{N})E_{\\mathcal{D}}\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)+O(\\frac{1}{N^2})\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)\\right] +O(\\frac{1}{N^2})\\\\\n",
    "&= E_{\\mathcal{D}^{N-2}}E_{(x_m,y_m),(x_n,y_n)}\\left[\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)\\right] +O(\\frac{1}{N^2})\\\\\n",
    "&= E_{\\mathcal{D}^{N-2}}\\left[E_{(x_m,y_m)}\\left(e_m-\\bar{E}_{out}(g^{N-2})\\right)E_{(x_n,y_n)}\\left(e_n - \\bar{E}_{out}(g^{N-2})\\right)\\right] +O(\\frac{1}{N^2})\\\\\n",
    "&= E_{\\mathcal{D}^{N-2}}\\left[\\left(E_{out}(g^{N-2})+O(\\frac{1}{N})-\\bar{E}_{out}(g^{N-2})\\right)\\left(E_{out}(g^{N-2})+O(\\frac{1}{N}) - \\bar{E}_{out}(g^{N-2})\\right)\\right]\\\\\n",
    "&= Var_{\\mathcal{D}^{N-2}}[E_{out}(g^{N-2})]+O(\\frac{1}{N^2})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) If we assume that terms involving $\\delta_n$, $\\delta_m$ are $O(\\frac{1}{N})$, then we have $Cov_{\\mathcal{D}}(e_m, e_n) \\approx  Var_{\\mathcal{D}}[E_{out}(g)]$\n",
    "\n",
    "\\begin{align*}\n",
    "Var_{\\mathcal{D}}[E_{cv}] &= \\frac{1}{N^2}\\sum^N_{n=1}Var_{\\mathcal{D}}(e_n) + \\frac{1}{N^2}\\sum^N_{m\\ne n}Cov_{\\mathcal{D}}(e_m, e_n)\\\\\n",
    "&= \\frac{1}{N}Var_{\\mathcal{D}}(e_1) + \\frac{1}{N^2}\\sum^N_{m\\ne n}Var_{\\mathcal{D}^{N-2}}[E_{out}(g^{N-2})]+O(\\frac{1}{N^2})\\\\\n",
    "&= \\frac{1}{N}Var_{\\mathcal{D}}(e_1) + (1-\\frac{1}{N})Var_{\\mathcal{D}}[E_{out}(g)]+O(\\frac{1}{N^2})\\\\\n",
    "&\\approx \\frac{1}{N}Var_{\\mathcal{D}}(e_1) +  Var_{\\mathcal{D}}[E_{out}(g)]+O(\\frac{1}{N})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since $e_1 = e(g^-_1(x_1), y_1)$, \n",
    "\n",
    "$Var_{\\mathcal{D}}[e_1] = E_{\\mathcal{D}}[e_1 -E_{\\mathcal{D}}[e_1]]^2 =  E_{\\mathcal{D}}[e_1 -\\bar{E}_{out}(g^{N-1})]^2 \\approx E_{\\mathcal{D}}[e_1 -\\bar{E}_{out}(g)]^2 $ when $N$ is large.\n",
    "\n",
    "$Var_{\\mathcal{D}}[E_{out}(g)] = E_{\\mathcal{D}}[E_{out}(g) -E_{\\mathcal{D}}[E_{out}(g)]]^2 = E_{\\mathcal{D}}[E_{out}(g) -\\bar{E}_{out}(g)]^2$\n",
    "\n",
    "\n",
    "$Var_{\\mathcal{D}}[e_1]$ doesn't decay to zero with $N$, because regardless of the value of $N$, we always evaluate $e_1$ on a single data point.\n",
    "$Var_{\\mathcal{D}}[E_{out}(g)]$ however, decays to zero, because as $N$ increases, the $g$ becomes better approximation to the target function, $E_{out}(g) = E_x[e]$ becomes closer to $\\bar{E}_{out}(g)$\n",
    "\n",
    "so $Var_{\\mathcal{D}}[E_{cv}]$ decays to 0 with $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.23 (d) TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.24 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.25\n",
    "\n",
    "* (a) We should not select the learner with minimum validation error. This is because each learner $m$ uses a different validation set, with different number of points $K_m$.  So for each learner $m$, by the VC-bound formula for a finite model with one hypothesis in it (formula 4.9), we have $E_{out}(g_m) \\le E_{in}(g_m) + O\\left(\\sqrt{\\frac{1}{K_m}}\\right)$. Lowest $E_{in}(g_m)$ doesn't necessarily end up with the lowest sum because of the $K_m$ term. \n",
    "\n",
    "\n",
    "* (b) If all models are validated on the same validation set as described in the text, it is okeay to select the learner with the lowest validation error because they have the same amount of data $K$, as from above formula, selecting the learner with lowest validation error guarantees the lowest bound.\n",
    "\n",
    "* (c) After selecting learner $m^*$, we have that \n",
    "\n",
    "\\begin{align*}\n",
    "P\\left[E_{out}(m^*) \\gt E_{val}(m^*) + \\epsilon\\right] &= P[E_{out}(m_1) \\gt E_{val}(m_1) + \\epsilon\\\\\n",
    "&\\text{or } E_{out}(m_2) \\gt E_{val}(m_2) + \\epsilon \\\\\n",
    "&\\dots\\\\\n",
    "&\\text{or } E_{out}(m_M) \\gt E_{val}(m_M) + \\epsilon]\\\\\n",
    "&\\le \\sum^M_{m=1}P[E_{out}(m) \\gt E_{val}(m) + \\epsilon]\\\\\n",
    "&\\le \\sum^M_{m=1}e^{-2\\epsilon^2K_m}\\\\\n",
    "&= Me^{-2\\epsilon^2k(\\epsilon)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $k(\\epsilon) = -\\frac{1}{2\\epsilon^2}\\ln \\left(\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K_m}\\right)$\n",
    "\n",
    "Note we can't apply VC-Bound here because of the various number of validation sets.\n",
    "\n",
    "* (d) For any $\\epsilon^*$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "P\\left[E_{out}(m^*) \\le E_{val}(m^*) + \\epsilon\\right] &= 1 - P\\left[E_{out}(m^*) \\gt E_{val}(m^*) + \\epsilon\\right] \\\\\n",
    "&\\ge 1 - Me^{-2(\\epsilon^*)^2k(\\epsilon^*)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To have a probability at least $1-\\delta$, let $Me^{-2(\\epsilon^*)^2k(\\epsilon^*)} \\le \\delta$, we have\n",
    "\n",
    "$\\epsilon^* \\ge \\sqrt{\\frac{\\ln (M/\\delta)}{2k(\\epsilon)}}$\n",
    "\n",
    "* (e) Let $K^{min} = \\min_m K_m$, we have $e^{-2\\epsilon^2K_m} \\le e^{-2\\epsilon^2K^{min}}$\n",
    "\n",
    "\\begin{align*}\n",
    "k(\\epsilon) &= -\\frac{1}{2\\epsilon^2}\\ln \\left(\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K_m}\\right)\\\\\n",
    "&\\ge -\\frac{1}{2\\epsilon^2}\\ln \\left(\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K^{min}}\\right)\\\\\n",
    "&= -\\frac{1}{2\\epsilon^2}\\ln e^{-2\\epsilon^2K^{min}}\\\\\n",
    "&= K^{min}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "On the other hand, by the Jesen's inequality of convex function, here $e^x$, we also have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K_m} &\\ge e^{\\frac{1}{M}\\sum^M_{m=1}-2\\epsilon^2K_m}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have \n",
    "\\begin{align*}\n",
    "\\ln\\left(\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K_m}\\right) &\\ge \\frac{1}{M}\\sum^M_{m=1}-2\\epsilon^2K_m\\\\\n",
    "-\\frac{1}{2\\epsilon^2}\\ln \\left(\\frac{1}{M}\\sum^M_{m=1}e^{-2\\epsilon^2K_m}\\right) &\\le \\frac{1}{M}\\sum^M_{m=1}K_m\\\\\n",
    "k(\\epsilon) &\\le \\frac{1}{M}\\sum^M_{m=1}K_m\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This bound is worse than the bound when all models use the same validation set size which equal to the average validation set size $\\frac{1}{M}\\sum^M_{m=1}K_m$ because $k(\\epsilon) \\le \\frac{1}{M}\\sum^M_{m=1}K_m$, so the bound is larger than the bound with the same validation set size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.26\n",
    "\n",
    "* (a) We have $z_n = \\Phi(x_n)$ is of dimension $d \\times 1$, then $z_nz^T_n$ has a dimension of $d\\times d$. Let $z_n = \\begin{bmatrix}z_{n1}\\\\z_{n2}\\\\\\dots\\\\z_{nd}\\end{bmatrix}$, then we have\n",
    "\n",
    "\\begin{align*}\n",
    "Z &= \\begin{bmatrix}z_1\\\\z_2\\\\\\dots\\\\z_N\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "z_{11}&z_{12}&\\dots&z_{1d}\\\\\n",
    "z_{21}&z_{22}&\\dots&z_{2d}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "z_{N1}&z_{N2}&\\dots&z_{Nd}\\\\\n",
    "\\end{bmatrix}_{N\\times d}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ &= \n",
    "\\begin{bmatrix}\n",
    "z_{11}&z_{21}&\\dots&z_{N1}\\\\\n",
    "z_{12}&z_{22}&\\dots&z_{N2}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "z_{1d}&z_{2d}&\\dots&z_{Nd}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "z_{11}&z_{12}&\\dots&z_{1d}\\\\\n",
    "z_{21}&z_{22}&\\dots&z_{2d}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "z_{N1}&z_{N2}&\\dots&z_{Nd}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\sum^N_{n=1}z_{n1}z_{n1}&\\sum^N_{n=1}z_{n1}z_{n2}&\\dots&\\sum^N_{n=1}z_{n1}z_{nd}\\\\\n",
    "\\sum^N_{n=1}z_{n2}z_{n1}&\\sum^N_{n=1}z_{n2}z_{n2}&\\dots&\\sum^N_{n=1}z_{n2}z_{nd}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "\\sum^N_{n=1}z_{nd}z_{n1}&\\sum^N_{n=1}z_{nd}z_{n2}&\\dots&\\sum^N_{n=1}z_{nd}z_{nd}\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\sum^N_{n=1}\\begin{bmatrix}\n",
    "z_{n1}z_{n1}&z_{n1}z_{n2}&\\dots&z_{n1}z_{nd}\\\\\n",
    "z_{n2}z_{n1}&z_{n2}z_{n2}&\\dots&z_{n2}z_{nd}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "z_{nd}z_{n1}&z_{nd}z_{n2}&\\dots&z_{nd}z_{nd}\\\\\n",
    "\\end{bmatrix}_{d\\times d}\\\\\n",
    "&= \\sum^N_{n=1}\\begin{bmatrix}z_{n1}\\\\z_{n2}\\\\\\dots\\\\z_{nd}\\end{bmatrix}\n",
    "\\begin{bmatrix}z_{n1}&z_{n2}&\\dots&z_{nd}\\end{bmatrix}\\\\\n",
    "&= \\sum^N_{n=1}z_nz^T_n\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $y = \\begin{bmatrix}y_1\\\\y_2\\\\\\dots\\\\y_N\\end{bmatrix}$. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Z^Ty &= \\begin{bmatrix}\n",
    "z_{11}&z_{21}&\\dots&z_{N1}\\\\\n",
    "z_{12}&z_{22}&\\dots&z_{N2}\\\\\n",
    "\\dots&\\dots&\\dots&\\dots\\\\\n",
    "z_{1d}&z_{2d}&\\dots&z_{Nd}\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}y_1\\\\y_2\\\\\\dots\\\\y_N\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\sum^N_{n=1}z_{n1}y_n\\\\\n",
    "\\sum^N_{n=1}z_{n2}y_n\\\\\n",
    "\\dots\\\\\n",
    "\\sum^N_{n=1}z_{nd}y_n\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\sum^N_{n=1}\\begin{bmatrix}\n",
    "z_{n1}y_n\\\\\n",
    "z_{n2}y_n\\\\\n",
    "\\dots\\\\\n",
    "z_{nd}y_n\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "&=\\sum^N_{n=1}z_ny_n\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now let's look at $H(\\lambda)$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z\\left( Z^TZ+\\lambda\\Gamma^T\\Gamma \\right)^{-1}Z^T\\\\\n",
    "&= ZA^{-1}(\\lambda)Z^T\\\\\n",
    "&= ZA^{-1}(\\lambda) \n",
    "\\begin{bmatrix}z_1&z_2&\\dots&z_N\\end{bmatrix}\\\\\\\\\n",
    "&= Z \n",
    "\\begin{bmatrix}A^{-1}(\\lambda)z_1&A^{-1}(\\lambda)z_2&\\dots&A^{-1}(\\lambda)z_N\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}z_1\\\\z_2\\\\\\dots&\\\\z_N\\end{bmatrix}\n",
    "\\begin{bmatrix}A^{-1}(\\lambda)z_1&A^{-1}(\\lambda)z_2&\\dots&A^{-1}(\\lambda)z_N\\end{bmatrix}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "It is easy to see that $H_{nm}(\\lambda) = z^T_nA^{-1}(\\lambda)z_m$.\n",
    "\n",
    "Clearly, we see that when $(z_n, y_n)$ is left out, we have $Z^TZ \\to Z^TZ - z_nz^T_n$ and $Z^Ty \\to Z^Ty - z_ny_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) Let's compute $w^-_n$ the weight vector learned when the $n$th data point is left out, \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "w^-_n &= \\left(Z_-^TZ_-+\\lambda \\Gamma^T\\Gamma\\right)^{-1}\n",
    "Z_-^Ty_-\\\\\n",
    "&= A_-^{-1}Z_-^Ty_-\\\\\n",
    "&= \\left(A - z_nz^T_n\\right)^{-1}(Z^Ty - z_ny_n)\\\\\n",
    "&= \\left(A^{-1} + \\frac{A^{-1}z_nz^T_nA^{-1}}{1-z^T_nA^{-1}z_n}\\right)(Z^Ty - z_ny_n)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* (c) From (a) and (b) we have\n",
    "\n",
    "\\begin{align*}\n",
    "w^-_n &= \\left(A^{-1} + \\frac{A^{-1}z_nz^T_nA^{-1}}{1-z^T_nA^{-1}z_n}\\right)(Z^Ty - z_ny_n)\\\\\n",
    "&= A^{-1}Z^Ty - A^{-1}z_ny_n + \\frac{A^{-1}z_nz^T_nA^{-1}}{1-z^T_nA^{-1}z_n}Z^Ty - \\frac{A^{-1}z_nz^T_nA^{-1}}{1-z^T_nA^{-1}z_n}z_ny_n\\\\\n",
    "&= w - A^{-1}z_ny_n + \\frac{A^{-1}z_nz^T_nA^{-1}}{1-H_{nn}}Z^Ty - \\frac{A^{-1}z_nz^T_nA^{-1}}{1-H_{nn}}z_ny_n\\\\\n",
    "&= w + A^{-1}z_n \\left[- y_n + \\frac{z^T_nA^{-1}}{1-H_{nn}}Z^Ty - \\frac{H_{nn}y_n}{1-H_{nn}}\\right]\\\\\n",
    "&= w + A^{-1}z_n \\left[\\frac{z^T_nA^{-1}}{1-H_{nn}}Z^Ty - \\frac{y_n}{1-H_{nn}}\\right]\\\\\n",
    "&= w + A^{-1}z_n \\frac{\\hat{y}_n - y_n}{1-H_{nn}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the fact \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y}_n &= H_{n}y \\\\\n",
    "&= \\begin{bmatrix}z^T_n A^{-1}z_1&z^T_n A^{-1}z_2&\\dots&z^T_n A^{-1}z_N\\end{bmatrix}y\\\\\n",
    "&= z^T_nA^{-1}Z^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (d) \n",
    "\n",
    "\\begin{align*}\n",
    "z^T_nw^-_n &= z^T_n \\left[w + A^{-1}z_n \\frac{\\hat{y}_n - y_n}{1-H_{nn}}\\right]\\\\\n",
    "&= \\hat{y}_n + z^T_nA^{-1}z_n \\frac{\\hat{y}_n - y_n}{1-H_{nn}}\\\\\n",
    "&= \\hat{y}_n + H_{nn} \\frac{\\hat{y}_n - y_n}{1-H_{nn}}\\\\\n",
    "&= \\frac{\\hat{y}_n - H_{nn}y_n}{1-H_{nn}}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (e) \n",
    "\n",
    "\\begin{align*}\n",
    "e_n &= \\left(z^T_nw^-_n - y_n\\right)^2\\\\\n",
    "&= \\left(\\frac{\\hat{y}_n - H_{nn}y_n}{1-H_{nn}} - y_n\\right)^2\\\\\n",
    "&= \\left(\\frac{\\hat{y}_n - y_n}{1-H_{nn}}\\right)^2\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.27 TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
