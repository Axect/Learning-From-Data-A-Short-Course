{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5.1\n",
    "\n",
    "#### Exercise 5.2\n",
    "\n",
    "#### Exercise 5.3\n",
    "\n",
    "The sampling bias might be caused by the size of the holes on the net. The fish that are bigger than the holes are not in the samples.\n",
    "\n",
    "#### Exercise 5.4\n",
    "\n",
    "* (a) It's not correct to use the perceptron's VC dimension for the generalization error. Because one has looked the data before selecting the perceptron model, so the actual hypothesis set is much larger than perceptron model.\n",
    "\n",
    "* (b) We don't know the $d_{VC}$ for the learning model that we actually used. As stated in problem (a), we started from a pretty big hypothesis which might include all models we are aware of. The $d_{VC}$ is not known for such a hypothesis set.\n",
    "\n",
    "#### Exercise 5.5\n",
    "\n",
    "* (a) The $M=3$ in equation (1.6) since we fixed 3 hypotheses before using the 100 examples from $\\mathcal{D}$ to select one of them. Our hypotheis set thus only has 3 hypotheses. \n",
    "\n",
    "* (b) The level of contamination is much larger if the 100 examples are used in training rather than in the final selection. Because if the 100 examples are used in training, they are reused by every hypothesis in the model (for training) while if they are used to select from final 3 hypotheses, they are only reused 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### problem 5.6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
