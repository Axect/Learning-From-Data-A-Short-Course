{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lib input sys.path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "import h5py\n",
    "from scipy.spatial import distance\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import libs.linear_models as lm\n",
    "import libs.data_util as data\n",
    "import libs.nn as nn\n",
    "import libs.plot as myplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7.1\n",
    "\n",
    "We consider the regions of $f$ which are '+' here. From the top '+' region, we have $\\bar{h}_1h_2h_3$, from the bottom left '+' region, we have $h_1h_2\\bar{h}_3$ and from the bottom right region, we have $h_1\\bar{h}_2h_3$. If either of them are True, then $f$ is True.\n",
    "\n",
    "so $f=\\bar{h}_1h_2h_3 + h_1h_2\\bar{h}_3 + h_1\\bar{h}_2h_3$\n",
    "\n",
    "#### Exercise 7.2\n",
    "\n",
    "* (a) Skipped for the graph. The Boolean $OR(x_1,\\dots,x_M)$ is similar to the $OR(x_1,x_2)$, except the weight for bias is $M-0.5$, all the weights for $x_i$ are still 1. For $AND(x_1, \\dots, x_M)$, the weight for bias is $-(M-0.5)$, while other weights are all 1.\n",
    "* (b) Skipped. The weights are $w_i$ for each $x$. \n",
    "* (c) Skipped. $\\bar{x}_2$ will take the input from $x_2$, and $x_1, \\bar{x}_2, x_3$ form the ordinary $OR$ operation, the weight on bias is 2.5\n",
    "\n",
    "#### Exercise 7.3\n",
    "\n",
    "It's straightforward to verify that the graph is consistent with $f(x)$.\n",
    "\n",
    "#### Exercise 7.4\n",
    "\n",
    "If we have $h_1(x)=\\text{sign}(w^T_1x)$, $h_2(x)=\\text{sign}(w^T_2x)$, $h_3(x)=\\text{sign}(w^T_3x)$, then we have \n",
    "$f= sign(sign(-h_1+h_2+h_3-2.5) + sign(h_1-h_2+h_3-2.5) - sign(h_1+h_2-h_3-2.5) + 2.5)$\n",
    "\n",
    "#### Exercise 7.5\n",
    "Follow the hint that for large enogh $\\alpha$, we have $sign(x)\\approx tanh(\\alpha x)$, so given $w_1$ and $\\epsilon >0$, we set $x = w^T_1x_n$, and $\\alpha = w^T_2 w^{-T}_1$, such that $\\alpha x = w^T_2x_n$. If we want the difference to be small, then we want $\\alpha = w^T_2 w^{-T}_1$ to be large enough, that is for a large enough $\\alpha$, we have $w^T_2 = \\alpha w^T_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
