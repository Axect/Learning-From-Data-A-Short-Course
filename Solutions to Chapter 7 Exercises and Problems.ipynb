{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lib input sys.path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "import h5py\n",
    "from scipy.spatial import distance\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import libs.linear_models as lm\n",
    "import libs.data_util as data\n",
    "import libs.nn as nn\n",
    "import libs.plot as myplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7.1\n",
    "\n",
    "We consider the regions of $f$ which are '+' here. From the top '+' region, we have $\\bar{h}_1h_2h_3$, from the bottom left '+' region, we have $h_1h_2\\bar{h}_3$ and from the bottom right region, we have $h_1\\bar{h}_2h_3$. If either of them are True, then $f$ is True.\n",
    "\n",
    "so $f=\\bar{h}_1h_2h_3 + h_1h_2\\bar{h}_3 + h_1\\bar{h}_2h_3$\n",
    "\n",
    "#### Exercise 7.2\n",
    "\n",
    "* (a) Skipped for the graph. The Boolean $OR(x_1,\\dots,x_M)$ is similar to the $OR(x_1,x_2)$, except the weight for bias is $M-0.5$, all the weights for $x_i$ are still 1. For $AND(x_1, \\dots, x_M)$, the weight for bias is $-(M-0.5)$, while other weights are all 1.\n",
    "* (b) Skipped. The weights are $w_i$ for each $x$. \n",
    "* (c) Skipped. $\\bar{x}_2$ will take the input from $x_2$, and $x_1, \\bar{x}_2, x_3$ form the ordinary $OR$ operation, the weight on bias is 2.5\n",
    "\n",
    "#### Exercise 7.3\n",
    "\n",
    "It's straightforward to verify that the graph is consistent with $f(x)$.\n",
    "\n",
    "#### Exercise 7.4\n",
    "\n",
    "If we have $h_1(x)=\\text{sign}(w^T_1x)$, $h_2(x)=\\text{sign}(w^T_2x)$, $h_3(x)=\\text{sign}(w^T_3x)$, then we have \n",
    "$f= sign(sign(-h_1+h_2+h_3-2.5) + sign(h_1-h_2+h_3-2.5) - sign(h_1+h_2-h_3-2.5) + 2.5)$\n",
    "\n",
    "#### Exercise 7.5\n",
    "Follow the hint that for large enogh $\\alpha$, we have $sign(x)\\approx tanh(\\alpha x)$, so given $w_1$ and $\\epsilon >0$, we set $x = w^T_1x_n$, and $\\alpha = w^T_2 w^{-T}_1$, such that $\\alpha x = w^T_2x_n$. If we want the difference to be small, then we want $\\alpha = w^T_2 w^{-T}_1$ to be large enough, that is for a large enough $\\alpha$, we have $w^T_2 = \\alpha w^T_1$\n",
    "\n",
    "#### Exercise 7.6\n",
    "\n",
    "On each layer $l$, we compute $s^{(l)} = \\left(W^{(l)}\\right)^Tx^{(l-1)}$, this takes $d^{(l)}(d^{(l-1)}+1)$ multiplications and additions. Then we compute $x^{(l)} = \\theta(s^{(l)})$, which takes $d^{(l)}$ $\\theta$-evaluations. Add them up from $l=1$ to $L$, we have a total of $O(Q)$ multiplications and additions, and $O(V)$ $\\theta$-evaluations.\n",
    "\n",
    "#### Exercise 7.7\n",
    "\n",
    "$\\tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$, we have $\\frac{d \\tanh(x)}{dx} = \\frac{4}{(e^x+e^{-x})^2} = 1 - \\tanh^2(x)$ , so we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{in}(w) &= \\nabla \\frac{1}{N}\\sum^N_{n=1}\\left(\\tanh(w^Tx_n)-y_n\\right)^2 \\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}\\nabla\\left(\\tanh(w^Tx_n)-y_n\\right)^2 \\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}2\\left(\\tanh(w^Tx_n)-y_n\\right)\\frac{d \\tanh(w^Tx_n)}{dx}  \\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}2\\left(\\tanh(w^Tx_n)-y_n\\right)\\left(1 - \\tanh^2(w^Tx_n)\\right)x_n  \\\\\n",
    "&= \\frac{2}{N}\\sum^N_{n=1}\\left(\\tanh(w^Tx_n)-y_n\\right)\\left(1 - \\tanh^2(w^Tx_n)\\right)x_n  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "If $w \\to \\infty$, $\\tanh(w^Tx_n) \\to 1$, and $\\nabla E_{in}(w)\\to 0$, so the gradient won't change when $w \\to \\infty$, which is hard to improvoe the perceptron solution.\n",
    "\n",
    "#### Exercise 7.8\n",
    "\n",
    "The weight matrices are the same as in example 7.1. For the data point $x=2$, $y=1$, we have $x^{(0)} = \\begin{bmatrix}1 \\\\ 2 \\end{bmatrix}$, $s^{(1)}$ is the same as before, $s^{(1)} = \\begin{bmatrix}0.7 \\\\ 1 \\end{bmatrix}$. \n",
    "\n",
    "So $x^{(1)} = \\begin{bmatrix}1 \\\\ 0.7 \\\\ 2 \\end{bmatrix}$, and $s^{(2)} = \\begin{bmatrix} -2.1 \\end{bmatrix}$, \n",
    "\n",
    "$x^{(2)} = \\begin{bmatrix}1 \\\\ -2.1 \\end{bmatrix}$\n",
    "\n",
    "$s^{(3)} = \\begin{bmatrix} -3.2 \\end{bmatrix}$\n",
    "\n",
    "$x^{(3)} = -3.2$\n",
    "\n",
    "\n",
    "Apply backpropagation and note that $\\theta'(s^{(l)}) = 1$ to compute \n",
    "\n",
    "$\\delta^{(3)} = 2(x^{(3)}-y) = -8.4$\n",
    "\n",
    "$\\delta^{(2)} = \\theta'(s^{(2)}) \\otimes \\left[W^{(3)}\\delta^{(3)}\\right] = -16.8$\n",
    "\n",
    "$\\delta^{(1)} = \\begin{bmatrix}-16.8 \\\\ 50.4 \\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial{e}}{\\partial{W^{(1)}}} = x^{(0)}(\\delta^{(1)})^T = \\begin{bmatrix}-16.8 & 50.4 \\\\ -33.6 & 100.8\\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial{e}}{\\partial{W^{(2)}}} = x^{(1)}(\\delta^{(2)})^T = \\begin{bmatrix}-16.8 \\\\ -11.76 \\\\ -33.6\\end{bmatrix}$\n",
    "\n",
    "$\\frac{\\partial{e}}{\\partial{W^{(3)}}} = x^{(2)}(\\delta^{(3)})^T = \\begin{bmatrix}-8.4 \\\\ -17.64 \\end{bmatrix}$\n",
    "\n",
    "\n",
    "#### Exercise 7.9 \n",
    "\n",
    "If we initialize all weights to be 0, all the inputs $s^{(l)}$ are zeros, sensitivity $\\delta^{(l)} = 0$ during backpropagation. So the gradient $G^{(l)}(x_n) = 0$ and the in-sample error $E_{in}$ is not updated through iterations.\n",
    "\n",
    "#### Exercise 7.10\n",
    "\n",
    "For layers 1 to $L$, the weights $W^{(l)}$ has a dimension of $d^{(l-1)}+1$ by $d^{(l)}$, so the total number of weight parameters are \n",
    "\n",
    "$Q = \\sum^L_{l=1}d^{(l)}(d^{(l-1)}+1)$\n",
    "\n",
    "For $L=3$ and $d^{(1)}=d^{(2)}=10$, also we assume $d^{(0)}=d$ where $d$ is the dimension of input $x$ and $d^{(3)}=1$, so $Q=131 + 10d$\n",
    "\n",
    "#### Exercise 7.11\n",
    "\n",
    "\n",
    "Take derivative w.r.t. $w^{(l)}_{ij}$ in the second term of $E_{aug}(w,\\lambda)$, we have its derivative equals to $\\frac{\\lambda}{N}\\frac{2w^{(l)}_{ij}}{\\left(1+(w^{(l)}_{ij})^2\\right)^2}$\n",
    "\n",
    "This proves the equation.\n",
    "\n",
    "We use the ratio of gradient versus weight to check the rate of decay.\n",
    "\n",
    "From the derivative, we check the ratio of the second term to the weight $w^{(l)}_{ij}$, and we have $\\frac{2\\lambda}{N}\\frac{1}{\\left(1+(w^{(l)}_{ij})^2\\right)^2}$, which achieves maximum value of 1 when $w^{(l)}_{ij} \\to 0$. So the smaller the weight, the larger the decay w.r.t. itself.\n",
    "\n",
    "This indicates that small weights decay much faster  than large ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66576144]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "W1 = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "W2 = np.array([[0.2], [1], [-3]])\n",
    "W3 = np.array([[1], [2]])\n",
    "x0 = np.array([[1], [2]])\n",
    "s1 = np.matmul(W1.transpose(), x0)\n",
    "x1 = np.array([[1], [0.60436778], [0.76159416]])\n",
    "s2 = np.matmul(W2.transpose(), x1)\n",
    "x20 = np.tanh(s2)\n",
    "x2 = np.array([[1], [-0.90154566]])\n",
    "s3 =np.matmul(W3.transpose(), x2)\n",
    "x3 = np.tanh(s3)\n",
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17.64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.1*8.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6640367702678489"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
