{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lib input sys.path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import libs.linear_models as lm\n",
    "import libs.data_util as data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.1\n",
    "\n",
    "* (a) Let $x=(0,1), x'=(0.01,1)$, then we have the Euclidean distance $d(x,x')=\\|x-x'\\| = 0.01$ and the cosine similarity $\\text{CosSim}(x,x')=\\frac{x\\cdot x'}{\\|x\\|\\|x'\\|} \\approx 0.99995$\n",
    "\n",
    "They have very high cosine similarity but very low Euclidean distance similarity.\n",
    "\n",
    "Let $x=(1,0), x'=(-1,0)$, then we have the Euclidean distance $d(x,x')=\\|x-x'\\| = 2$ and the cosine similarity $\\text{CosSim}(x,x')=\\frac{x\\cdot x'}{\\|x\\|\\|x'\\|} = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance:  0.01\n",
      "Cosine similarity:  0.9999500037496877\n"
     ]
    }
   ],
   "source": [
    "#### Exercise 6.1\n",
    "x = np.array([0,1])\n",
    "y = np.array([0.01, 1])\n",
    "\n",
    "d = np.linalg.norm(x-y)\n",
    "print('Euclidean distance: ', d)\n",
    "\n",
    "xdoty = np.dot(x,y)\n",
    "cos = xdoty/np.linalg.norm(x)/np.linalg.norm(y)\n",
    "print('Cosine similarity: ', cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.1\n",
    "\n",
    "* (b) Suppose we have $x,y$ w.r.t. the original origin. If the origin of the coordinate system changes, suppose we move the origin to $P$, then we have w.r.t. the new origin, $x'=x-P, y'=y-P$, so it's easy to see that the Euclidean distance similarity doesn't change. The cosine similarity however changes, this can be seen by checking two vectors perpendicular to each other, if we move to a new origin, they don't perpendicular to each other anymore. \n",
    "\n",
    "This puts some restriction on the choice of features, if we want to use cosine similarity, we can't change their magnitude, e.g. mean subtraction, this may affect some algorithms which may perform badly given large differences between different features. \n",
    "\n",
    "#### Exercise 6.2\n",
    "\n",
    "Given a test point $(x,y)$ (Generated from $P(x)$ and $\\pi(x)=P[y=1|x]$) and the target function $f(x)$, The probability of error on a test point $x$ is: \n",
    "\n",
    "\\begin{align*}\n",
    "e(f(x)) &=P[f(x)\\ne y] \\\\\n",
    "&= P[f(x)=1,y=-1] + P[f(x)=-1,y=1] \\\\\n",
    "&= P(y=-1)1(\\pi(x)\\ge \\frac{1}{2}) + P(y=1)1(\\pi(x) < \\frac{1}{2}) \\\\\n",
    "&= (1-\\pi(x))1(\\pi(x)\\ge \\frac{1}{2}) + \\pi(x)1(\\pi(x) < \\frac{1}{2}) \\\\\n",
    "&= \\min(\\pi(x), 1-\\pi(x))\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For any other hypothesis $h$ (deterministic or not), we apply the same calculation, and have \n",
    "\n",
    "\\begin{align*}\n",
    "e(h(x)) &=P[h(x)\\ne y] \\\\\n",
    "&= P[h(x)=1,y=-1] + P[h(x)=-1,y=1] \\\\\n",
    "&= P(y=-1)P(h(x)=1) + P(y=1)(1-P(h(x)=1)) \\\\\n",
    "&= (1-\\pi(x))P_1 + \\pi(x)(1-P_1) \\\\\n",
    "&= \\pi(x)+(1-2\\pi(x))P_1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $P_1 = P(h(x)=1)$, If $\\pi(x) \\ge \\frac{1}{2}$, then $1-2\\pi(x) \\le 0$, and $\\pi(x)+(1-2\\pi(x))P_1 \\ge \\pi(x) + (1-2\\pi(x)) = 1-\\pi(x)$, since $P_1 \\le 1$.\n",
    "\n",
    "If $\\pi(x) < \\frac{1}{2}$, then $1-2\\pi(x) > 0$, and $\\pi(x)+(1-2\\pi(x))P_1 \\ge \\pi(x)$, since $P_1 \\ge 0$. \n",
    "\n",
    "So we conclude that: $e(h(x)) \\ge e(f(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6.3 TODO\n",
    "\n",
    "#### Exercise 6.4\n",
    "\n",
    "The method will always pick $k=1$, where the $E_{in}=0$ for any data.  So the hypothesis set doesn't depend on the data. It can't find usefule $k$.\n",
    "\n",
    "#### Exercise 6.5\n",
    "\n",
    "* (a) Since we are selecting hypothesis from a fixed set of $\\mathcal{H}_{train}$, there are $N-K$ hypotheses and the validation data set is the 'input data set', which has a size of $K$. We apply the generalization bound equation (2.1), and for any $g^-_k, k=1,2,\\dots,N-K$, we have \n",
    "\n",
    "$E_{out}(g^-_k) \\le E_{val}(g^-_k) + \\sqrt{\\frac{1}{2K}\\ln\\frac{2(N-K)}{\\delta}}$\n",
    "\n",
    "If we assume $\\frac{K}{\\log(N-K)}\\to \\infty$, then we have $E_{out}(g^-_k) \\approx E_{val}(g^-_k)$\n",
    "\n",
    "Since $g^-$ is the hypothesis with minimum validation error $E_{val}(g^-)$, so we have \n",
    "\n",
    "$E_{out}(g^-) \\approx E_{val}(g^-) \\le E_{val}(g^-_*) \\approx E_{out}(g^-_*)$\n",
    "\n",
    "On the other hand, $g^-_*$ minimizes $E_{out}$, so we always have $E_{out}(g^-_*) \\le E_{out}(g^-)$\n",
    "\n",
    "Compare the two inequalities, we conclude $E_{out}(g^-) \\approx E_{out}(g^-_*)$.\n",
    "\n",
    "\n",
    "* (b) If $N-K\\to \\infty$, according to Theorem 6.2, we can find a $k(N-K)$, such that $k(N-K)\\to \\infty$ and $\\frac{k(N-K)}{N-K} \\to 0$, then we know that $E_{in}(g^-_k) \\to E_{out}(g^-_k)$ and $E_{out}(g^-_k) \\to E^*_{out}$.\n",
    "\n",
    "Since $E^*_{out}$ is the optimal out-of-sample error we can ever achieve, so we know that $E_{out}(g^-_k) \\approx E_{out}(g^-_*)$, by problem (a), we thus conclude $E_{out}(g^-) \\approx E^*_{out}$\n",
    "\n",
    "* If we used the $k^--NN$ rule on the full data set $\\mathcal{D}$, we would see performance improvement because the learning curve tells us we should use more data to achieve better performance. \n",
    "\n",
    "#### Exercise 6.6\n",
    "\n",
    "In a 10-fold cross validation, suppose we have data folds as $D_1, \\dots, D_{10}$, each fold will have $\\frac{N}{10}$ data points. For a given $k$ and a given fold $D_i$, for each point $x$ in $D_i$, we need compute its distance with all other points in $d$ dimension, which requires $O(Nd)$ computations. We then need select the smallest $k$ distances, which requires $O(N\\log k)$ computations. So for each point in fold $D_i$, we need $O(Nd+N\\log k)$ computations. \n",
    "\n",
    "We need to do the same for all $\\frac{N}{10}$ points in fold $D_i$, so the total computation for a given fold is $O(\\frac{N}{10}(Nd+N\\log k))$, with 10 folds, we have total cost of $O(N^2d+N^2\\log k)$ for a given $k$.\n",
    "\n",
    "\n",
    "Now sum up over $k$, we have the total cost\n",
    "\n",
    "$T=\\sum^{\\frac{N+1}{2}}_{j=1} O(N^2d+N^2\\log (2j-1)) = O(N^2d\\frac{N+1}{2}) + O(N^2\\sum^{\\frac{N+1}{2}}_{j=1}\\log (2j-1))$\n",
    "\n",
    "The first term is approximately on the order of $O(N^3d)$\n",
    "\n",
    "For the second term, we have $\\sum^{\\frac{N+1}{2}}_{j=1}\\log (2j-1)) \\approx \\sum^{\\frac{N+1}{2}}_{j=1}\\log (2j)) \\approx \\frac{N+1}{2}\\log 2 + \\log (\\frac{N+1}{2})!$\n",
    "\n",
    "By Sterling approximation formula, we have $\\ln n! = n\\ln n - n + O(\\ln n)$, so \n",
    "\n",
    "$\\log (\\frac{N+1}{2})! \\approx \\frac{N+1}{2}\\log \\frac{N+1}{2} - \\frac{N+1}{2} + O(\\log \\frac{N+1}{2}) = O(N\\log N - N)$\n",
    "\n",
    "Then $O(N^2\\sum^{\\frac{N+1}{2}}_{j=1}\\log (2j-1)) = O(N^2(N\\log N - N)) = O(N^3\\log N - N^3)$\n",
    "\n",
    "Adding up with the first term, we have $T=O(N^3d + N^3\\log N)$\n",
    "\n",
    "#### Exercise 6.7\n",
    "\n",
    "* (a) If $S$ is not training set consistent, and if $x_*$ is a point which is not training set consistent, so we have $g_S(x_*) \\ne g_{\\mathcal{D}}(x_*)$. According to CNN heuristic, we can find the nearest data point to $x_*$ which is not in $S$ and has class $g_{\\mathcal{D}}(x_*)$. We need to prove that such a point exists.\n",
    "\n",
    "We first fix the number of nearest neighbors to $k$, and consider the neighborhood of $N_S$ in $S$ and $N_{\\mathcal{D}}$ in $\\mathcal{D}$, both $N_S$ and $N_{\\mathcal{D}}$ have the same number of points, i.e. $k$. \n",
    "\n",
    "We say $N_S$ and $N_{\\mathcal{D}}$ are not equal to each other, otherwise, $x_*$ won't be an inconsistent point. So there must be some point that is in $N_{\\mathcal{D}}$ but not in $N_S$.\n",
    "\n",
    "Now we say that among these points that not in $S$, there must be a point $x'$ such that $g_{\\mathcal{D}}(x') = g_{\\mathcal{D}}(x_*)$. \n",
    "\n",
    "If no such point exists, then all the points in $N_{\\mathcal{D}}$ but not in $N_S$ will have the class $g_S(x_*)$. Suppose the intersection of $N_{\\mathcal{D}}$ and $N_S$ has $m$ points, then $k-m$ points in $N_{\\mathcal{D}}$ will have class $g_S(x_*)$, when it combines with the intersection of $m$ points, we will definitely find that the class of $x_*$ in $\\mathcal{D}$ to be $g_D(x_*)$ = g_S(x_*)$, because $N_S$ is composed of $k-m$ points plus the same $m$ points, but gives $x_*$ a class of $g_S(x_*)$ without requiring the $k-m$ points to be all in class $g_S(x_*)$.\n",
    "\n",
    "This is the contradiction, so we proved that the CNN heuristic will surely find a point $x'$ that is not in $S$ and has class $g_D(x_*)$$\n",
    "\n",
    "* (b) Since the set $S$ is a subset of $\\mathcal{D}$, so for any of the $k$ nearest neighbors to $x_*$ from $\\mathcal{D}$, i.e. $N_{\\mathcal{D}}$, if it belongs to $S$, it must belong to the neighbord from $S$ as well, i.e. $N_S$. So the new nearest neighbors added to $S$, will become neighbors of $x_*$ in $N_S$.\n",
    "\n",
    "\n",
    "* (c) Since there are total $N$ points, from problem (a), each step will add at most 1 point into the original $k$ points in $S$ if $S$ is still not training set consistent, and also the $N$ points is the set of $\\mathcal{D}$, which automatically satisfies the training set consistency. So after at most $N-k$ iterations, the CNN heuristic must termiate with a training set consistent $S$.\n",
    "\n",
    "#### Exercise 6.8\n",
    "\n",
    "* (a) Pseudo code for the recursive branch and bound search for the nearest neighbor.\n",
    "\n",
    "  * Given a test point $x$, we search for its nearest neighbor in the data set $D$.\n",
    "  * We look for algorithm called $F(S, x)$ to find the nearest neighbor of $x$ in a given cluster $S$. Let's initialize with the whole data set as our first cluster $\\mathcal{D}$. From clustering algorithm, we should know its two children clusters, their centers, $\\mu_1,\\mu2$, and radii $r_1,r_2$.\n",
    "  * Here is how we implement $F(S,x)$:\n",
    "     1. Set the nearest neighbor be $x_{nn} = \\text{None}$ as in python.\n",
    "     2. If the input cluster has no children cluster, i.e. it has at most 2 data points\n",
    "       * Find the nearest neighbor in the cluster by computing the distance from $x$ to all the points in the cluster. There are at most 2 computations needed since we assume every cluster with more than 2 points is split into 2 sub-clusters. \n",
    "       * Return this point as the nearest neighbor $x_{nn}$ in the input cluster to $x$\n",
    "     3. Else, suppose the two children clusters of input cluster are: $S_1, S_2$\n",
    "       * If $\\|x-\\mu_1\\|  \\le \\|x-\\mu_2\\|$: \n",
    "         * let $x_{nn} = F(S_1, x)$\n",
    "       * Else: let $x_{nn} = F(S_2, x)$\n",
    "     4. Now we have $x_{nn}$, let's call the sibling cluster of $S$ be $S^*$, let's compute the bound condition\n",
    "     5. If $\\|x-x_{nn}\\| > \\| x - \\mu^*\\| - r^*$: (Need check cluster $S^*$ for the nearest neighbor)\n",
    "       * $x^*_{nn} = F(S^*, x)$\n",
    "       * If $\\|x-x^*_{nn}\\| < \\|x-x_{nn}\\|$: $x_{nn} = x^*_{nn}$\n",
    "     6. Return $x_{nn}$\n",
    "  4. We call $F(S, x)$ with $S =\\mathcal{D}$ to get the nearest neighbor in the data set to $x$\n",
    "  \n",
    "* (b) If the sub-clusters of a cluster are balanced, and assume the number of data points is power of 2, i.e. $N=2^k$, the maximum depth of the recursive search for a nearest neighbor can be $\\log_2 N = k$\n",
    "\n",
    "* (c) If we have balanced sub-clusters and if the bound conditions always hold, then the if-condition in step 5 will be always False, so we just need to go through $\\log N$ steps, each with a computation of $d$ for distance in the $R^d$ space. The total cost is thus $d\\log N$\n",
    "\n",
    "* (d) To apply the branch and bound algorithm to $k$-nearest neighbors, we can apply the above algorithm iteratively, first, find the nearest neighbor, remove this point from the cluster, then find the nearest neighbor in the rest of points, continue this for $k$ times to find all the $k$-nearest neighbors. So the best case cost will be $kd\\log N$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
