{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lib input sys.path\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import libs.linear_models as lm\n",
    "import libs.data_util as data\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    "\n",
    "* For $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2$, the parameters are $a_0, a_1, a_2$.\n",
    "* For $h(x) \\in \\mathcal{H}_{10}$, we have $h(x) = \\sum^{10}_{k=0} a_k x^k$.\n",
    "\n",
    "For any given $h(x) \\in \\mathcal{H}_2$, we have $h(x) = a_0 + a_1x + a_2x^2 = \\sum^{10}_{k=0} a_k x^k$ where $a_k = 0$ when $k\\gt 2$. So $h(x) \\in \\mathcal{H}_{10}$ as well. We thus conclude $\\mathcal{H}_2 \\subset \\mathcal{H}_{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2\n",
    "Reading exercise.\n",
    "\n",
    "#### Exercise 4.3\n",
    "\n",
    "* (a) Consider a given $\\mathcal{H}$\n",
    "  * If the best approximation from $\\mathcal{H}$ is less complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general should increase, since it'll be harder for functions in $\\mathcal{H}$ to fit the target function. There'll be a higher tendency to overfit. \n",
    "  * If the best approximation from $\\mathcal{H}$ is more complex than the initial target function, then when we increase the complexity of $f$, the deterministic noise in general may decrease first, reducing the deterministic noise and there'll be a lower tendency to overfit. But once the complexity of $f$ exceeds the best function approximation from $\\mathcal{H}$, and if we continue increase the complexity of $f$, we will increase the deterministic noise and thus increase the tendency to overfit.\n",
    "  \n",
    "* (b) Given a fixed $f$ \n",
    "  * If the best approximation from $\\mathcal{H}$ is less complex than the target function, then when we decrease the complexity of $\\mathcal{H}$, we increase the deterministic noise thus increasing the tendency of overfit. \n",
    "  * If the best approximation from $\\mathcal{H}$ is more complex than the target function, then when we decrease the complexity of $\\mathcal{H}$, we will decrease the deterministic noise thus decreasing the tendency of overfit. Well, if we continue to decrease the complexity of $\\mathcal{H}$, passing the point where its complexity is equal to $f$, we start to increase the deministic noise again and thus increasing overfit.\n",
    "  \n",
    "#### Exercise 4.4\n",
    "\n",
    "Let's compute $E_{in}(w)$: \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) &= \\frac{1}{N}\\sum^N_{n=1}\\left(w^Tz_n - y_n\\right)^2\\\\\n",
    "&= \\frac{1}{N} \\|Zw - y\\|^2\\\\\n",
    "&= \\frac{1}{N} (Zw - y)^T(Zw - y)\\\\\n",
    "&= \\frac{1}{N} (w^TZ^T - y^T)(Zw - y)\\\\\n",
    "&= \\frac{1}{N} w^TZ^TZw - w^TZ^Ty - y^TZw + y^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "On the other hand, the equation (4.3) is\n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) &= \\frac{(w-w_{lin})^TZ^TZ(w-w_{lin})+y^T(I-H)y}{N}\\\\\n",
    "&= \\frac{(w^T-w^T_{lin})Z^TZ(w-w_{lin})+y^Ty-y^THy}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^TZw_{lin}-w^T_{lin}Z^TZw+w^T_{lin}Z^TZw_{lin}+y^Ty-y^THy}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^TZ(Z^TZ)^{-1}Z^Ty-\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZw}{N}\\\\\n",
    "&+\\frac{\\left((Z^TZ)^{-1}Z^Ty\\right)^TZ^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-T}Z^TZw}{N}\\\\\n",
    "&+\\frac{y^TZ(Z^TZ)^{-T}Z^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZ(Z^TZ)^{-1}Z^TZw}{N}\\\\\n",
    "&+\\frac{y^TZ(Z^TZ)^{-1}Z^Ty+y^Ty-y^TZ(Z^TZ)^{-1}Z^Ty}{N}\\\\\n",
    "&= \\frac{w^TZ^TZw-w^TZ^Ty-y^TZw+y^Ty}{N}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This agrees with the result derived above. So we proved equation (4.3).\n",
    "\n",
    "* (a) The value of $w$ that minimizes $E_{in}$ is $w_{lin}$, since it makes the first term $0$, which is a quadratic term on $w$ and is greater or equal to $0$ all the time. The second term doesn't depend on $w$. \n",
    "\n",
    "* (b) The minimum in-sample error when $w=w_{lin}$ is simply the second term, i.e. $\\min E_{in}(w) = \\frac{y^T(I-H)y}{N}$.\n",
    "\n",
    "#### Exercise 4.5 [Tikhonov regularizer]\n",
    "\n",
    "* (a) $\\sum^Q_{q=0}w^2_q = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix} = w^Tw$, so $\\Gamma = I$.\n",
    "\n",
    "* (b) $\\left(\\sum^Q_{q=0}w_q\\right)^2 = [w_1, w_2, \\dots, w_Q]\\begin{bmatrix}1\\\\1\\\\\\dots\\\\1\\end{bmatrix}[1,1,\\dots,1]\\begin{bmatrix}w_1\\\\ w_2\\\\ \\dots\\\\ w_Q\\end{bmatrix}$, so $\\Gamma = [1,1,\\dots,1]$.\n",
    "\n",
    "#### Exercise 4.6\n",
    "\n",
    "If we use the hard-order constraint, with less parameters, the perceptron's VC dimension decreases, and it's less likely to classify the same amount of points with more parameters. If we use the soft-order constraint, it won't change the signs of $(w^Tx)$ even when $w$ is small. So we'll still be able to classify the points.\n",
    "\n",
    "#### Exercise 4.7\n",
    "* (a) Note that the expectation w.r.t. $\\mathcal{D}_{val}$ is equivalent to $x$ because the $y$ are assumed to be generated by a true $f(x)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2_{val} &= Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right]\\\\\n",
    "&= Var_{\\mathcal{D}_{val}}\\left[\\frac{1}{K}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[Var_{\\mathcal{D}_{val}}\\sum_{x_n \\in \\mathcal{D}_{val}} e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{\\mathcal{D}_{val}} \\left[e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}} Var_{x} \\left[e\\left(g^{-}\n",
    "(x_n),y_n\\right)\\right]\\right]\\\\\n",
    "&= \\frac{1}{K^2}\\left[\\sum_{x_n \\in \\mathcal{D}_{val}}\\sigma^2(g^{-})\\right]\\\\\n",
    "&= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "* (b) In classification problem, $e\\left(g^{-}(x), y\\right) = 1(g^{-}(x) \\ne y)$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "E_x\\left[e\\left(g^{-}(x), y\\right)\\right] & = P(g^{-}(x) \\ne y)\\times 1 + P(g^{-}(x) = y)\\times 0\\\\\n",
    "& = P(g^{-}(x) \\ne y)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the variance is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2(g^{-}) &= Var_x\\left[e\\left(g^{-}(x), y\\right)\\right] \\\\\n",
    "&= E_x\\left[\\left(e - E_x[e]\\right)^2\\right]\\\\\n",
    "&= P(g^{-}(x) \\ne y)\\left[(1-E_x[e])^2\\right] + \\left(1-P(g^{-}(x) \\ne y)\\right)\\left[(0-E_x[e])^2\\right]\\\\\n",
    "&= P(1-P)^2 + (1-P)P^2\\\\\n",
    "&= P(1-P)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) In the end\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2_{val} &= \\frac{1}{K}\\sigma^2(g^{-})\\\\\n",
    "&= \\frac{P(1-P)}{K}\\\\\n",
    "&= \\frac{-(P-0.5)^2+0.25}{K}\\\\\n",
    "&\\le \\frac{1}{4K}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (d) The squared error $e\\left(g^{-}(x),y\\right)$ is unbounded. The variance of it is also unbounded. So there's no uniform upper bound for $Var_{\\mathcal{D}_{val}}\\left[E_{val}(g^{-})\\right] = \\frac{1}{K}\\sigma^2(g^{-})$. \n",
    "\n",
    "* (e) For regression with squared error, if we train using fewer points (smaller $N-K$) to get $g^{-}$, then the resulting $g^{-}$ will be worse, the expectation of the squared error $E\\left[e\\left(g^{-}(x),y\\right)\\right]$ becomes larger. For continuous, non-negative random variables, higher mean often implies higher variance, so $\\sigma^2(g^{-})$ will be higher.\n",
    "\n",
    "* (f) When we increasing the size of validation set $K$, the error between $E_{val}(g^{-})$ and $E_{out}(g^{-})$ is $\\frac{\\sigma(g^{-})}{\\sqrt{K}}$. It can drop in the case of classification. But for regression, it depends on which of $\\sigma(g^{-})$ or $K$ increases faster, so the  $E_{val}(g^{-})$ as an estimate of $E_{out}$ can become worse or better.\n",
    "\n",
    "\n",
    "* TODO\n",
    "\n",
    "Does it mean for classification the estimate will always become better when we increase the K? \n",
    "\n",
    "But note, the $E_{out}(g^{-})$ is only for the hypothesis $g^{-}$, which can be pretty bad when $K$ is large. So for classification problem, even the error between $E_{out}(g^{-})$ and $E_{val}(g^{-})$ goes to zero, but the  $E_{out}(g^{-})$ can be quite large.\n",
    "\n",
    "\n",
    "#### Exercise 4.8\n",
    "\n",
    "For each model $\\mathcal{H}_m$, $g^-_m$ is independently learned of from the validation set. When we take the expectation w.r.t. validation data set, the $g^-_m$ doesn't change, the expectation depends only on $x_n$, as in the derivation of equation (4.8). \n",
    "So I think $E_m$ is an unbiased estimate for the out-of-sample error $E_{out}(g^-_m)$.\n",
    "\n",
    "#### Exercise 4.9\n",
    "\n",
    "When $K$ increases, there are less points available to train the models, so the learned models become worse, and $E_{val}(g^-_m)$ becomes larger for each model $m$. $g^-_{m^*}$ is the model with the lowest validation error among all models, so  $E_{val}(g^-_{m^*})$ will also increases with $k$. The same logic applies to $E_{out}(g^-_{m^*})$ as well since $E_{val}(g^-_{m^*})$ is an estimate of $E_{out}(g^-_{m^*})$.\n",
    "\n",
    "When $K$ increases to certain large value, there are much less points $N-K$ available to train the models, complex models converge to simple models(TODO), so the optimistic validation error $E_{val}(g^-_{m^*})$  is closer to all validation errors $E_{val}(g^-_{m})$ for all models. Since each single validation error $E_{val}(g^-_{m})$  is an unbiased estimate of the $E_{out}(g^-_{m})$, the optimistic validation error $E_{val}(g^-_{m^*})$ converges to $E_{out}(g^-_{m^*})$ as well.\n",
    "\n",
    "#### Exercise 4.10 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.1\n",
    "\n",
    "As we can see from below graphs, when we increase the order, the graph of monomials $\\phi_i(x)$ don't seem to become more complex. This does not correspond to the intuitive notion of increasing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Problem 4.1 \n",
    "\n",
    "def monomials(x, i):\n",
    "    return np.exp(i*np.log(np.abs(x)))\n",
    "    #return np.power(x, i) #This causes overflow\n",
    "\n",
    "xs = np.arange(1,50,1)\n",
    "figsize = plt.figaspect(1)\n",
    "f, axs = plt.subplots(4, 4, figsize=figsize)\n",
    "for i in np.arange(0,16):\n",
    "    ys = monomials(xs, i)\n",
    "    k = int(i / 4)\n",
    "    j = int(i - 4*k)\n",
    "    axs[k][j].plot(xs, ys)\n",
    "    #axs[k][j].set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.2\n",
    "\n",
    "$z = \\left[L_0(x), L_1(x), L_2(x) \\right]^T = \\left[1, x, \\frac{3}{2}x^2 - \\frac{1}{2}\\right]^T$. For the hypothesis with $w=[1,-1,1]^T$, we have\n",
    "\n",
    "$h(x) = w^Tz = [1, -1, 1]\\begin{bmatrix}1\\\\ x \\\\\\frac{3}{2}x^2 - \\frac{1}{2}\\end{bmatrix} = 1-x+\\frac{3}{2}x^2 - \\frac{1}{2} = \\frac{3}{2}x^2 - x + \\frac{1}{2}$. \n",
    "\n",
    "This is degree-2 polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.3\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "L_0(x) &= 1\\\\\n",
    "L_1(x) &= x\\\\\n",
    "L_2(x) &= \\frac{3}{2}x^2 - \\frac{1}{2}\\\\\n",
    "L_3(x) &= \\frac{5}{2}x^3 - \\frac{3}{2}x\\\\\n",
    "L_4(x) &= \\frac{35}{8}x^4 - \\frac{15}{4}x^2+\\frac{3}{8}\\\\\n",
    "L_5(x) &= \\frac{63}{8}x^5 - \\frac{35}{4}x^3+\\frac{75}{40}x\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Legendre(x, K):\n",
    "    \"\"\"Compute Legendre at x up to the Kth Legendre Polynomials\n",
    "    \"\"\"\n",
    "    \n",
    "    L0, L1 = 1, x\n",
    "    if K < 0:\n",
    "        raise ValueError(\"The order of Legendre polynomial can't be negative\")\n",
    "    \n",
    "    if K == 0:\n",
    "        return [L0]\n",
    "    elif K == 1:\n",
    "        return [L0, L1]\n",
    "\n",
    "    Ls = [L0, L1]\n",
    "    Lkm1, Lkm2 = L1, L0\n",
    "    for k in range(2, K+1):\n",
    "        Lk = (2*k-1)*x*Lkm1/k - (k-1)*Lkm2/k\n",
    "        Ls.append(Lk)\n",
    "        Lkm2 = Lkm1\n",
    "        Lkm1 = Lk\n",
    "    return Ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.arange(1,50,1)\n",
    "figsize = plt.figaspect(1)\n",
    "f, axs = plt.subplots(2, 3, figsize=figsize)\n",
    "np_arrays = []\n",
    "K=6\n",
    "\n",
    "for x in xs:\n",
    "    y = np.array(Legendre(x, K))\n",
    "    np_arrays.append(y)\n",
    "ys = np.vstack(np_arrays)\n",
    "\n",
    "for i in np.arange(0,K):\n",
    "    k = int(i / 3)\n",
    "    j = int(i - 3*k)\n",
    "    axs[k][j].plot(xs, ys[:,i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.3 (b) \n",
    "\n",
    "We use induction. It's easy to see that when $k=0$ and $k=1$, $L_k(x)$ is a linear combination of monomials $x^k, x^{k-2},\\dots$.\n",
    "\n",
    "Suppose this is also true for $L_m(x)$ where $m\\le k$, then when $m=k+1$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "L_{k+1}(x) &= \\frac{2k-1}{k}xL_{k-1}(x) - \\frac{k-1}{k}L_{k-2}(x)\\\\\n",
    "&= \\frac{2k-1}{k}x\\left(a_0x^k+a_1x^{k-2}+\\dots\\right) - \\frac{k-1}{k}\\left(b_0x^{k-1}+b_1x^{k-3}+\\dots\\right)\\\\\n",
    "&= \\frac{2k-1}{k}\\left(a_0x^{k+1}+a_1x^{k-1}+\\dots\\right) - \\frac{k-1}{k}\\left(b_0x^{k-1}+b_1x^{k-3}+\\dots\\right)\\\\\n",
    "&= c_0x^{k+1}+c_1x^{k-1}+\\dots\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So $L_{k+1}(x)$ is a linear combination of monomials $x^{k+1}, x^{k-1},\\dots$. Thus\n",
    "\n",
    "\\begin{align*}\n",
    "L_k(-x) = (-1)^kL_k(x)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Problem 4.3 (c)\n",
    "\n",
    "* $k=1$, $LHS=(x^2-1)\\frac{dL_1(x)}{dx} = (x^2-1)$, and $RHS=xL_1(x) - L_0(x) = x^2 -1$. So the equation is correct when $k=1$.\n",
    "\n",
    "* Suppose for all $m\\le k$, we have $\\frac{x^2-1}{m}\\frac{dL_m(x)}{dx}=xL_m(x)-L_{m-1}(x)$.\n",
    "* When $m=k+1$, we have (To make the display simple, we use $L_k$ to represent $L_k(x)$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL_{k+1}(x)}{dx} &= \\frac{2(k+1)-1}{k+1}\\left(L_k + x\\frac{dL_k}{dx}\\right) - \\frac{k}{k+1}\\frac{dL_{k-1}}{dx}\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(L_k + x\\frac{k}{x^2-1}\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\frac{k-1}{x^2-1}\\left(xL_{k-1} - L_{k-2}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take $L_{k-2} = \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}$ into above equation we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dL_{k+1}}{dx} &= \\frac{2k+1}{k+1}\\left(L_k + x\\frac{k}{x^2-1}\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\frac{k-1}{x^2-1}\\left(xL_{k-1} - \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Multiply both sides with $x^2-1$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "(x^2-1)\\frac{dL_{k+1}}{dx} &= \\frac{2k+1}{k+1}\\left((x^2-1)L_k + kx\\left(xL_k-L_{k-1}\\right)\\right) \\\\\n",
    "&- \\frac{k(k-1)}{k+1}\\left(xL_{k-1} - \\frac{(2k-1)xL_{k-1} - kL_k}{k-1}\\right)\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(\\left((k+1)x^2-1\\right)L_k - kxL_{k-1}\\right) \\\\\n",
    "&- \\frac{k}{k+1}\\left(\\left(k-1\\right)xL_{k-1} - \\left(2k-1\\right)xL_{k-1} +kL_k\\right)\\\\\n",
    "&= \\frac{2k+1}{k+1}\\left(\\left((k+1)x^2-1\\right)L_k - kxL_{k-1}\\right)- \\frac{k^2}{k+1}\\left(L_k-xL_{k-1}\\right)\\\\\n",
    "&= \\left((2k+1)x^2-(k+1)\\right)L_k-kxL_{k-1}\\\\\n",
    "&= x\\left((2k+1)xL_k-kL_{k-1}\\right)-(k+1)L_k\\\\\n",
    "&= x(k+1)L_{k+1} - (k+1)L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So we have\n",
    "\\begin{align*}\n",
    "\\frac{x^2-1}{k+1}\\frac{dL_{k+1}}{dx} &=xL_{k+1} - L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Problem 4.3 (d) \n",
    "\n",
    "From problem 3.4(c), we have \n",
    "\\begin{align*}\n",
    "(x^2-1)\\frac{dL_{k}}{dx} &= k(xL_{k} - L_{k-1})\\\\\n",
    "(x^2-1)\\frac{dL_{k-1}}{dx} &= (k-1)(xL_{k-1} - L_{k-2})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.t $x$ on $\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}&= \\frac{d}{dx}\\left(kxL_k - kL_{k-1}\\right)  \\\\\n",
    "&= k\\left(L_k + x\\frac{dL_k}{dx}\\right) - k\\frac{dL_{k-1}}{dx}\\\\\n",
    "&= kL_k + k\\left(x\\frac{dL_k}{dx} - \\frac{dL_{k-1}}{dx}\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Replace the above computed $\\frac{dL_{k}}{dx}$ and $\\frac{dL_{k-1}}{dx}$, we obtain\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{dx}(x^2-1)\\frac{dL_k}{dx}&= kL_k + k\\left(x\\frac{dL_k}{dx} - \\frac{dL_{k-1}}{dx}\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(xk(xL_{k} - L_{k-1}) - (k-1)(xL_{k-1} - L_{k-2})\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(kx^2L_{k} - \\left((2k-1)xL_{k-1} - (k-1)L_{k-2}\\right)\\right)\\\\\n",
    "&= kL_k + \\frac{k}{x^2-1}\\left(kx^2L_{k} - kL_k\\right)\\\\\n",
    "&= kL_k + k^2L_k\\\\\n",
    "&= k(k+1)L_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (e) It doesn't hurt if we can prove that the equation is correct for $l\\le k$ for any given $k$. We use induction to prove this.\n",
    "  * When $k=0$, $l$ can only be $0$, so we have $\\int^1_{-1}dxL_0L_0 = \\int^1_{-1}dx = 2 = \\frac{2}{2k+1}$. The equation is thus correct when $k=0$.\n",
    "  * Assume that the equation is correct when $k\\le K$, let's look at the case when $k = K+1$.\n",
    "  \n",
    "\\begin{align*}\n",
    "L_{K+1}L_l &= \\frac{2K+1}{K+1}xL_{K}L_l - \\frac{K}{K+1}L_{K-1}L_l\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We need work out the integration of $\\int^1_{-1}xL_{K}L_ldx$. Let's do it.\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_ldx &= \\frac{1}{2}(x^2-1)L_KL_l|^1_{-1} - \\int^1_{-1}\\frac{1}{2}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\\\\\n",
    "&= - \\int^1_{-1}\\frac{1}{2}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\n",
    "\\end{align*}\n",
    "\n",
    "Plug the result of problem (c), i.e. $\\frac{x^2-1}{k}\\frac{dL_{k}}{dx} =xL_{k} - L_{k-1}$ for $k=K$ and $k=l$ into above equation, we have\n",
    "\n",
    "\\begin{align*}\n",
    "-2\\int^1_{-1}xL_{K}L_ldx &= \\int^1_{-1}(x^2-1)\\left(L_KdL_l + L_ldL_K\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(L_K(x^2-1)dL_l + L_l(x^2-1)dL_K\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(L_K\\left(lxL_l-lL_{l-1}\\right) + L_l\\left(KxL_K-KL_{K-1}\\right)\\right)dx\\\\\n",
    "&= \\int^1_{-1}\\left(\\left(l + K\\right)xL_KL_l -\\left(lL_KL_{l-1} + KL_lL_{K-1}\\right)\\right)dx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Move the term with $xL_KL_l$ to the same side, we have\n",
    "\\begin{align*}\n",
    "(2+K+l)\\int^1_{-1}xL_{K}L_ldx &= \\int^1_{-1}\\left(lL_KL_{l-1} + KL_lL_{K-1}\\right)dx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let's consider different cases of different $l$ values to compute the $\\int^1_{-1}xL_{K}L_ldx$.\n",
    "\n",
    "  * If $l \\lt K-1$, then clearly by our assumptions using induction, we have $\\int^1_{-1}xL_{K}L_ldx = 0$. Plug into the formula for $\\int^1_{-1}L_{K+1}L_ldx$ we have \n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_ldx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_l - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_l\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K-1$\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_{K-1}dx &= \\frac{1}{2K+1}\\int^1_{-1}\\left((K-1)L_KL_{K-2} + KL^2_{K-1}\\right)dx\\\\\n",
    "&= \\frac{1}{2K+1}\\int^1_{-1}KL^2_{K-1}dx\\\\\n",
    "&= \\frac{2K}{(2K-1)(2K+1)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_{K-1}dx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_{K-1} - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_{K-1}\\\\\n",
    "&= \\frac{2K+1}{K+1}\\frac{2K}{(2K-1)(2K+1)} - \\frac{K}{K+1}\\frac{2}{2K-1}\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K$\n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_Kdx &= \\frac{1}{2(K+1)} \\int^1_{-1}\\left(KL_KL_{K-1} + KL_KL_{K-1}\\right)dx\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_Kdx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_K - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_K\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * If $l = K+1$\n",
    "  \n",
    "\\begin{align*}\n",
    "\\int^1_{-1}xL_{K}L_{K+1}dx &= \\frac{1}{2K+3} \\int^1_{-1}\\left((K+1)L_KL_K + KL_{K+1}L_{K-1}\\right)dx\\\\\n",
    "&= \\frac{K+1}{2K+3}\\frac{2}{2K+1}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\int^1_{-1}L_{K+1}L_{K+1}dx &= \\frac{2K+1}{K+1}\\int^1_{-1}xL_{K}L_{K+1} - \\frac{K}{K+1}\\int^1_{-1}L_{K-1}L_{K+1}\\\\\n",
    "&= \\frac{2K+1}{K+1}\\frac{K+1}{2K+3}\\frac{2}{2K+1}\\\\\n",
    "&= \\frac{2}{2K+3}\\\\\n",
    "&= \\frac{2}{2(K+1)+1}\\\\\n",
    "\\end{align*}  \n",
    "\n",
    "Where the seond term on the right hand side is $0$ because we just proved when $l =K-1$.\n",
    "\n",
    "We now conclude that the equation is correct for all $k$ and $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.4 TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.5\n",
    "\n",
    "If $\\lambda \\lt 0$, since $w^Tw$ is positive, to minimize the $E_{aug}(w)$, the learning algorithm will try to increase $|w|$ as much as possible. Then this corresponds to a soft order constraint of $w^Tw =\\infty$.\n",
    "\n",
    "#### Problem 4.6\n",
    "\n",
    "For $\\Gamma = I$ and $\\lambda \\gt 0$. \n",
    "* (a) Assume we have $\\|w_{reg}\\| \\gt \\|w_{lin}\\|$, then $w^T_{reg}w_{reg}=\\|w_{reg}\\|^2 \\gt \\|w_{lin}\\|^2 =w^T_{lin}w_{lin} $\n",
    "\n",
    "\\begin{align*}\n",
    "E_{aug}(w_{reg}) &= E_{in}(w_{reg}) + \\lambda w^T_{reg}w_{reg}\\\\\n",
    "&\\gt E_{in}(w_{reg}) + \\lambda w^T_{lin}w_{lin}\\\\\n",
    "&\\ge E_{in}(w_{lin}) + \\lambda w^T_{lin}w_{lin}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The last step is valid because $w_{lin}$ minimizes $E_{in}(w)$, so we have $E_{in}(w_{lin}) \\le E_{in}(w_{reg})$.\n",
    "\n",
    "so we have $E_{aug}(w_{reg}) \\gt E_{aug}(w_{lin})$. This is contradictary though because $w_{reg}$ is assumed to minimize $E_{aug}(w)$.\n",
    "\n",
    "So we must have $\\|w_{reg}\\| \\le \\|w_{lin}\\|$\n",
    "\n",
    "* (b) For linear models, we have $w_{reg} = \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty$, then \n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}w_{reg} &= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-T} \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-1} \\left(Z^TZ+\\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= y^TZ\\left(Z^TZ+\\lambda I\\right)^{-2} Z^Ty\\\\\n",
    "&= u^T\\left(Z^TZ+\\lambda I\\right)^{-2} u\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $u=Z^Ty$.\n",
    "\n",
    "Now we show that $Z^TZ+\\lambda I$ has the same eigenvectors with correspondingly larger eigenvalues as $Z^TZ$. \n",
    "\n",
    "Assume $v$ is an eigenvector of $Z^TZ$ and $\\gamma$ is the corresponding eigenvalue, then we have \n",
    "$Z^TZv=\\gamma v$.\n",
    "\n",
    "\\begin{align*}\n",
    "(Z^TZ+\\lambda I)v &= Z^TZv + lambda I v\\\\\n",
    "&= \\gamma v + \\lambda v\\\\\n",
    "&= (\\gamma + \\lambda)v\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So $v$ is also an eigenvector of $Z^Z+\\lambda I$ and the corresponding eigenvalue is $\\gamma + \\lambda \\gt \\gamma$ since $\\lambda \\gt 0$. \n",
    "\n",
    "Also note, for any matrix $A$, the eigenvectors of $A^{-1}$ are the same as the eigenvectors of $A$, as $A^{-1}v = \\lambda^{-1}v$. The eigenvalues of $A^{-1}$ are inverse of eigenvalues of $A$, i.e. $\\lambda^{-1}$. Similarly, the eigenvectors of $A^{-2}$ are the same as the eigenvectors of $A$, but the eigenvalues of $A^{-2}$ are to the $-2$ power of eigenvalues of $A$, i.e. $\\lambda^{-2}$.\n",
    "\n",
    "Suppose the $v_1,v_2,\\dots,v_{d+1}$ are the eigenvectors of $Z^TZ$ with corresponding eigenvalues of $\\lambda_1, \\lambda_2, \\dots, \\lambda_{d+1}$, then write $u$ in the eigenbasis of these vectors, we have $u = \\sum^{d+1}_{k=1}c_kv_k$.\n",
    "\n",
    "Let $A=Z^TZ+\\lambda I$, then we have\n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}w_{reg} &= u^TA^{-2}u \\\\\n",
    "&= u^TA^{-2}(\\sum^{d+1}_{k=1}c_kv_k)\\\\\n",
    "&= u^T(\\sum^{d+1}_{k=1}c_kA^{-2}v_k)\\\\\n",
    "&= u^T(\\sum^{d+1}_{k=1}c_k\\lambda^{-2}_kv_k)\\\\\n",
    "&= (\\sum^{d+1}_{k=1}c_kv_k)(\\sum^{d+1}_{k=1}c_k\\lambda^{-2}_kv_k)\\\\\n",
    "&=\\sum^{d+1}_{k=1}\\lambda^{-2}_kc^2_k\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The second to last equation is because the eigenvectors are orthogonal to each other. \n",
    "\n",
    "The $\\lambda_k$s in the equation are eigenvalues of matrix $Z^TZ+\\lambda I$, the corresponding matrix for $w_{lin}$ without weight decay is $Z^TZ$, and $w_{lin}^Tw_{lin} = u^T(Z^TZ)^{-2}u = \\sum^{d+1}_{k=1}(\\lambda^s_k)^{-2}c^2_k$. Assume the eigenvalues of matrix  $Z^TZ$ are $\\lambda^s_1,\\lambda^s_2,\\dots,\\lambda^s_{d+1}$, where $\\lambda^s_k \\le \\lambda_k$ as proved above. So \n",
    "we have $w^T_{reg}w_{reg} \\le w_{lin}^Tw_{lin}$ and achieve the equality when $\\lambda = 0$.\n",
    "\n",
    "#### Problem 4.7\n",
    "\n",
    "For a matrix $Z$ that is $n\\times d$, let the SVD of $Z=U\\Gamma V^T$, where $\\Gamma$ is $d\\times d$ square diagnoal matrix, and $r$ of its diagonal elements are positive with the rest zeroes. $U$ is $n \\times d$ and $V$ is $d \\times d$. Also we have $U^TU=I_{d\\times d}$ and $V^TV = VV^T = I_{d\\times d}$. Note that $UU^T$ is not identity matrix. Let $Z^TZ$ have eigenvalues $\\sigma^2_1, \\sigma^2_2, \\dots, \\sigma^2_d$, define $a=U^Ty$, so we have\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ &= V\\Gamma U^T U\\Gamma V^T\\\\\n",
    "&= V\\Gamma^2V^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the eigenvalues in $\\sigma^2_1, \\sigma^2_2, \\dots, \\sigma^2_d$ (there might be zeroes) are the elements on the diagonal of matrix $\\Gamma^2$. \n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T \\\\\n",
    "&= U\\Gamma V^T \\left(V\\Gamma^2V^T + \\lambda VV^T\\right)^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T \\left(V\\left(\\Gamma^2 + \\lambda I \\right)V^T\\right)^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T V^{-T} \\left(\\Gamma^2 + \\lambda I \\right)^{-1} V^{-1} V\\Gamma U^T\\\\\n",
    "&= U\\Gamma^2 \\left(\\Gamma^2 + \\lambda I \\right)^{-1}U^T\\\\\n",
    "&= U\\Lambda U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where the diagonal matrix $\\Lambda$ has elements of $\\frac{\\sigma^2_i}{\\sigma^2_i + \\lambda}$ on its $i$th diagnoal. This is because the matrix $\\Gamma^2 + \\lambda I$ is always invertible when $\\lambda \\gt 0$.\n",
    "\n",
    "Let $\\lambda = 0$, we have $H= U\\Gamma V^T \\left(V\\Gamma^2V^T \\right)^{-1} V\\Gamma U^T$, since $\\Gamma$ may not have inverse, since some of its diagnoal entries might be zero. For its inverse, let the non-zero entries to be inverse, and let the zero entries to keep zeroes. We have $H= UU^T$.\n",
    "\n",
    "Now it's time to compute $E_{in}(w_{reg})$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w_{reg}) - E_{in}(w_{lin}) &= \\frac{1}{N}y^T(I-H(\\lambda))^2y - \\frac{1}{N}y^T(I-H)^2y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[(I-H(\\lambda))^2 - (I-H)^2\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[(I-H(\\lambda))^T(I-H(\\lambda)) - (I-H)\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[I-2H(\\lambda)+H(\\lambda)^2 - (I-H)\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[-2H(\\lambda)+H(\\lambda)^2 + H\\right]y\\\\\n",
    "&= \\frac{1}{N}y^T\\left[-2U\\Lambda U^T+ U\\Lambda^2 U^T + UU^T\\right]y\\\\\n",
    "&= \\frac{1}{N}y^TU\\left[-2\\Lambda + \\Lambda^2  + I\\right]U^Ty\\\\\n",
    "&= \\frac{1}{N}a^T(I-\\Lambda)^2a\\\\\n",
    "&=\\frac{1}{N}\\sum^d_{i=1}a^2_i\\left(1-\\frac{\\sigma^2_i}{\\sigma^2_i + \\lambda}\\right)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We thus finished the proof.\n",
    "\n",
    "#### Problem 4.8\n",
    "\n",
    "In the augmented error minimization with $\\Gamma = I$ and $\\lambda \\gt 0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{aug}(w) &= E_{in}(w) + \\lambda w^T\\Gamma^T\\Gamma w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.t. $w$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{aug}(w) &= \\nabla E_{in}(w) + \\lambda \\left(2\\Gamma^T\\Gamma\\right)w\\\\\n",
    "&= \\nabla E_{in}(w) + 2\\lambda w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then the update rule is \n",
    "\n",
    "\\begin{align*}\n",
    "w(t+1) &= w(t) - \\eta \\nabla E_{aug}(w)\\\\\n",
    "&= w(t) - \\eta \\left(\\nabla E_{in}(w) + 2\\lambda w\\right)\\\\\n",
    "&= (1-2\\eta\\lambda)w(t) -  \\eta \\nabla E_{in}(w)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is the weight decay rule: $w(t)$ decays before being updated by the gradient of $E_{in}(w)$.\n",
    "\n",
    "#### Problem 4.9\n",
    "\n",
    "* (a) $\\sqrt{\\lambda}\\Gamma$ has a dimension of $k \\times (d+1)$, while $Z$ has a dimension of $n\\times (d+1)$. $Z_{aug} = \\begin{bmatrix}Z\\\\ \\sqrt{\\lambda}\\Gamma\\end{bmatrix}$ and $y=\\begin{bmatrix}y\\\\ 0\\end{bmatrix}$.\n",
    "\n",
    "We also have $Z^T_{aug} = \\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix}$\n",
    "\n",
    "* (b) The least square solution with augmented data is\n",
    "\n",
    "\\begin{align*}\n",
    "\\left(Z^T_{aug}Z_{aug}\\right)^{-1}Z^T_{aug}y_{aug} &= \\left(\\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix} \\begin{bmatrix}Z\\\\ \\sqrt{\\lambda}\\Gamma\\end{bmatrix}\\right)^{-1} \\begin{bmatrix}Z^T&\\sqrt{\\lambda}\\Gamma^T\\end{bmatrix}\\begin{bmatrix}y\\\\ 0\\end{bmatrix}\\\\\n",
    "&= \\left(Z^TZ + \\lambda\\Gamma^T\\Gamma\\right)^{-1} Z^Ty\\\\\n",
    "&= w_{reg}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Problem 4.10\n",
    "\n",
    "* (a) If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, then $w_{reg} = w_{lin}$\n",
    "* (b) $E_{in}(w) = \\frac{1}{N}\\sum^N_{i=1}\\left(y_i - w^Tx_i\\right)^2$. $E_{in}(w)$ is thus a quadratic function of $w$, so the contours of constant $E_{in}$ are ellipsoids.  In this case, $w^T_{reg}\\Gamma^T\\Gamma w_{reg} = C$. First notice that the region $w^T\\Gamma^T\\Gamma w \\le C$ is convex. When $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, $w_{lin}$ is outside of this region, as we move away from $w_{lin}$, the $E_{in}(w)$ increases (because $w_{lin}$ minimizes $E_{in}(w)$) but $w^T\\Gamma^T\\Gamma w$ decreases. The $w$ that satisfies $w^T\\Gamma^T\\Gamma w =C$ has the minimum possible $E_{in}(w)$ under constraint. So this is the solution $w_{reg}$.\n",
    "\n",
    "* (c) From part (b), consider that case when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, we have $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$, let's replace the inequality constraint to equality constraint. \n",
    "\n",
    "Use Lagrange multiplier, we can instead minimize \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in}(w) + \\lambda \\left(w^T\\Gamma^T\\Gamma w - C\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take derivative w.r.r. $w$, and let it equal to $0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{in}(w_{reg}) + 2\\lambda_C \\Gamma^T\\Gamma w_{reg} &= 0\\\\\n",
    "\\Gamma^T\\Gamma w_{reg} &= -\\frac{1}{2\\lambda_C}\\nabla E_{in}(w_{reg}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Since we know that $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$, (or take derivative w.r.t. $\\lambda$), we have \n",
    "\n",
    "\\begin{align*}\n",
    "C &= -\\frac{1}{2\\lambda_C}w^T_{reg}\\nabla E_{in}(w_{reg})\\\\\n",
    "\\lambda_C &= -\\frac{1}{2C}w^T_{reg}\\nabla E_{in}(w_{reg})\n",
    "\\end{align*}\n",
    "\n",
    "Also note that when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, $w_{reg} = w_{lin}$, we have $\\nabla E_{in}(w_{reg})= 0$, thus $\\lambda_C = 0$.\n",
    "\n",
    "Combine above results, we see that $w_{reg}$ minimizes $E_{in}(w) + \\lambda_Cw^T\\Gamma^T\\Gamma w$. \n",
    "\n",
    "* (d) \n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\le C$, we have $w_{reg} = w_{lin}$, and $\\lambda_C = 0$.\n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, Consider the norm to the surface of $w^T\\Gamma^T\\Gamma w = C$, it's $2\\Gamma^T\\Gamma w$.\n",
    "\n",
    "We claim that the norm and the graident of $E_{in}$ have to be in the opposite directions at the point $w_{reg}$, which minimizes the augmented error. \n",
    "\n",
    "To prove this is correct, Let's look at the projection of $\\nabla E_{in}(w)$ onto the norm. If this projection is along the same direction as norm, then we can move along the opposite of this projection, which will decrease $E_{in}(w)$ (because we move along some component of $-\\nabla E_{in}(w))$ and decrease $w^T\\Gamma^T\\Gamma w$ (it's also the opposite of norm), which will move to region of $w^T\\Gamma^T\\Gamma w \\lt C$. This contradicts our conclusion in part(b) that  the optimal $w_{reg}$ satisfies $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$. So the projection of $\\nabla E_{in}(w)$ must be at the opposite of the norm. \n",
    "\n",
    "Now suppose that in addition to the projection component, there's a non-zero component of $\\nabla E_{in}(w)$ at $w_{reg}$ that is perpendicular to the norm. If we move along the opposite of this component, we stay on the surface, but we'll decrease the $E_{in}$, this contradicts with the assumption that $w_{reg}$ minimizes the agumented error.\n",
    "\n",
    "Now we proved that the norm at $w_{reg}$, i.e. $\\Gamma^T\\Gamma w_{reg}$ is in opposite direction of the gradient of $E_{in}$ at $w_{reg}$, i.e. $\\nabla E_{in}(w_{reg})$. From the derivation of part (c) we have following equation\n",
    "  \n",
    "\\begin{align*}\n",
    "\\Gamma^T\\Gamma w_{reg} &= -\\frac{1}{2\\lambda_C}\\nabla E_{in}(w_{reg}) \\\\\n",
    "\\end{align*} \n",
    "\n",
    "This shows that we must have $\\lambda_C \\gt 0$.\n",
    "\n",
    "  * If $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$. \n",
    "\n",
    "Note that $w_{reg}$ depends on $C$. We first work out $\\frac{d\\nabla E_{in}(w_{reg})}{dC}$. We know that $E_{in}(w_{reg}) = \\frac{1}{N}\\|Xw-Y\\|^2 = \\frac{1}{N}(Xw-Y)^T(Xw-Y) = \\frac{1}{N}(w^TX^TXw - w^TX^Ty - y^TX^Tw + y^Ty)$\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E_{in}(w_{reg}) &=  \\frac{2}{N}\\left(X^TXw -X^Ty\\right)\\\\\n",
    "w^T_{reg}\\nabla E_{in}(w_{reg}) &=  \\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right)\\\\\n",
    "\\frac{d\\left(w^T_{reg}\\nabla E_{in}(w_{reg})\\right)}{dC} &= \\frac{2}{N}\\frac{dw}{dC}|_{w_{reg}}\\left(2X^TXw_{reg} -X^Ty\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also take derivative w.r.t. $C$ on surface $w^T\\Gamma^T\\Gamma w = C$, we have $2\\Gamma^T\\Gamma w \\frac{dw}{dC} = 1$, at $w = w_{reg}$, we have $w_{reg}^T\\Gamma^T\\Gamma w_{reg} = C$ when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$, that is $2w_{reg}^T\\Gamma^T\\Gamma w_{reg}\\frac{dw}{dC}|_{w_{reg}}=1$, we obtain\n",
    "$\\frac{dw}{dC}|_{w_{reg}} = \\frac{1}{2C}w^T_{reg}$.\n",
    "\n",
    "\n",
    "Let's now take derivative of $\\lambda_C$ w.r.t. $C$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d\\lambda_C}{dC} &= \\frac{1}{2C^2}w^T_{reg}\\nabla E_{in}(w_{reg}) -\\frac{1}{2C}\\frac{d\\left(w^T_{reg}\\nabla E_{in}(w_{reg})\\right)}{dC} \\\\\n",
    "&= \\frac{1}{2C^2}\\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{2C}\\frac{2}{N}\\frac{dw}{dC}|_{w_{reg}}\\left(2X^TXw_{reg} -X^Ty\\right) \\\\\n",
    "&= \\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{2C}\\frac{2}{N}\\frac{1}{2C}w^T_{reg}\\left(2X^TXw_{reg} -X^Ty\\right) \\\\\n",
    "&= \\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right) -\\frac{1}{NC^2}\\left(w^T_{reg}X^TXw_{reg} - \\frac{1}{2}w^T_{reg}X^Ty\\right) \\\\\n",
    "&= -\\frac{1}{2NC^2}w^T_{reg}X^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "From previous derivation, $\\lambda_C = -\\frac{1}{2C}w^T_{reg}\\nabla E_{in}(w_{reg})$, we know that $\\lambda_C \\gt 0$, so we have  $w^T_{reg}\\nabla E_{in}(w_{reg}) \\lt 0$ because $C \\gt 0$. \n",
    "\n",
    "\\begin{align*}\n",
    "w^T_{reg}\\nabla E_{in}(w_{reg}) &= \\frac{2}{N}\\left(w^T_{reg}X^TXw_{reg} - w^T_{reg}X^Ty\\right)\\\\\n",
    "w^T_{reg}X^TXw_{reg} &\\lt w^T_{reg}X^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then\n",
    "\\begin{align*}\n",
    "\\frac{d\\lambda_C}{dC} &= -\\frac{1}{2NC^2}w^T_{reg}X^Ty\\\\\n",
    "&\\lt -\\frac{1}{2NC^2}w^T_{reg}X^TXw_{reg}\\\\\n",
    "&\\le 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where the last inequation comes from the fact that $X^TX$ is positive semi-definite matrix. \n",
    "\n",
    "  * Proof: For any non-zero vector $v$, $v^TX^TXv = (Xv)^T(Xv) \\ge 0$ since $Xv$ is a vector.\n",
    "  \n",
    "So we have proved that $\\frac{d\\lambda_C}{dC} \\lt 0$, $\\lambda_C$ is a strictly decreasing function of $C$ when $w^T_{lin}\\Gamma^T\\Gamma w_{lin} \\gt C$.\n",
    "\n",
    "#### Problem 4.11\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "w_{lin} &= (Z^TZ)^{-1}Z^Ty\\\\\n",
    "&= (Z^TZ)^{-1}Z^T(Zw_f+\\epsilon)\\\\\n",
    "&= (Z^TZ)^{-1}Z^TZw_f+ (Z^TZ)^{-1}Z^T\\epsilon\\\\\n",
    "&= w_f+ (Z^TZ)^{-1}Z^T\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{lin}x]\\\\\n",
    "&= E_{\\mathcal{D}}[\\left(w_f+ (Z^TZ)^{-1}Z^T\\epsilon\\right)^Tx]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx + \\epsilon^T Z(Z^TZ)^{-T} x]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx] + E_{Z,\\epsilon}[\\epsilon^T Z(Z^TZ)^{-1} x]\\\\\n",
    "&= w^T_fx + E_{Z,\\epsilon}[\\epsilon^T]E_{Z,\\epsilon}[Z(Z^TZ)^{-1}]x\\\\\n",
    "&= w^T_fx\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the average function $\\bar{g}(x)$ is exactly the target function $f(x)$, i.e. $\\bar{g}(x) = f(x)$. \n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}[\\left(w^T_fx - w^T_fx\\right)^2]\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (b) Now we can compute variance\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D},x}\\left[\\left(g^{\\mathcal{D}}(x)-\\bar{g}(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(w^T_{lin}\\Phi(x)-w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(((Z^TZ)^{-1}Z^T\\epsilon)^T\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)^T\\left(\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)\\right]\\\\\n",
    "&= E_{Z, \\epsilon,x}\\left[\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[trace\\left(\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T Z(Z^TZ)^{-1}\\Phi(x)\\right)\\right]\\\\\n",
    "&= E_{Z, x}E_{\\epsilon}\\left[trace\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= E_{Z, x}trace\\left[E_{\\epsilon}\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= E_{Z, x}trace\\left[E_{\\epsilon}\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\right)E_{\\epsilon}\\left(\\epsilon\\epsilon^T \\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left(Z(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ)^{-1}Z^T\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left((Z^TZ)^{-1}Z^TZ(Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z, x}trace\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}E_xtrace\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}traceE_x\\left[\\left((Z^TZ)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\right]\\\\\n",
    "&= \\sigma^2E_{Z}trace\\left[E_x\\left[(Z^TZ)^{-1}\\right]E_x\\left[\\Phi(x)\\Phi^T(x)\\right]\\right]\\\\\n",
    "&= \\sigma^2E_{Z}trace\\left[(Z^TZ)^{-1}\\Sigma_{\\Phi}\\right]\\\\\n",
    "&= \\sigma^2trace\\left[\\Sigma_{\\Phi}E_{Z}(Z^TZ)^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}E_{Z}(\\frac{1}{N}Z^TZ)^{-1}\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{N}Z^TZ &= \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is the in-sample estimate of $\\Sigma_{\\Phi}$, by the law of large numbers, we have $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$. The $\\Sigma_{\\Phi}$ has a dimension of $(Q+1)\\times (Q+1)$ for a given $x$, so we have\n",
    "\n",
    "\\begin{align*}\n",
    "var &= \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}E_{Z}(\\frac{1}{N}Z^TZ)^{-1}\\right]\\\\\n",
    "&\\approx \\frac{\\sigma^2}{N}trace\\left[\\Sigma_{\\Phi}(\\Sigma_{\\Phi}+o(1))^{-1}\\right]\\\\\n",
    "&= \\frac{\\sigma^2}{N}traceI_{(Q+1)\\times (Q+1)}\\\\\n",
    "&= \\frac{\\sigma^2 (Q+1)}{N}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So for the well specified linear model, the bias is zero and the variance is increasing as the model gets larger (i.e. $Q$ increases) but decreasing in $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.12\n",
    "\n",
    "* (a) \n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\left(Zw_f+\\epsilon\\right)\\\\\n",
    "&= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^TZw_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= \\left(I - \\left(Z^TZ + \\lambda I\\right)^{-1}\\lambda \\right)w_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= w_f - \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{reg}x]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(w_f- \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\right)^Tx\\right]\\\\\n",
    "&= E_{Z,\\epsilon}\\left[w^T_fx - \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x + \\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x\\right]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx- \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x] + E_{Z,\\epsilon}[\\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x]\\\\\n",
    "&= w^T_fx - \\lambda E_Z\\left[ w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}\\right]x + E_{Z,\\epsilon}[\\epsilon^T]E_{Z,\\epsilon}[Z(Z^TZ+ \\lambda I)^{-1}]x\\\\\n",
    "&= w^T_fx- \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}x\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\frac{1}{N}Z^TZ = \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)$ is the in-sample estimate of $\\Sigma_{\\Phi} = E[\\Phi(x)\\Phi^T(x)] = I_{(d+1)\\times(d+1)}$, so by the law of large numbers, $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$, so $\\frac{1}{N}(Z^TZ + \\lambda I) = \\Sigma_{\\Phi} + \\frac{1}{N}\\lambda I + o(1) = (1+\\frac{\\lambda}{N})I + o(1)$.\n",
    "\n",
    "Then when $N$ is large\n",
    "\n",
    "\\begin{align*}\n",
    "E_Z\\left(Z^TZ + \\lambda I\\right)^{-1} &= \\frac{1}{N}E_Z\\left(\\frac{1}{N}(Z^TZ + \\lambda I)\\right)^{-1} \\\\\n",
    "&= \\frac{1}{N}(1+\\frac{\\lambda}{N})^{-1}I \\\\\n",
    "&= \\frac{1}{\\lambda + N}I \\\\\n",
    "\\end{align*}\n",
    "\n",
    " \n",
    "\n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)- w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(\\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)E_Z\\left(Z^TZ + \\lambda I\\right)^{-T}w_f w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)\\frac{1}{\\lambda + N}I w_f w^T_f\\frac{1}{\\lambda + N}I \\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} E_{\\mathcal{D},x}\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace E_{\\mathcal{D},x}\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace \\left[E_{\\mathcal{D},x}[\\Phi(x)\\Phi^T(x)]w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [Iw_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [w_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} \\|w_f\\|^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the assumption that the input probability satisfies $E_{x} \\left[\\Phi(x)\\Phi^T(x)\\right]= I $\n",
    "\n",
    "Also note, that $w_f w^T_f$ is a $(d+1)\\times (d+1)$ matrix, its trace is just $\\|w_f\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.12 (b)\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D}, x}\\left[(g^{\\mathcal{D}}(x) - \\bar{g}(x))^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left(w^T_{reg}\\Phi(x) - (w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left((w^T_f-w^T_f\\lambda(Z^TZ+\\lambda I)^{-1}+\\epsilon^TZ(Z^TZ+\\lambda I)^{-1})\\Phi(x) - (w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left[\\left((-w^T_f\\lambda(Z^TZ+\\lambda I)^{-1}+\\epsilon^TZ(Z^TZ+\\lambda I)^{-1})\\Phi(x) - ( - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x) \\right)^2\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To the first oder in $\\frac{1}{N}$, we know from 4.12 (a) that $\\frac{1}{N}(Z^TZ + \\lambda I) = (1+\\frac{\\lambda}{N})I + o(1)$ and $E_Z\\left(Z^TZ + \\lambda I\\right)^{-1} = \\frac{1}{\\lambda + N}I$\n",
    "\n",
    "So we have\n",
    "\n",
    "\\begin{align*}\n",
    "var &= E_{\\mathcal{D}, x}\\left(\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)^2\\\\\n",
    "&= E_{\\mathcal{D}, x}\\left(\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)\\\\\n",
    "&= E_{\\mathcal{D}, x}trace\\left(\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\right)\\\\\n",
    "&= E_{\\mathcal{D}, x}trace\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)E_{\\mathcal{D}, x}\\left(\\epsilon\\epsilon^T\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}, x}\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}, x}trace\\left(Z(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}, x}trace\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\Phi(x)\\Phi^T(x)\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}}\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right) E_{ x}\\left(\\Phi(x)\\Phi^T(x)\\right)\\\\\n",
    "&= \\sigma^2 trace E_{\\mathcal{D}}\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}}trace\\left((Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}\\right)\\\\\n",
    "&= \\sigma^2  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "To first order in $\\frac{1}{N}$, we have $Z^TZ = NI$, take this into variance expression, we obtain\n",
    "\n",
    "\\begin{align*}\n",
    "var &= \\sigma^2  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\frac{\\sigma^2}{N}  E_{\\mathcal{D}}trace\\left(Z(Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}Z^T\\right)\\\\\n",
    "&= \\frac{\\sigma^2}{N}  E_{\\mathcal{D}}[trace(H^2(\\lambda))]\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.13\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "H^2(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^TZ(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also from exercise 3.3 (d), when $\\lambda=0$, $H(\\lambda) = H$, and we have $trace(H) = \\tilde{d}+1$, and $H^2=H$, so $trace(H^2) = \\tilde{d}+1$.\n",
    "\n",
    "* (a) When $\\lambda = 0$, we have $trace(H(\\lambda)) = trace\\left(Z(Z^TZ)^{-1}Z^T\\right) = trace(H) = \\tilde{d}+1$\n",
    "  * (i) $d_{eff}(\\lambda) = 2trace(H(\\lambda)) - trace(H^2(\\lambda)) = 2trace(H) - trace(H^2) = trace(H) = \\tilde{d} + 1$.\n",
    "  * (ii) $d_{eff}(\\lambda)= trace(H(\\lambda)) = trace(H) = \\tilde{d} + 1$\n",
    "  * (iii) $d_{eff}(\\lambda)= trace(H^2(\\lambda)) = trace(H) = \\tilde{d} + 1$\n",
    "  \n",
    "* (b) When $\\lambda \\gt 0$. Let the SVD of $Z=U\\Gamma V^T$, where $U$ is $n\\times (\\tilde{d}+1)$ matrix, $U^TU=I_{(\\tilde{d} + 1) \\times (\\tilde{d}+1)}$ and $V$ is $(\\tilde{d} + 1) \\times (\\tilde{d}+1)$, $V^TV=VV^T=I_{(\\tilde{d} + 1) \\times (\\tilde{d}+1)}$, and $\\Gamma$ is $(\\tilde{d} + 1) \\times (\\tilde{d}+1)$ diagonal matrix. We have\n",
    "\n",
    "\\begin{align*}\n",
    "Z^TZ &= V\\Gamma U^T U\\Gamma V^T\\\\\n",
    "&= V\\Gamma^2V^T\\\\\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda I)^{-1}Z^T\\\\\n",
    "&= U\\Gamma V^T\\left(V\\Gamma^2V^T+\\lambda I\\right)^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^T\\left(V(\\Gamma^2+\\lambda I)V^T\\right)^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma V^TV^{-T}(\\Gamma^2+\\lambda I)^{-1}V^{-1}V\\Gamma U^T\\\\\n",
    "&= U\\Gamma^2(\\Gamma^2+\\lambda I)^{-1}U^T\\\\\n",
    "&= U\\Sigma U^T\\\\\n",
    "H^2(\\lambda) &= (U\\Sigma U^T)^TU\\Sigma U^T\\\\\n",
    "&= U\\Sigma^2U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\Sigma = \\Gamma^2(\\Gamma^2+\\lambda I)^{-1}$, let $\\sigma_1, \\sigma_2, \\dots, \\sigma_d$ be the eigenvalues of matrix $\\Gamma$.\n",
    "\n",
    "So the traces are\n",
    "\n",
    "\\begin{align*}\n",
    "trace(H(\\lambda)) &= trace(U\\Sigma U^T) \\\\\n",
    "&= trace(U^TU\\Sigma) \\\\\n",
    "&= trace(\\Sigma)\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^2_i}{\\sigma^2_i+\\lambda}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "trace(H^2(\\lambda)) &= trace(U\\Sigma^2 U^T) \\\\\n",
    "&= trace(U^TU\\Sigma^2) \\\\\n",
    "&= trace(\\Sigma^2)\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i}{(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (i) \n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= 2trace(H(\\lambda)) - trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{2\\sigma^2_i}{\\sigma^2_i+\\lambda} - \\frac{\\sigma^4_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i+2\\lambda\\sigma^2_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (ii)\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &=trace(H(\\lambda))\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^2_i}{\\sigma^2_i+\\lambda}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "  * (iii)\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &=trace(H^2(\\lambda))\\\\\n",
    "&= \\sum^{\\tilde{d}+1}_{i=1}\\frac{\\sigma^4_i} {(\\sigma^2_i+\\lambda)^2}\\\\\n",
    "&\\le \\sum^{\\tilde{d}+1}_{i=1}1\\\\\n",
    "&= \\tilde{d}+1\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Also, it's clear that they are all larger or equal to zero. So we have finished the proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.14 \n",
    "\n",
    "\\begin{align*}\n",
    "E_{in} &= \\frac{1}{N}\\|y-Xw\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|y-H(\\lambda)y\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|\\left(I-H(\\lambda)\\right)y\\|^2\\\\\n",
    "&= \\frac{1}{N}\\|\\left(I-H(\\lambda)\\right)(f+\\epsilon)\\|^2\\\\\n",
    "&= \\frac{1}{N}(f+\\epsilon)^T\\left(I-H(\\lambda)\\right)^T\\left(I-H(\\lambda)\\right)(f+\\epsilon)\\\\\n",
    "&= \\frac{1}{N}(f^T+\\epsilon^T)\\left(I-H(\\lambda)\\right)^2(f+\\epsilon)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}f^T\\left(I-H(\\lambda)\\right)^2\\epsilon +  \\frac{1}{N}\\\\epsilon^T\\left(I-H(\\lambda)\\right)^2f +  \\frac{1}{N}\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Take expectation w.r.t. $\\epsilon$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}[E_{in}] &= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We need work out the expectation of the second term, we have\n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right] &= E_{\\epsilon}trace\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "&= E_{\\epsilon}trace\\left[\\epsilon\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\right]\\\\\n",
    "&= traceE_{\\epsilon}\\left[\\epsilon\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\right]\\\\\n",
    "&= traceE_{\\epsilon}[\\epsilon\\epsilon^T]E_{\\epsilon}\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= trace\\sigma^2\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= \\sigma^2trace\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "So the final expectation of \n",
    "\n",
    "\\begin{align*}\n",
    "E_{\\epsilon}[E_{in}] &= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}E_{\\epsilon}\\left[\\epsilon^T\\left(I-H(\\lambda)\\right)^2\\epsilon\\right]\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2trace\\left(I-H(\\lambda)\\right)^2\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2trace\\left(I-2H(\\lambda)+H^2(\\lambda)\\right)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\frac{1}{N}\\sigma^2\\left(N -\\left(2traceH(\\lambda)-trace(H^2(\\lambda))\\right)\\right)\\\\\n",
    "&= \\frac{1}{N} f^T \\left(I-H(\\lambda)\\right)^2 f +  \\sigma^2(1 -\\frac{d_{eff}}{N})\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (a) If the noise was not overfit, the term involving $\\sigma^2$ should equal to $\\sigma^2$ because \n",
    "$E_{\\epsilon}[E_{in}]$ is an estimate of the out-of-sample error, which should have constant term $\\sigma^2$. When the noise was overfit, $E_{\\epsilon}[E_{in}]$ underestimates the out-of-sample error. \n",
    "\n",
    "* (b) Hence, the degree to which the noise has been overfit is $\\frac{\\sigma^2d_{eff}}{N}$. It says that when $d_{eff}$ increases, the overfit to noise increases, when $N$ increases, the overift decreases. $d_{eff}$ acts as an effective number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.15 \n",
    "\n",
    "* (a) Let $\\tilde{Z} = Z\\Gamma^{-1}$, then $Z=\\tilde{Z}\\Gamma$, also assume $\\Gamma$ is square and invertible, we have\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\lambda) &= Z(Z^TZ+\\lambda \\Gamma^T\\Gamma)^{-1}Z^T\\\\\n",
    "&= \\tilde{Z}\\Gamma((\\tilde{Z}\\Gamma)^T\\tilde{Z}\\Gamma+\\lambda \\Gamma^T\\Gamma)^{-1}(\\tilde{Z}\\Gamma)^T\\\\\n",
    "&= \\tilde{Z}\\Gamma(\\Gamma^T\\tilde{Z}^T\\tilde{Z}\\Gamma+\\Gamma^T\\lambda \\Gamma)^{-1}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\Gamma\\left(\\Gamma^T(\\tilde{Z}^T\\tilde{Z}+\\lambda I) \\Gamma\\right)^{-1}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\Gamma\\Gamma^{-1}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\Gamma^{-T}\\Gamma^T\\tilde{Z}^T\\\\\n",
    "&= \\tilde{Z}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\tilde{Z}^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now we can proceed similar with problem 4.13 (i), we let the SVD of $\\tilde{Z}=U\\Lambda V^T$, where $U$ is $n\\times (d+1)$ matrix, $U^TU=I_{(d + 1) \\times (d+1)}$ and $V$ is $(d + 1) \\times (d+1)$, $V^TV=VV^T=I_{(d + 1) \\times (d+1)}$, and $\\Lambda$ is $(d + 1) \\times (d+1)$ diagonal matrix with entries $s_0,s_1,\\dots,s_{d}$. We have\n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{Z}^T\\tilde{Z} &= V\\Lambda U^T U\\Lambda V^T\\\\\n",
    "&= V\\Lambda^2V^T\\\\\n",
    "H(\\lambda) &= \\tilde{Z}\\left(\\tilde{Z}^T\\tilde{Z}+\\lambda I \\right)^{-1}\\tilde{Z}^T\\\\\n",
    "&= U\\Lambda V^T\\left(V\\Lambda^2V^T+\\lambda \\right)^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda V^T\\left(V(\\Lambda^2+\\lambda )V^T\\right)^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda V^TV^{-T}(\\Lambda^2+\\lambda I)^{-1}V^{-1}V\\Lambda U^T\\\\\n",
    "&= U\\Lambda^2(\\Lambda^2+\\lambda I)^{-1}U^T\\\\\n",
    "&= U\\Sigma U^T\\\\\n",
    "H^2(\\lambda) &= (U\\Sigma U^T)^TU\\Sigma U^T\\\\\n",
    "&= U\\Sigma^2U^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\Sigma = \\Lambda^2(\\Lambda^2+\\lambda I)^{-1}$ with entries on diagonal of $\\frac{s^2_i}{s^2_i+\\lambda}$, the effective degree of freedom is then\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= 2trace(H(\\lambda)) - trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^4_i+2\\lambda s^2_i} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{(s^2_i+\\lambda)^2-\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= \\sum^{d}_{i=0}1 - \\frac{\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "&= d+1 - \\sum^{d}_{i=0}\\frac{\\lambda^2} {(s^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (b) Similarly, we have\n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= trace(H(\\lambda)) \\\\\n",
    "&= trace(U\\Sigma U^T) \\\\\n",
    "&= trace(U^TU\\Sigma) \\\\\n",
    "&= trace(\\Sigma) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^2_i} {s^2_i+\\lambda}\\\\\n",
    "&= d+1- \\sum^{d}_{i=0}\\frac{\\lambda} {s^2_i+\\lambda}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (c) \n",
    "\n",
    "\\begin{align*}\n",
    "d_{eff}(\\lambda) &= trace(H^2(\\lambda)) \\\\\n",
    "&= \\sum^{d}_{i=0}\\frac{s^4_i} {(s^2_i+\\lambda)^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "In all cases, for $\\lambda \\ge 0$, we have $d_{eff}(\\lambda) \\ge 0$ because all the terms in the sum are non-negative. Also $d_{eff}(\\lambda) \\le d+1$ as shown in problem 4.13. \n",
    "\n",
    "When $\\lambda = 0$, we have $d_{eff}(0) = d+1$ as easily shown from above formulas. \n",
    "\n",
    "It's also easy to see that $d_{eff}$ is decreasing with $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.16\n",
    "\n",
    "For linear models and the general Tikhonov regularizer $\\Gamma$ with penalty term $\\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w$ in the augmented error, we have $E_{aug}(w) = E_{in}(w) + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w = \\frac{1}{N}\\|Zw-y\\|^2 + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w$. Take derivative w.r.t. $w$ and let it equal to $0$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{dE_{aug}}{dw} &= \\frac{1}{N}\\left(2Z^TZw - 2Z^Ty\\right) + \\frac{2\\lambda}{N}\\Gamma^T\\Gamma w\\\\\n",
    "&= \\frac{2}{N}\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)w - \\frac{2}{N}Z^Ty\\\\\n",
    "&= 0\\\\\n",
    "\\end{align*}\n",
    "\n",
    "We then have\n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (a) For the in-sample predictions, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{y} &= Zw_{reg}\\\\\n",
    "&= Z\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "&= H(\\lambda)y\\\\\n",
    "\\end{align*}\n",
    "\n",
    "* (b) When $\\Gamma = Z$, we have \n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &=\\left(Z^TZ + \\lambda \\Gamma^T\\Gamma\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^TZ + \\lambda Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left((I+\\lambda)Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= (Z^TZ)^{-1}(I+\\lambda)^{-1}Z^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}Z(Z^TZ)^{-1}Z^Ty\\\\\n",
    "&= \\frac{1}{1+\\lambda}w_{lin}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is called uniform weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.17\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{E}_{in}(w) &= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left(w^T\\hat{x}_n - y_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left(w^T(x_n+\\epsilon_n) - y_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}\\left((w^Tx_n-y_n)+w^T\\epsilon_n\\right)^2\\right]\\\\\n",
    "&= E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\frac{1}{N}\\sum^N_{n=1}(w^Tx_n-y_n)^2+2w^T\\epsilon_n(w^Tx_n-y_n) + (w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}(w^Tx_n-y_n)^2 + \\frac{1}{N}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\sum^N_{n=1}(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[\\sum^N_{n=1}(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}E_{\\epsilon_1,\\dots,\\epsilon_N}\\left[(w^T\\epsilon_n)^2\\right]\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}w^TE_{\\epsilon_n}[\\epsilon_n\\epsilon^T_n]w\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}\\sum^N_{n=1}w^T\\sigma^2_nIw\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}w^T\\left(\\sum^N_{n=1}\\sigma^2_nI\\right)w\\\\\n",
    "&= E_{in}(w) + \\frac{1}{N}w^T(\\sum^N_{n=1}\\sigma^2_n)w\\\\\n",
    "&= E_{in}(w) + \\frac{\\lambda}{N}w^T\\Gamma^T\\Gamma w\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\lambda = 1$, and $\\Gamma = \\begin{bmatrix}\\sigma_1\\\\\\sigma_2\\\\\\dots\\\\\\sigma_N\\end{bmatrix}$\n",
    "\n",
    "So the weights $\\hat{w}_{lin}$ resulting from minimizing $\\hat{E}_{in}$ are equivalent to the weights that would have been obtained by minimizing $E_{in}$ for the observed data with Tikhonov regularization using $\\Gamma$ and $\\lambda$ as above. \n",
    "\n",
    "We can intrepret this result as: regularization enforces a robustness to potential measurement errors (noise) in the observed inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4.18\n",
    "\n",
    "* (a) The regularized weights are given by\n",
    "\n",
    "\\begin{align*}\n",
    "w_{reg} &= \\left(Z^TZ + \\lambda Z^TZ\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^T(1+\\lambda)Z\\right)^{-1}Z^Ty\\\\\n",
    "&= \\left(Z^T(1+\\lambda)Z\\right)^{-1}Z^T\\left(Zw_f+\\epsilon\\right)\\\\\n",
    "&= \\left(Z^TZ + \\lambda I\\right)^{-1}Z^TZw_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= \\left(I - \\left(Z^TZ + \\lambda I\\right)^{-1}\\lambda \\right)w_f+\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "&= w_f - \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\\\\n",
    "\\end{align*}\n",
    "\n",
    "The average function $\\bar{g}$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{g}(x) &= E_{\\mathcal{D}}[g(x)]\\\\\n",
    "&= E_{\\mathcal{D}}[w^T_{reg}x]\\\\\n",
    "&= E_{\\mathcal{D}}\\left[\\left(w_f- \\lambda\\left(Z^TZ + \\lambda I\\right)^{-1}w_f +\\left(Z^TZ + \\lambda I\\right)^{-1}Z^T\\epsilon\\right)^Tx\\right]\\\\\n",
    "&= E_{Z,\\epsilon}\\left[w^T_fx - \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x + \\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x\\right]\\\\\n",
    "&= E_{Z,\\epsilon}[w^T_fx- \\lambda w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}x] + E_{Z,\\epsilon}[\\epsilon^T Z(Z^TZ+ \\lambda I)^{-1} x]\\\\\n",
    "&= w^T_fx - \\lambda E_Z\\left[ w^T_f\\left(Z^TZ + \\lambda I\\right)^{-1}\\right]x + E_{Z,\\epsilon}[\\epsilon^T]E_{Z,\\epsilon}[Z(Z^TZ+ \\lambda I)^{-1}]x\\\\\n",
    "&= w^T_fx- \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}x\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\frac{1}{N}Z^TZ = \\frac{1}{N}\\sum^N_{n=1}\\Phi(x_n)\\Phi^T(x_n)$ is the in-sample estimate of $\\Sigma_{\\Phi} = E[\\Phi(x)\\Phi^T(x)] = I_{(d+1)\\times(d+1)}$, so by the law of large numbers, $\\frac{1}{N}Z^TZ = \\Sigma_{\\Phi} + o(1)$, so $\\frac{1}{N}(Z^TZ + \\lambda I) = \\Sigma_{\\Phi} + \\frac{1}{N}\\lambda I + o(1) = (1+\\frac{\\lambda}{N})I + o(1)$.\n",
    "\n",
    "Then when $N$ is large\n",
    "\n",
    "\\begin{align*}\n",
    "E_Z\\left(Z^TZ + \\lambda I\\right)^{-1} &= \\frac{1}{N}E_Z\\left(\\frac{1}{N}(Z^TZ + \\lambda I)\\right)^{-1} \\\\\n",
    "&= \\frac{1}{N}(1+\\frac{\\lambda}{N})^{-1}I \\\\\n",
    "&= \\frac{1}{\\lambda + N}I \\\\\n",
    "\\end{align*}\n",
    "\n",
    " \n",
    "\n",
    "We can compute the bias as: \n",
    "\n",
    "\\begin{align*}\n",
    "bias &= E_{\\mathcal{D},x}[\\left(\\bar{g}(x) - f(x)\\right)^2]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(w^T_f\\Phi(x) - \\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)- w^T_f\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= E_{\\mathcal{D},x}\\left[\\left(\\lambda w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right)^2\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)E_Z\\left(Z^TZ + \\lambda I\\right)^{-T}w_f w^T_fE_Z\\left(Z^TZ + \\lambda I\\right)^{-1}\\Phi(x)\\right]\\\\\n",
    "&= \\lambda^2 E_{\\mathcal{D},x}\\left[\\Phi^T(x)\\frac{1}{\\lambda + N}I w_f w^T_f\\frac{1}{\\lambda + N}I \\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} E_{\\mathcal{D},x}\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi^T(x)w_f w^T_f\\Phi(x)\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2}  E_{\\mathcal{D},x}trace\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace E_{\\mathcal{D},x}\\left[\\Phi(x)\\Phi^T(x)w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace \\left[E_{\\mathcal{D},x}[\\Phi(x)\\Phi^T(x)]w_f w^T_f\\right]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [Iw_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} trace [w_f w^T_f]\\\\\n",
    "&= \\frac{\\lambda^2}{(\\lambda + N)^2} \\|w_f\\|^2\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Where we have used the assumption that the input probability satisfies $E_{x} \\left[\\Phi(x)\\Phi^T(x)\\right]= I $\n",
    "\n",
    "Also note, that $w_f w^T_f$ is a $(d+1)\\times (d+1)$ matrix, its trace is just $\\|w_f\\|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
