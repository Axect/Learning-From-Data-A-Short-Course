# Chapter 1
#### Exercise 1.4
Pick line $y=2x+2$ as the separating line in the 2D plane. We need pick 20 data points:

x1, x2, y
0,0,1
1,1,1
0,1,1
-0.5,0,1
-1,-1,1
2,4,1
1,-1,1
3,-5,1
0.5,-2,1
-1,1,-1
0,3,-1
-2,0,-1
-3,-1,-1
1,6,-1
2,8,-1
-4,2,-1
-3,3,-1
3,10,-1
-0.5,2,-1
-2,1,-1

# Questions
* How to understand VC dimension? 
* How to understand the bounds? 

### 3.3 Logistic Regression
* Why minimize perceptron requires combinatorial efforts while minimize linear regression requires just analytic solution. Logistic regression needs gradient descent.

* When can GD be used? What algorithms use gradient descent method? What use sub-gradient methods? What are the requirements for GD? 
* Why fixed rate learning rate GD works? 







  
