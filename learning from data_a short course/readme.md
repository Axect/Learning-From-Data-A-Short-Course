# Chapter 1
#### Exercise 1.4
Pick line $y=2x+2$ as the separating line in the 2D plane. We need pick 20 data points:

x1, x2, y
0,0,1
1,1,1
0,1,1
-0.5,0,1
-1,-1,1
2,4,1
1,-1,1
3,-5,1
0.5,-2,1
-1,1,-1
0,3,-1
-2,0,-1
-3,-1,-1
1,6,-1
2,8,-1
-4,2,-1
-3,3,-1
3,10,-1
-0.5,2,-1
-2,1,-1

# Questions
## Chapter 1
* What are the components of a learning algorithm? 
* What are different types of learning problems? 
* Why we are able to learn at all? 
* What are the meanings of various probability quantities? 

## Chapter 2
* How do we generalize from training data? 
* How to understand VC dimension? What is the VC dimension of linear models, e.g. perceptron? 
* How to understand the bounds? 
* How to understand the two competing forces: approximation and generalization?
* How to understand the bias variance trade off? How to derive it? 
* Why do we need test set? What's the difference between train and test set? What are the advantages of using a test set?
* What is learning curve? How to intepret it? 

## Chapter 3
* What are the linear models? Linear classification, linear regression and logistic regression.
* What's the application of approximation-generalization in linear models? 
* Why minimize perceptron requires combinatorial efforts while minimize linear regression requires just analytic solution. Logistic regression needs gradient descent.
* When can GD be used? What algorithms use gradient descent method? What use sub-gradient methods? What are the requirements for GD? 
What are the advantages of using SGD? What does SGD work at all? What's the convergence speed between GD and SGD? 
* Why fixed rate learning rate GD works? 
* How does feature transformation affect the VC dimension? 
* What are the advantages and disadvantages of feature transformation? 








  
