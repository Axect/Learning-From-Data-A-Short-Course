
# Questions
## Chapter 1
* What are the components of a learning algorithm? 
* What are different types of learning problems? 
* Why we are able to learn at all? 
* What are the meanings of various probability quantities? 

## Chapter 2
* How do we generalize from training data? 
* How to understand VC dimension? What is the VC dimension of linear models, e.g. perceptron? 
* How to understand the bounds? 
* How to understand the two competing forces: approximation and generalization?
* How to understand the bias variance trade off? How to derive it? 
* Why do we need test set? What's the difference between train and test set? What are the advantages of using a test set?
* What is learning curve? How to intepret it? 

## Chapter 3
* What are the linear models? Linear classification, linear regression and logistic regression.
* What's the application of approximation-generalization in linear models? 
* Why minimize perceptron requires combinatorial efforts while minimize linear regression requires just analytic solution. Logistic regression needs gradient descent.
* When can GD be used? What algorithms use gradient descent method? What use sub-gradient methods? What are the requirements for GD? 
What are the advantages of using SGD? What does SGD work at all? What's the convergence speed between GD and SGD? 
* Why fixed rate learning rate GD works? 
* How does feature transformation affect the VC dimension? 
* What are the advantages and disadvantages of feature transformation? 








  
