{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Training and Testing\n",
    "## Exercises\n",
    "\n",
    "#### Exercise 2.1\n",
    "1. Positive rays: break point $k = 2$ and $m_{\\mathcal{H}}(k) = k+1 = 3 \\lt 2^k = 4$.\n",
    "1. Positive intervals: break point $k = 3$ and $m_{\\mathcal{H}}(k) = {k+1 \\choose 2} +1 = 7 \\lt 2^k = 8$.\n",
    "1. Convex sets: no break point exists. For any $k$, we can find a set of $k$ points on a circle that can be shattered. \n",
    "\n",
    "#### Exercise 2.2\n",
    "1. (a) \n",
    "  * (i) $k=2$, $RHS = \\sum^{1}_{i=0}{N \\choose i} = N+1$, while $m_{\\mathcal{H}}(N) = N+1$, so $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  * (ii) $k=3$, $RHS = \\sum^{2}_{i=0}{N \\choose i} = \\frac{N(N-1)}{2} + N+1 = \\frac{N(N+1)}{2} + 1$, while $m_{\\mathcal{H}}(N) = {N+1 \\choose 2}+1 = \\frac{N(N+1)}{2} + 1$, so $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  * (iii) There's no such $k$ exists. Maximum $k = N+1$, since $\\sum^{N}_{i=0}{N \\choose i} = 2^N$, we still have $m_{\\mathcal{H}}(N) \\le \\sum^{k}_{i=0}{N \\choose i}$\n",
    "  \n",
    "1. (b) If $m_{\\mathcal{H}}(N) = N+2^{\\frac{N}{2}}$, then the break point $k=3$. According to bound theorem 2.4, we have for all $N$, $m_{\\mathcal{H}}(N) = N+2^{\\frac{N}{2}} \\le \\sum^{2}_{i=0}{N \\choose i} = \\frac{N(N+1)}{2} + 1$. But this won't hold for all $N$ since left hand side is exponentially increasing while the RHS is polynomical increasing. For example, when $N=20$, the inequality breaks. So such hypothesis set doesn't exist. \n",
    "\n",
    "#### Exercise 2.3\n",
    "1. (i) $d_{VC} = 1$\n",
    "1. (ii) $d_{VC} = 2$\n",
    "1. (iii) $d_{VC} = \\infty$\n",
    "\n",
    "#### Exercise 2.4 TODO\n",
    "1. (a) \n",
    "\n",
    "#### Exercise 2.5\n",
    "Through equation (2.12), we find that $\\delta = 709.527509678$, so the probability is just greater or equal to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709.5275096780147, 1.1331484530668263)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2.5\n",
    "import numpy as np\n",
    "N = 100\n",
    "d = 0.1\n",
    "mh = 2*N + 1\n",
    "delta = 4*mh / np.exp(N * d**2 /8)\n",
    "delta, np.exp(N * d**2 /8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.6\n",
    "* (a) Apply the error bar in $(2.1)$, i.e. $E_{out}(g) \\le E_{in}(g) + \\sqrt{\\frac{1}{2N}\\ln{\\frac{2M}{\\delta}}}$. \n",
    "\n",
    "The following calculation shows that error on $E_{in}(g) = 0.1151$ and error on $E_{test}(g) = 0.096$. So the error bar on in-sample error is higher than the error bar from test error. \n",
    "\n",
    "* (b) If we reserve more examples for testing, then we'll have less samples for training. We may end up with a hypothesis that is not as good as we could have arrived if using more training samples. So $E_{test}(g)$ might be way too off even the error bar on it is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test bound:  0.09603227913199208\n",
      "train bound:  0.11509037065006825\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.6\n",
    "import numpy as np\n",
    "epsilon = 0.05\n",
    "\n",
    "N = 200\n",
    "# test bound\n",
    "print('test bound: ', np.sqrt(np.log(2/epsilon)/2/N))\n",
    "\n",
    "# train bound\n",
    "M = 1000\n",
    "N = 400\n",
    "print('train bound: ', np.sqrt(np.log(2*M/epsilon)/2/N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.7 \n",
    "1. (a) \n",
    "\\begin{align}\n",
    "P[h(x)\\ne f(x)] &= P[h(x)\\ne f(x)]\\cdot 1 + P[h(x) = f(x)]\\cdot0 \\\\\n",
    "&= P[h(x)\\ne f(x)] (h(x)-f(x))^2 + P[h(x) = f(x)](h(x)-f(x))^2 \\\\\n",
    "&= E[(h(x)-f(x))^2]\n",
    "\\end{align}\n",
    "\n",
    "1. (b) \n",
    "\\begin{align}\n",
    "P[h(x)\\ne f(x)] &= \\frac{1}{4}P[h(x)\\ne f(x)]\\cdot 4 + \\frac{1}{4}P[h(x) = f(x)]\\cdot0 \\\\\n",
    "&= \\frac{1}{4}P[h(x)\\ne f(x)] (h(x)-f(x))^2 + \\frac{1}{4}P[h(x) = f(x)](h(x)-f(x))^2 \\\\\n",
    "&= \\frac{1}{4}E[(h(x)-f(x))^2]\n",
    "\\end{align}\n",
    "\n",
    "#### Exercise 2.8\n",
    "\n",
    "1. (a) If $\\mathcal{H}$ is closed under linear combination, for any $x$, $\\bar{g}(x)$ is weighted (by probability of data) average of hypotheses in $\\mathcal{H}$, so $\\bar{g}(x) \\in \\mathcal{H}$.\n",
    "\n",
    "1. (b) If $\\mathcal{H}$ is a set of functions defined on intervals, e.g. $f(x) = c$ when $x \\in [a,b]$, otherwise $f(x) = 0$. Then $\\bar{g}(x)$ probably won't have constant value in an interval and not in the original hypothesis set.\n",
    "\n",
    "1. (c) For binary classification, each $g(x)$ will have value $+1$ or $-1$, when weighted by probabilities, the average is not binary any more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.1\n",
    "\n",
    "For $\\epsilon \\le k$, we have $N \\ge \\frac{1}{2k^2}ln\\frac{2M}{\\delta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples needed for M = 1:\tN = 839.9410155759854\n",
      "Samples needed for M = 100:\tN = 1760.9750527736032\n",
      "Samples needed for M = 10000:\tN = 2682.0090899712213\n"
     ]
    }
   ],
   "source": [
    "def calc_N(M, delta, k):\n",
    "    return np.log(2*M/delta)/2/k/k\n",
    "delta = 0.03\n",
    "k = 0.05\n",
    "for M in [1,100, 10000]:\n",
    "    N = calc_N(M, delta, k)\n",
    "    print(\"Samples needed for M = {}:\\tN = {}\".format(M, N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.2\n",
    "For $N=4$, we can pick points: $(1,3),(2,4),(3,1),(4,2)$. It's easy to see that these points are shattered by positive rectangles. So $m_{\\mathcal{H}}(4) = 2^4$.\n",
    "\n",
    "The idea is that for any two points, if we draw a rectangle using them as diagnoal points, the rectangle should NOT contain any other point. Otherwise, whenever the two diagnoal points have values 1, the middle point will have value 1 as well, which excludes the possibility of having -1. \n",
    "\n",
    "For $N=5$, if we draw horizontal and vertical lines through each of the four points above, the plane is divided into grids. The four points enclusing a 9-grid area. It's clear that the fifth point can't lie within the 9-grid area. Otherwise, there'll always a rectangle (constructed by two points) contains the fifth point. \n",
    "\n",
    "In the same way, if we place the fifth point outside the 9-grid area, it's easy to see that the point will always lie below or above at least two points (in either x or y direction). These three points construct a rectangle which contains a point in it. \n",
    "This shows that $m_{\\mathcal{H}}(5) \\lt 2^5$.\n",
    "\n",
    "We have the VC dimension $d_{VC}(\\mathcal{H}) = 4$, and $m_{\\mathcal{H}}(N) \\le \\sum^{4}_{i=0} {N \\choose i}$.\n",
    "\n",
    "#### Problem 2.3\n",
    "1. (a) $d_{VC}(\\mathcal{H}) = 2$, $m_{\\mathcal{H}}(N) = {N+1 \\choose 1} + {N-1 \\choose 1} = 2N$\n",
    "\n",
    "1. (b) $d_{VC}(\\mathcal{H}) = 3$, $m_{\\mathcal{H}}(N) = {N+1 \\choose 2}+1 +{N-1 \\choose 2} = N^2 - N + 2$\n",
    "\n",
    "1. (c) $d_{VC}(\\mathcal{H}) = 2$, $m_{\\mathcal{H}}(N) = {N+1 \\choose 2}+1 = \\frac{(N+1)N}{2} + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.4 TODO\n",
    "\n",
    "#### Problem 2.5\n",
    "\n",
    "For $N=1$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
