{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Theory\n",
    "Decision theory is about finding \"optimal\" actions, under various definitions of optimality.\n",
    "\n",
    "\n",
    "## Typical Sequence of Events\n",
    "* Many problem domains can be formalized as follows:\n",
    "  * Observe input $x$\n",
    "  * Take action $a$\n",
    "  * Observe outcome $y$\n",
    "    * Outcome $y$ is often **independent** of action $a$\n",
    "    * But this is **not always the case**:\n",
    "      * search result ranking\n",
    "      * automated driving\n",
    "      * stock market predicitons from analysts might affect market movement\n",
    "  * Evaluate action in relation to the outcome: $L(a,y)$\n",
    "  \n",
    "## The Three Spaces\n",
    "* Input space: $\\mathcal X$\n",
    "* Action spcae: $\\mathcal A$\n",
    "* Outcome space: $\\mathcal Y$\n",
    "\n",
    "### Action\n",
    "* Definition: An action is the generic term for what is produced by our system (my understanding the system here means prediction function)\n",
    "\n",
    "* Examples of Actions\n",
    "  * Produce a $0/1$ classification [classical ML]\n",
    "  * Reject hypothesis that $\\theta = 0$ [classical Statistics]\n",
    "  * Written English text [image captioning, speech recognition, machine translation]\n",
    "  \n",
    "### Decision Function\n",
    "Definition: A **decision function (or **predicition function**) gets input $x \\in \\mathcal{X}$ and produces an action $a \\in \\mathcal {A}$:\n",
    "\n",
    "\\begin{equation}\n",
    "f: \\mathcal{X} \\to \\mathcal{A}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x \\to f(x)\n",
    "\\end{equation}\n",
    "\n",
    "### Loss Function\n",
    "Definition: A **loss function** evaluates an action in the context of the outcome $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "L: \\mathcal {A} \\times \\mathcal{Y} \\to \\mathbb{R} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "(a,y) \\to L(a,y)\n",
    "\\end{equation}\n",
    "\n",
    "## Formalizing a \"Data Science\" Problem\n",
    "1. First two steps to formalizing a problem  \n",
    "  1. Define the *action space* (i.e. the set of possible actions)\n",
    "  1. Specify the evaluation criterion.\n",
    "1. When a \"stakeholder\" asks the data scientist to solve a problem, she\n",
    "  1. may have an opinion on what the action space should be, and\n",
    "  1. hopefully has an opinion on the evaluation criterion, but\n",
    "  1. she really cares about your **producing a \"good\" decision function**.\n",
    "1. Typical sequence:\n",
    "  1. Stakeholder presents problem to data scientist\n",
    "  1. Data scientist produces decision function.\n",
    "  1. Engineer deploys \"industrial strength\" version of decision function.\n",
    "\n",
    "## Evaluating a Decision Function\n",
    "* Loss function $L$ only evaluates a single action\n",
    "* How to evaluate the decision function as a whole? (Answer: Statistical Learning Theory)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning Theory\n",
    "\n",
    "## A Simplifying Assumption\n",
    "* Assume action has no effect on the output\n",
    "* Assume there is a data generating distribution $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$.\n",
    "* All input/output pairs $(x,y)$ are generated i.i.d. from $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$.\n",
    "  * no covariate shift\n",
    "  * no concept drift\n",
    "* Want decision function $f(x)$ that generally \"does well on average\":\n",
    "\\begin{equation}\n",
    "L(f(x), y) \\;\\;\\;\\text{ is usually small, in some sense}\n",
    "\\end{equation}\n",
    "\n",
    "## Risk of a Decision Function\n",
    "Definition: Given a decision function (or prediction function) $f(x): \\mathcal{X} \\to \\mathcal{A}$, the **risk** of this decision funciton is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "R(f) = E[L(f(x), y)]\n",
    "\\end{equation}\n",
    "\n",
    "where $L(f(x), y)$ is the **loss function**.\n",
    "\n",
    "In words, it's the **expected loss** of $f$ on a new example $(x,y)$ drawn randomly from $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$.\n",
    "\n",
    "* We usually don't know $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$, so we cannot compute the expectation. But we can estimate it.\n",
    "\n",
    "### The Bayes Decision Function\n",
    "Definition: A **Bayes decision function** $f^* : \\mathcal{X} \\to \\mathcal{A}$ is a function that achieves the *minimal risk* among all possible functions:\n",
    "\n",
    "\\begin{equation}\n",
    "f^* = argmin_f{R(f)}\n",
    "\\end{equation}\n",
    "\n",
    "where the minimum is taken over all functions from $\\mathcal{X}$ to $\\mathcal{A}$.\n",
    "\n",
    "* The risk of a Bayes decision function is called the **Bayes Risk**.\n",
    "  * There can be multiple Bayes decision functions that achieve the same minimal risk.\n",
    "* A Bayes decision function is often called the \"target function\", since it's the best decision function we can possibly produce.\n",
    "\n",
    "\n",
    "### Regression Function and Squared Error Loss\n",
    "Let $X \\in R^p$ denote a real valued random input vector, and $Y \\in R$ a real valued random output variable, with joint distribution $P(X,Y)$. \n",
    "\n",
    "Consider the **squared error loss** function, where $L(f(x), y) = (y-f(x))^2$.\n",
    "\n",
    "\\begin{equation}\n",
    "R(f) = E[(Y-f(X))^2] = \\int [y-f(x)]^2P(x,y)\n",
    "\\end{equation}\n",
    "\n",
    "By conditioning on $X$, we can write the risk as\n",
    "\n",
    "\\begin{equation}\n",
    "R(f) = E_XE_{Y|X}[(Y-f(X))^2|X]\n",
    "\\end{equation}\n",
    "\n",
    "We see that it suffices to minimize $R(f)$ pointwise:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = argmin_cE_{Y|X}[(Y-c)^2|X]\n",
    "\\end{equation}\n",
    "\n",
    "The solution is\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = E[Y|X]\n",
    "\\end{equation}\n",
    "\n",
    "the conditional expectation, also known as the **regression function**. Thus the best prediction of $Y$ at any point $X=x$ is the conditional mean, when best is measured by average squared error.\n",
    "\n",
    "## The Empirical Risk Functional\n",
    "### The Empirical Risk of a Decision Function\n",
    "Let $\\mathcal{D}_n=((x_1,y_1),\\dots,(x_n,y_n))$ be drawn i.i.d. from $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$.\n",
    "\n",
    "Definition: The **empirical risk** of $f:\\mathcal{X}\\to \\mathcal{A}$ with respect to $\\mathcal{D}_n$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}_n(f) = \\frac{1}{n}\\sum_{i=1}^{n}L(f(x_i),y_i)\n",
    "\\end{equation}\n",
    "\n",
    "By the Strong Law of Large Numbers,\n",
    "\n",
    "\\begin{equation}\n",
    "\\lim_{n\\to \\infty}\\hat{R}_n(f)=R(f) \\;\\;\\;\\text{ almost surely.}\n",
    "\\end{equation}\n",
    "\n",
    "### Empirical Risk Minimization (ERM)\n",
    "Definition: A function $\\hat{f}$ is an **empirical risk minimizer** if \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f} = argmin_f{\\hat{R}_n(f)}\n",
    "\\end{equation}\n",
    "\n",
    "where the minimum is takend over all function.\n",
    "\n",
    "\n",
    "### Constrained Empirical Risk Minimization (CERM)\n",
    "* ERM led to a function $f$ that just memorized the data.\n",
    "* How to spread information or \"generalize\" from training inputs to new inputs?\n",
    "* Need to smooth things out somehow...\n",
    "  * A lot of modeling is about spreading and extrapolating information from one part of the input space $\\mathcal{X}$ into unobserved parts of the space.\n",
    "* One approach: \"Constrained ERM\"\n",
    "  * Instead of minimizing empirical risk over all decision functions,\n",
    "  * constrain to a particular subset, called a **hypothesis space**.\n",
    "\n",
    "#### Hypothesis Spaces \n",
    "Definition: A **hypothesis space** $\\mathcal{F}$ is a set of [decision ] functions mapping $\\mathcal{X} \\to \\mathcal{A}$. It is the collection of decision functions we are considering.\n",
    "\n",
    "#### CERM\n",
    "* **Empirical Risk Minimizer** (ERM) in $\\mathcal{F}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_n = argmin_{f\\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}{L(f(x_i),y_i)}\n",
    "\\end{equation}\n",
    "\n",
    "* **Risk minimizer** in $\\mathcal{F}$ is $f^*_{\\mathcal{F}} \\in \\mathcal{F}$, where\n",
    "\n",
    "\\begin{equation}\n",
    "f^*_{\\mathcal{F}} = argmin_{f\\in \\mathcal{F}}E[L(f(x),y)]\n",
    "\\end{equation}\n",
    "\n",
    "### Procedure of ERM\n",
    "* Given a loss function $L:\\mathcal{A}\\times \\mathcal{Y} \\to \\mathbb{R}$\n",
    "* Choose hypothesis space $\\mathcal{F}$\n",
    "* Use an optimization method to find ERM $\\hat{f}_n \\in \\mathcal{F}$\n",
    "  \n",
    "\\begin{equation}\n",
    "\\hat{f}_n=argmin_{f\\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}{L(f(x_i),y_i)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Excess Risk Decomposition\n",
    "\n",
    "### Error Decomposition\n",
    "\n",
    "Consider following decision functions\n",
    "\n",
    "* Risk minimizer (i.e. Bayes decision function) over all functions\n",
    "\n",
    "\\begin{equation}\n",
    "f^{*}=argmin_{f} E[L(f(X),Y)]\n",
    "\\end{equation}\n",
    "\n",
    "* Risk minimizer over all functions within hypothesis space $\\mathcal{F}$\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\mathcal{F}} = argmin_{f\\in \\mathcal{F}} E[L(f(X),Y)]\n",
    "\\end{equation}\n",
    "\n",
    "* Empirical Risk minimizer over all functions within hypothesis space $\\mathcal{F}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_n = argmin_{f\\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}{L(f(x_i),y_i)}\n",
    "\\end{equation}\n",
    "\n",
    "![Risk Error Decomposition](resources/risk_error_decomposition.gif)\n",
    "\n",
    "### Approximation Error (of $\\mathcal{F}$)  =  $R(f_{\\mathcal{F}}) - R(f^*)$\n",
    "* Approximation error is a property of the class $\\mathcal{F}$\n",
    "* It is the penalty for restricting to $\\mathcal{F}$ (rather than consdering all possible functions)\n",
    "* Bigger $\\mathcal{F}$ means smaller approximation error.\n",
    "* Approximation error is a non-random variable.\n",
    "\n",
    "### Estimation Error (of $\\hat{f}_n\\;in\\; \\mathcal{F}$)  =  $R(\\hat{f}_n) - R(f_{\\mathcal{F}})$\n",
    "* Note, $R(\\hat{f}_n) = E[L(\\hat{f}_n(X),Y)]$. \n",
    "* Estimation error is a random variable since the data, $(x_i,y_i), i = 1, 2, \\dots, n$, used to compute  $\\hat{f}_n$, i.e. $argmin_{f\\in \\mathcal{F}}\\frac{1}{n}\\sum_{i=1}^{n}{L(f(x_i),y_i)}$, is randomly sampled from distribution $\\mathcal{P}_{\\mathcal{X}\\times\\mathcal{Y}}$. So the risk $R(\\hat{f}_n)$ depends on training data.\n",
    "* Estimation error is the performance hit for choosing $f$ using finite training data\n",
    "* It is the performance hit for minimizing empirical risk rather than true risk\n",
    "* With smaller $\\mathcal{F}$ we expect smaller estimation error. \n",
    "* Under typical conditions: \"With infinite training data, estimation error goes to zero\"\n",
    "\n",
    "### Optimization Error\n",
    "* In practice of ERM, we don't find the empirical risk minimizer $\\hat{f}_n \\in \\mathcal{F}$\n",
    "  * For nice choices of loss functions and classes $\\mathcal{F}$, we can get arbitrarily close to a empirical risk minimizer, but that takes time, is it worth it?\n",
    "  * For some hypothesis spaces (e.g. neural networks), we don't know how to find $\\hat{f}_n \\in \\mathcal{F}$\n",
    "* In stead, in practice, we find $\\tilde{f}_n \\in \\mathcal{F}$ that we hope is good enough\n",
    "\n",
    "Definition: If $\\tilde{f}_n$ is the function our optimization method returns, and $\\hat{f}_n$ is the empirical risk minimizer, then \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Optimization Error } = R(\\tilde{f}_n) - R(\\hat{f}_n)\n",
    "\\end{equation}\n",
    "\n",
    "* Note: optimization error can be negative. \n",
    "* But by definition of $\\hat{f}_n$, in empirical risk we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{R}(\\tilde{f}_n) - \\hat{R}(\\hat{f}_n) \\ge 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Excess Risk\n",
    "Definition: The **excess risk** compares the risk of $f$ to the Bayes optimal $f^*$\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Excess Risk}(f)=R(f) - R(f^*)\n",
    "\\end{equation}\n",
    "\n",
    "Note: excess risk can never be negative.\n",
    "\n",
    "### Excess Risk Decomposition for ERM\n",
    "#### The excess risk of the ERM $\\hat{f}_n$ can be decomposed\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Excess Risk}(\\hat{f}_n) = R(\\hat{f}_n)-R(f^*)= [R(\\hat{f}_n)-R(f_{\\mathcal{F}})] + [R(f_{\\mathcal{F}}) - R(f^*)]= \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{                                                           Estimation Error } + \\text{        Approximation Error}\n",
    "\\end{equation}\n",
    "\n",
    "* Data scientist's job\n",
    "  * choose $\\mathcal{F}$ to balance between approximation and estimation error\n",
    "  * as we get more training data, use a bigger $\\mathcal{F}$.\n",
    "  \n",
    "#### The excess risk for function $\\tilde{f}_n$ can be decomposed  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Excess Risk}(\\tilde{f}_n) = R(\\tilde{f}_n)-R(f^*)= \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "[R(\\tilde{f}_n)-R(\\hat{f}_n)] + [R(\\hat{f}_n)-R(f_{\\mathcal{F}})] + [R(f_{\\mathcal{F}}) - R(f^*)]= \\\\\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{  Optimization Error} + \\text{ Estimation Error } + \\text{        Approximation Error}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
